<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2022-12-22T14:12:13-08:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Scaling Vision Model Training Platforms with PyTorch</title>
      <link href="https://pytorch.org/blog/scaling-vision-model-training-platforms-with-pytorch/" rel="alternate" type="text/html" title="Scaling Vision Model Training Platforms with PyTorch" />
      <published>2022-12-22T00:00:00-08:00</published>
      <updated>2022-12-22T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/scaling-vision-model-training-platforms-with-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/scaling-vision-model-training-platforms-with-pytorch/">&lt;p&gt;&lt;em&gt;TL;DR: We demonstrate the use of PyTorch with FairScale’s FullyShardedDataParallel (FSDP) API in writing large vision transformer models. We discuss our techniques for scaling and optimizing these models on a GPU cluster. The goal of this platform scaling effort is to enable research at scale. This blog does not discuss model accuracy, new model architectures, or new training recipes.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Latest vision research [1, 2] demonstrates model scaling as a promising research direction. In this project, we aim to enable our platforms to train massive vision transformer (ViT) [3] models. We present our work on scaling the largest trainable ViT from 1B to 120B parameters in FAIR vision platforms. We wrote ViT in PyTorch and leveraged its support for large-scale, distributed training on a GPU cluster.&lt;/p&gt;

&lt;p&gt;In the rest of this blog, we will first discuss the main challenges, namely &lt;em&gt;scalability&lt;/em&gt;, &lt;em&gt;optimization&lt;/em&gt;, and &lt;em&gt;numerical stability&lt;/em&gt;. Then we will discuss how we tackle them with techniques including &lt;em&gt;data and model parallelism&lt;/em&gt;, &lt;em&gt;automatic mixed precision&lt;/em&gt;, &lt;em&gt;kernel fusion&lt;/em&gt;, and &lt;em&gt;bfloat16&lt;/em&gt;. Finally, we present our results and conclude.&lt;/p&gt;

&lt;h2 id=&quot;2-main-challenges&quot;&gt;2. Main Challenges&lt;/h2&gt;

&lt;h3 id=&quot;21-scalability&quot;&gt;2.1 Scalability&lt;/h3&gt;

&lt;p&gt;The key scalability challenge is to efficiently shard a model’s operations and state across multiple GPUs. A 100B parameter model requires ~200GB of RAM just for parameters, assuming fp16 representation. So, it is impossible to fit the model on a single GPU (A100 has at most 80GB RAM). Therefore, we need some way to efficiently shard a model’s data (input, parameters, activations, and optimizer state) across multiple GPUs.&lt;/p&gt;

&lt;p&gt;Another aspect of this problem is to scale without significantly changing the training recipe. E.g. Certain representation learning recipes use a global batch size of up to 4096 beyond which we start to see accuracy degradation. We cannot scale to more than 4096 GPUs without using some form of tensor or pipeline parallelism.&lt;/p&gt;

&lt;h3 id=&quot;22-optimization&quot;&gt;2.2 Optimization&lt;/h3&gt;

&lt;p&gt;The key optimization challenge is to maintain high GPU utilization even as we scale the number of model parameters and flops. When we scale models to teraflops and beyond, we start to hit major bottlenecks in our software stack that super-linearly increase training time and reduce accelerator utilization. We require hundreds or thousands of GPUs to run just a single experiment. Improvements in accelerator utilization can lead to significant reductions in cost and improve fleet utilization. It enables us to fund more projects and run more experiments in parallel.&lt;/p&gt;

&lt;h3 id=&quot;23-numerical-stability&quot;&gt;2.3 Numerical Stability&lt;/h3&gt;

&lt;p&gt;The key stability challenge is to avoid numerical instability and divergence at large scale. We empirically observed in our experiments that the training instability gets severe and hard to deal with when we scale up model sizes, data, batch sizes, learning rate, etc. Vision Transformers particularly face training instability even at a lower parameter threshold. E.g., we find it challenging to train even ViT-H (with just 630M parameters) in mixed-precision mode without using strong data augmentation. We need to study the model properties and training recipes to make sure that the models train stably and converge.&lt;/p&gt;

&lt;h2 id=&quot;3-our-solutions&quot;&gt;3. Our Solutions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt; depicts our solutions to each of the challenges.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_1-solutions-to-the-challenges.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;31-addressing-scaling-challenges-with-data-parallelism-and-model-parallelism&quot;&gt;3.1 Addressing scaling challenges with data parallelism and model parallelism&lt;/h3&gt;

&lt;p&gt;We apply various forms of data and model parallelism to enable fitting very large models in GPU memory.&lt;/p&gt;

&lt;p&gt;We use FairScale’s &lt;em&gt;FullyShardedDataParallel (FSDP)&lt;/em&gt; API [4], based on PyTorch, to shard parameters, gradients, and optimizer state across multiple GPUs, thereby reducing the memory footprint per GPU. This process consists of the following three steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step 1: We wrapped the entire model in a single FSDP instance. This shards the model parameters at the end of a forward pass and gathers parameters at the beginning of a forward pass. This enabled us to scale ~3x from 1.5B to 4.5B parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 2: We experimented with wrapping individual model layers in separate FSDP instances. This nested wrapping further reduced the memory footprint by sharding and gathering parameters of individual model layers instead of an entire model. The peak memory is then determined by an individually wrapped transformer block in GPU memory in this mode instead of the entire model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 3: We used &lt;em&gt;activation-checkpoint&lt;/em&gt; to reduce the memory consumption by activations. It saves the input tensors and discards the intermediate activation tensors during the forward pass. These are recomputed during the backward pass.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we experimented with model-parallelism techniques such as pipeline parallelism [5], which allow us to scale to more GPUs without increasing the batch size.&lt;/p&gt;

&lt;h3 id=&quot;32-addressing-optimization-challenges-with-advanced-amp-and-kernel-fusion&quot;&gt;3.2 Addressing optimization challenges with advanced AMP and kernel fusion&lt;/h3&gt;

&lt;h4 id=&quot;advanced-amp&quot;&gt;Advanced AMP&lt;/h4&gt;

&lt;p&gt;Automatic Mixed Precision (AMP) [6] training refers to training models using a lower precision of bits than FP32 or the default but still maintaining accuracy. We experimented with three levels of AMP as described below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AMP O1: This refers to training in mixed precision where weights are in FP32 and some operations are in FP16. With AMP O1, the ops that might impact accuracy remain in FP32 and are not autocasted to FP16.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AMP O2: This refers to training in mixed precision but with more weights and ops in FP16 than in O1. Weights do not implicitly remain in FP32 and are cast to FP16. A copy of the master weights is maintained in the FP32 precision that is used by the optimizer. If we want the normalization layer weights in FP32 then we need to explicitly use layer wrapping to ensure that.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Full FP16: This refers to training in full FP16 where weights and operations are in FP16. FP16  is challenging to enable for training due to convergence issues.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We found that AMP O2 with LayerNorm wrapping in FP32 leads to the best performance without sacrificing accuracy.&lt;/p&gt;

&lt;h4 id=&quot;kernel-fusion&quot;&gt;Kernel Fusion&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;To reduce GPU kernel launch overhead and increase GPU work granularity, we experimented with kernel fusions, including fused dropout and fused layer-norm, using the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xformers library&lt;/a&gt; [7].&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;33-addressing-stability-challenges-by-studying-ops-numerical-stability-and-training-recipes&quot;&gt;3.3 Addressing stability challenges by studying ops numerical stability and training recipes&lt;/h3&gt;

&lt;h4 id=&quot;bfloat16-in-general-but-with-layernorm-in-fp32&quot;&gt;BFloat16 in general but with LayerNorm in FP32&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/tpu/docs/bfloat16&quot;&gt;bfloat16&lt;/a&gt; (BF16) [8] floating-point format provides the same dynamic range as FP32 with a memory footprint identical to FP16. We found that we could train models in the BF16 format using the same set of hyperparameters as in FP32, without special parameter tuning. Nevertheless, we found that we need to keep LayerNorm in FP32 mode in order for the training to converge.&lt;/p&gt;

&lt;h3 id=&quot;34-final-training-recipe&quot;&gt;3.4 Final training recipe&lt;/h3&gt;

&lt;p&gt;A summary of the final training recipe.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Wrap the outer model in an FSDP instance. Enable parameter sharding after the forward pass.&lt;/li&gt;
  &lt;li&gt;Wrap individual ViT blocks with activation checkpointing, nested FSDP wrapping, and parameter flattening.&lt;/li&gt;
  &lt;li&gt;Enable mixed precision mode (AMP O2) with bfloat16 representation. Maintain the optimizer state in FP32 precision to enhance numerical stability.&lt;/li&gt;
  &lt;li&gt;Wrap normalization layers like LayerNorm in FP32 for better numerical stability.&lt;/li&gt;
  &lt;li&gt;Maximize the Nvidia TensorCore utilization by keeping matrix dimensions to be multiple of 8. For More details check &lt;a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf&quot;&gt;Nvidia Tensor Core Performance Guide&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;4-results&quot;&gt;4. Results&lt;/h2&gt;

&lt;p&gt;In this section, we show the scaling results of ViT on three types of tasks: (1) image classification, (2) object detection (3) video understanding. &lt;strong&gt;Our key result is that we are able to train massive ViT backbones across these vision tasks after applying the discussed scaling and optimization techniques. This enables vision research at a much larger scale.&lt;/strong&gt; We trained the models to convergence to verify that we maintain the current baselines even with all the optimizations. A common trend in Figures 2, 3, 4 is that we are able to train up to 25B-param models with an epoch time of less than 4 hours on 128 A100 GPUs. The 60B and 120B models are relatively slower to train.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt; shows the &lt;em&gt;image-classification&lt;/em&gt; scaling result. It plots the epoch time for training ViTs on ImageNet using 128 A100-80GB GPUs with different model sizes.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_2-image-classification-scaling-result.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 2: Image-classification scaling result.&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt; shows the &lt;em&gt;object-detection&lt;/em&gt; scaling result. It plots the epoch time for training &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;&gt;ViTDet&lt;/a&gt; [9] with different ViT backbones on COCO using 128 A100-80GB GPUs.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_3-object-detection-scaling-result.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 3: Object-detection scaling result.&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt; shows the &lt;em&gt;video-understanding&lt;/em&gt; scaling result. It plots the epoch time for training &lt;a href=&quot;https://arxiv.org/abs/2112.01526&quot;&gt;MViTv2&lt;/a&gt; [10] models on &lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;&gt;Kinetics 400&lt;/a&gt; [11] using 128 V100 (32 GB) GPUs in FP32.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_4-video-understanding-scaling-result.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 4: Video-understanding scaling result.&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt; shows the optimization result with the ViT-H model in Figure 2 on 8 A100-40GB GPUs.
Three versions are used: (1) the baseline uses PyTorch’s DDP [12] with AMP O1, (2) FSDP + AMP-O2 + other optimizations, and (3) FSDP + FP16 + other optimizations. These optimizations altogether speed up the training by up to 2.2x.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_5-training-speedups-from-various-optimizations.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 5: Training speedups from various optimizations.&lt;/b&gt;
&lt;/p&gt;

&lt;h2 id=&quot;5-concluding-remarks&quot;&gt;5. Concluding Remarks&lt;/h2&gt;

&lt;p&gt;We have demonstrated the use of PyTorch with FairScale’s FullyShardedDataParallel (FSDP) API in writing large vision transformer models. We discuss our techniques for scaling and optimizing these models on a GPU cluster.  We hope that this article can motivate others to develop large-scale ML models with PyTorch and its ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/abs/2201.08371&quot;&gt;Revisiting Weakly Supervised Pre-Training of Visual Perception Models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://arxiv.org/abs/2010.11929v2&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html&quot;&gt;fairscale.nn.FullyShardedDataParallel&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://pytorch.org/docs/stable/pipeline.html&quot;&gt;Pipeline parallelism in PyTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html#module-torch.amp&quot;&gt;Automatic Mixed Precision (AMP) in PyTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xformers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&quot;https://cloud.google.com/tpu/docs/bfloat16&quot;&gt;The bfloat16 numerical format&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;&gt;Exploring Plain Vision Transformer Backbones for Object Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] &lt;a href=&quot;https://arxiv.org/abs/2112.01526&quot;&gt;MViTv2: Improved Multiscale Vision Transformers for Classification and Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[11] &lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;&gt;https://www.deepmind.com/open-source/kinetics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[12] &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&quot;&gt;Getting Started with Distributed Data Parallel (DDP)&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Vaibhav Aggarwal, Mannat Singh, Anjali Sridhar, Yanghao Li, Shoubhik Debnath, Ronghang Hu, Will Feng, Xinlei Chen, Tingting Markstrum, Diana Liskovich, Anupam Bhatnagar, Chay Ryali, Haoqi Fan, Tete Xiao, Min Xu, Rahul Iyer, Christoph Feichtenhofer, Ross Girshick, Piotr Dollar, Aaron Adcock, Wan-Yen Lo, CK Luk</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR: We demonstrate the use of PyTorch with FairScale’s FullyShardedDataParallel (FSDP) API in writing large vision transformer models. We discuss our techniques for scaling and optimizing these models on a GPU cluster. The goal of this platform scaling effort is to enable research at scale. This blog does not discuss model accuracy, new model architectures, or new training recipes.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Efficient Large-Scale Training with Pytorch FSDP and AWS</title>
      <link href="https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/" rel="alternate" type="text/html" title="Efficient Large-Scale Training with Pytorch FSDP and AWS" />
      <published>2022-12-16T00:00:00-08:00</published>
      <updated>2022-12-16T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/efficient-large-scale-training-with-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/">&lt;p&gt;Cutting-edge AI models are becoming extremely large. The cost and overhead of training these models is increasing rapidly, and involves large amounts of engineering and guesswork to find the right training regime. FSDP reduces these costs significantly by enabling you to train much larger models with the same amount of resources. FSDP lowers the memory footprint on your GPUs, and is usable via a lightweight configuration that requires substantially less effort, typically with just a few lines of code.&lt;/p&gt;

&lt;p&gt;The main performance gains in FSDP come from maximizing the overlap between network communication and model computation, and eliminating the memory redundancy inherent in traditional data parallel training (DDP).  PyTorch FSDP can train models approximately 4x larger on the same server resources as DDP and 20x larger if we combine activation checkpointing and activation offloading.&lt;/p&gt;

&lt;p&gt;Since PyTorch 1.12, FSDP is now in beta status, and has added a number of new features that can be tuned to further accelerate your model training.&lt;/p&gt;

&lt;p&gt;In this series of blog posts, we will explain multiple performance optimizations you can run with FSDP to boost your distributed training speed and model sizes within the context of your available server resources.  We use the HuggingFace T5 3B, 11B and DeepVit, in fine-tuning mode, as the running examples throughout the series.&lt;/p&gt;

&lt;p&gt;As a preview of some of the optimizations discussed in this series, we show the before and after performance scaled in Flops below (Note that these results can vary based on your server resources and model architecture).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_1.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;i&gt; *T5 3B Performance measured on AWS A100 and A10 servers. Original with no optimizations and Tuned with the applied optimization &lt;/i&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_2.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;i&gt; *T5 11B Performance measured on A100 servers. Original with no optimizations and Tuned with the applied optimization &lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In this first post, we will provide a quick overview of FSDP and how it can make training large- scale AI models more efficient.  We will highlight briefly the multiple performance options available, and dive deeper into the details on these in upcoming posts.  We will then conclude with an overview on how to leverage AWS parallel cluster for large- scale training with FSDP.&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimization &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T5 Model &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Throughput Improvement &lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Mixed Precision
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;5x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;10x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Activation Checkpointing (AC)
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;10x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;100x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Transformer Wrapping Policy
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;2x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;Unable to run the experiment without the Transformer wrapping policy.&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Full Shard Strategy
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;1.5x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;Not able to run with Zero2&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Performance optimization gains on T5 models over non-optimized.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In our experiments with the T5 3B model, using the  &lt;a href=&quot;https://www.youtube.com/watch?v=HQeKwCsnH4k&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=2&quot;&gt;transformer wrapping policy&lt;/a&gt; resulted in &amp;gt;2x higher throughput measured in TFLOPS versus the default wrapping policy. &lt;a href=&quot;https://www.youtube.com/watch?v=5B4d0FuxSQc&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=3&quot;&gt;Activation checkpointing&lt;/a&gt; resulted in 10x improvement by reinvesting the freed memory from the checkpoints into larger batch size. &lt;a href=&quot;https://www.youtube.com/watch?v=-caN92JtKqA&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=4&quot;&gt;Mixed precision&lt;/a&gt; with BFloat16 resulted in ~5x improvement versus FP32 and finally the &lt;a href=&quot;https://www.youtube.com/watch?v=a3iW6Cggccw&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=5&quot;&gt;full sharding strategy&lt;/a&gt; versus zero2 (DDP)  resulted in 1.5x improvement.&lt;/p&gt;

&lt;p&gt;We ran similar experiments for a larger model, T5 11B, but the larger model size resulted in some changes to the experiment space.  Specifically, we found that two optimizations,  transformer wrapping policy and activation checkpointing, were needed to enable us to run these experiments on 3 nodes (each node had 8 A100 gpus with 80 GB of memory). With these optimizations, we could fit a batch size of 50 and get higher throughput compared to removing each one of them. Thus rather than running on/off solely for a single optimization test as with the 3B model, the larger model experiments were done with 1 of 3 optimizations turned on/off while always running the other two in order to allow a usable batch size for both test states for each item.&lt;/p&gt;

&lt;p&gt;Based on TFLOP comparisons, with the 11B model, we saw even more payoff from the optimizations.  Mixed precision(~10x improvement) and activation checkpointing (~100x improvement) had a much larger impact with the 11B model compared to the 3B parameter model. With mixed precision we could fit ~2x larger batch sizes and with activation checkpointing &amp;gt;15x batch sizes (from 3 with no activation checkpointing to 50 with activation checkpointing) which translated into large throughput improvements.&lt;/p&gt;

&lt;p&gt;We also have observed that for these larger models &amp;gt; 3B, using Zero2 sharding strategy would result in minimal room left in memory for the batch data, and had to go with very small batch sizes (e.g 1-2) that essentially makes full sharding strategy a necessity to enable fitting larger batches sizes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note - this tutorial assumes a basic understanding of FSDP. To learn more about basics of FSDP please refer to the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;getting started&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html&quot;&gt;advanced FSDP &lt;/a&gt;tutorials.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is FSDP? How does it make Large-Scale Training More Efficient&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FSDP&lt;/strong&gt; expands upon distributed data parallel, by parallelizing not just data, but the model parameters, the optimizer states and gradients associated with the model. Specifically - &lt;strong&gt;each&lt;/strong&gt; &lt;strong&gt;GPU only stores a subset of the entire model&lt;/strong&gt; &lt;strong&gt;and the associated subset of optimizer states and gradients.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;To show the evolution of distributed training, we can start from the beginning, where AI models were simply trained on a single GPU.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;DDP (Distributed Data Parallel) was the initial step up from training with only a single GPU, and was an effort to address the data and model size growth, where multiple GPUs each housed their own copy of the same model. The gain here is that the data for each batch could be split and processed independently on each GPU, all at the same time,thus parallelizing the processing of the data set and increasing training speed by the increasing number of GPUs. The tradeoff is the need to communicate the gradients between each GPU to synchronize the models after the backward pass.&lt;/p&gt;

&lt;p&gt;FSDP expands on scaling models by removing the redundancy of optimizer calculations and state storage, as well as gradient and memory storage of model parameters that are present in DDP (DDP = Distributed Data Parallel). This redundancy reduction, along with increased communication overlap where model parameter communication takes place at the same time as model computation, is what allows FSDP to train much larger models with the same resources as DDP.&lt;/p&gt;

&lt;p&gt;A key point is that this efficiency also allows for AI models that are larger than a single GPU to be trained. The model size available for training is now increased to the aggregate memory of all GPUs, rather than the size of a single GPU. (And as a point of note, FSDP can go beyond aggregated GPU memory by leveraging CPU memory as well, though we will not directly cover this aspect here).&lt;/p&gt;

&lt;p&gt;As discussed in a previous &lt;a href=&quot;https://medium.com/pytorch/pytorch-data-parallel-best-practices-on-google-cloud-6c8da2be180d&quot;&gt;blog post&lt;/a&gt;, with DDP the largest model that we could train on 32, A100 gpus with 40 GB memory (4 nodes) was up to 3B parameters, and batch size of 128, with the help of activation checkpointing. By contrast, using FSDP we were able to train up to 81B model size, combining activation checkpointing, along with activation and parameter offloading. In another &lt;a href=&quot;https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff&quot;&gt;experiment&lt;/a&gt;, we benchmarked a 1T parameter model with FSDP using 512 gpus.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_3.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;For intuition on the parameter level workings of FSDP, below we show an animation detailing how the model parameters are sharded and communicated assuming a two GPU scenario and a simple 8 parameter model:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_5.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Above - the animations walk through the steps involved with the initial sharding of the model amongst ranks, and we start the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gathers&lt;/code&gt; and forward pass&lt;/em&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_6.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We continue through the model with the forward pass. After each FSDP unit completes, non-locally owned params are dropped to free memory, and optionally activations can be checkpointed. This continues until we finish the forward pass and compute the loss.&lt;/em&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_6.5.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;During the backward pass, another &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; is used to load the parameters and the gradients are computed. These gradients are then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce_scattered&lt;/code&gt; so that the local owners of each param can aggregate and prepare to update the weights.&lt;/em&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_7.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Finally, each rank passes the summed gradients through the optimizer states and updates the weights to complete the mini-batch.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the model now distributed across the entire set of available GPUs, the logical question is how data moves through the model given this sharding of model parameters.&lt;/p&gt;

&lt;p&gt;This is accomplished by FSDP coordinating with all GPUs to effectively share (communicate) the respective parts of the model.  The model is decomposed into FSDP units and parameters within each unit are flattened and then sharded across all GPUs.  Within each FSDP unit, GPU’s are assigned interleaving ownership of individual model parameters.&lt;/p&gt;

&lt;p&gt;By interleaving, we mean the following - assuming 2 gpus with an id of 1 and 2, the FSDP unit ownership pattern would be [12121212],  rather than a contiguous chunk of [111222].&lt;/p&gt;

&lt;p&gt;During training, an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; is initiated and the locally owned model parameters within a FSDP unit are shared by the owner GPU with the other non-owners, when they need it, on a ‘just in time’ type basis. FSDP prefetches parameters to overlap &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; communication with computation.&lt;/p&gt;

&lt;p&gt;When those requested parameters arrive, the GPU uses the delivered parameters, in combination with the parameters it already owns, to create a fully populated FSDP unit. Thus there is a moment where each GPU hits peak memory usage while holding a fully populated FSDP unit.&lt;/p&gt;

&lt;p&gt;It then processes the data through the FSDP unit, and drops the parameters it received from other GPU’s to free up memory for the next unit…the process continues over and over proceeding through the entire model to complete the forward pass.The process is then repeated (in general) for the backward pass.(note - this is a simplified version for understanding..there is additional complexity but this should help construct a basic mental model of the FSDP process).&lt;/p&gt;

&lt;p&gt;This eliminates much of the memory redundancy present in DDP, but imposes the cost of higher amounts of network communication to shuttle these requested parameters back and forth amongst all the GPUs.&lt;strong&gt;Overlapping the communication timing with the computation taking place is the basis of many of the performance improvements we’ll discuss in this series.&lt;/strong&gt; The key gains are frequently based on the fact that communication can often take place at the same time as computation.As you can surmise, &lt;strong&gt;having high communication speed is vital for FSDP performance.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-do-i-optimize-my-training-with-fsdp&quot;&gt;&lt;strong&gt;How do I optimize my training with FSDP?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;There are four main performance improvements we will cover - the transformer wrapper, activation checkpointing, mixed precision, and selecting the proper sharding strategy. The flowchart below will help as a checklist for tuning options that we will discuss in this post.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_8.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wrapping policy - &lt;em&gt;for transformers, use Transformer wrapping policy&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first performance optimization is leveraging the FSDP transformer wrapper for transformer models.&lt;/p&gt;

&lt;p&gt;One of the pre-defined wrapping policy is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_autowrap_policy&lt;/code&gt;. With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_autowrap_policy&lt;/code&gt;, FSDP will traverse the module structure from bottom to top, a new FSDP unit will be created once the current unit has at least the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_num_params&lt;/code&gt; specified within the size policy (this defaults to 1e8, or 100M). If the module can not be created as an FSDP unit, FSDP will continue to check its parent module. This size based wrapping policy may not be ideal for some model structures, PyTorch distributed team is actively working on a new default wrapping policy in the next release which is based on size and also module execution order, users can simply tune the size and achieve the optimized performance.&lt;/p&gt;

&lt;p&gt;In the current release, you can greatly improve your performance when running Transformer models by using the ‘transformer wrapper’. You will need to provide the appropriate layer class for your model. Here, layer class is the class that houses the Multi-Head Attention and Feed Forward Network.&lt;/p&gt;

&lt;p&gt;FSDP will then form the FSDP units around the layer class rather than arbitrary breaks based on parameter size. By sharding the model around layer classes that are uniformly repeated within the transformer, FSDP can create uniform FSDP units that better balance the overlap of computation and communication. By contrast, size based wrapping can produce very uneven or skewed shards for models, which then have uneven matching of compute vs communication overlap. As discussed earlier, the main driver of FSDP high performance is the overlap of communication and computation, and hence why the Transformer wrapper provides improved performance. Note that the Transformer wrapper can also be used for non-transformer models if these models have a list of uniform layers.&lt;/p&gt;

&lt;p&gt;Let’s compare the performance difference on a T5, 3B parameter model when running under the default wrapper and the transformer wrapper.&lt;/p&gt;

&lt;p&gt;For default wrapping, we don’t need to take any action - we simply pass the model to FSDP as shown:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;device_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case FSDP will simply wrap the whole model in a single FSDP unit.&lt;/p&gt;

&lt;p&gt;Running on an &lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf&quot;&gt;NVIDIA A100-SXM4–40GB&lt;/a&gt; with 8 GPUs, we are able to reach 2.3 TFlops and 95% GPU memory utilization with a batch size of 14.&lt;/p&gt;

&lt;p&gt;However, since T5 is a transformer model, we are better served to leverage the transformer wrapper for this model.&lt;/p&gt;

&lt;p&gt;To use that, we need to isolate the layer class for the transformer, and then pass it in to create our transformer wrapper.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers.models.t5.modeling_t5&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T5Block&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And now we can create our Transformer wrapper:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;transformer_auto_wrapper_policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;transformer_auto_wrap_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;transformer_layer_cls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;T5Block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# &amp;lt; ---- Your Transformer layer class
&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With our model aware wrapper ready, we can initialize FSDP:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# invoke FSDP with your transformer wrapper policy:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;auto_wrap_policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_auto_wrapper_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;device_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# streaming init
&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running this wrapped model, we can see some substantial performance gains.We can fit nearly double the batch size, going to 28, and with better memory and communication efficiency, we see a TFlops increase to 5.07 from 2.3.&lt;/p&gt;

&lt;p&gt;Thus, we’ve increased our training throughput by over 200% (2.19x) due to providing greater model info to FSDP! The transformer wrapping policy results in more fine-grained and balanced FSDP units each holding a layer class, which leads to a more effective communication-computation overlap.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_9.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Above: Graphical comparison of TFlops based on wrapper type&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you are training a Transformer model, it pays to configure your training with FSDP using the transformer wrapper. For more information on how to isolate your layer class, please see our in depth video on Transformer wrapping &lt;a href=&quot;https://www.youtube.com/watch?v=HQeKwCsnH4k&quot;&gt;here&lt;/a&gt;, where we walk through a number of transformers showing where the layer class can be found.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mixed precision - &lt;em&gt;use BF16 if you have an Ampere architecture GPU&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;FSDP supports a flexible mixed precision policy that gives you granular control over parameters, gradients and buffer data types. This lets you easily leverage BFloat16 or FP16 to increase your training speed by up to 70%.&lt;/p&gt;

&lt;p&gt;*Note that BFloat 16 is only available on Ampere type GPUs. On AWS this is available with p4dn and g5 instances.&lt;/p&gt;

&lt;p&gt;By way of comparison, we can show a 77% speed improvement when comparing fully tuned BFloat16 vs FP32 on an 8B DeepVit model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_10.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We have obtained even greater acceleration using BFloat16 in fine-tuning a 3B HuggingFace T5 model as shown in the figures below. We observed that because of the lower precision the validation loss of BFloat16 is slightly behind in the first few epochs, but it is able to catch up and results in the same final accuracy as FP32.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_10a.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;To use mixed precision, we create a policy with our desired data types, and pass it in during the FSDP initialization.&lt;/p&gt;

&lt;p&gt;To create our policy, we need to import the MixedPrecision class, and then define our custom policy using our customized class:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed.fsdp&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixedPrecision&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bfSixteen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixedPrecision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;param_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;# Gradient communication precision.
&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;reduce_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;# Buffer precision.
&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;buffer_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;auto_wrap_policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_auto_wrapper_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;mixed_precision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloatPolicy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can mix and match the precision for parameters, gradients and buffers as you prefer:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;comboPolicy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixedPrecision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Param precision
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;param_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Gradient communication precision.
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;reduce_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Buffer precision.
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;buffer_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For training with FP16, you will need to also use the ShardedGradScaler, which we will cover in subsequent posts. For BFloat16, it is a drop-in replacement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AnyPrecision Optimizer - &lt;em&gt;going beyond mixed precision with full BF16 training&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mixed precision training, both in FSDP and elsewhere, maintains the working weights in the reduced datatype (BF16 or FP16) while keeping the master weights in full FP32. The reason for the master weights in FP32 is that running in pure BF16 will result in ‘weight stagnation’, where very small weight updates are lost due to the lower precision, and the accuracy flatlines over time while FP32 weights can continue to improve from these small updates.&lt;/p&gt;

&lt;p&gt;In order to resolve this dilemma, we can use the new AnyPrecision optimizer available in &lt;a href=&quot;https://github.com/pytorch/torchdistx&quot;&gt;TorchDistX&lt;/a&gt; (Torch Distributed Experimental) that allows you to successfully train and keep the master weights in pure BF16 instead of FP32. In addition, unlike the typical storage of optimizer states in FP32, AnyPrecision is able to maintain states in pure BF16 as well.&lt;/p&gt;

&lt;p&gt;AnyPrecision enables pure BF16 training by maintaining an extra buffer that tracks the precision lost during the weight updates and re-applies that during the next update…effectively resolving the weight stagnation issue without requiring FP32.&lt;/p&gt;

&lt;p&gt;As a comparison of the throughput gains available with pure BF16 training using AnyPrecision, we ran experiments using FSDP with the T5 11B model with regular FP32 training, Mixed Precision training with BF16, and pure BF16 training using the AnyPrecision optimizer on 3 nodes with A100 gpus as mentioned previously.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_11.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As shown above, training with AnyPrecision and pure BF16 resulted in 2x the throughput vs Mixed Precision, and over 20x improvement vs FP32.&lt;/p&gt;

&lt;p&gt;The potential tradeoff is the impact on final accuracy - in the cases we tested, the accuracy was equal or better than FP32 due to a regularization effect from the slightly reduced precision, but your results may vary.&lt;/p&gt;

&lt;p&gt;AnyPrecision optimizer is available for you to test with &lt;a href=&quot;https://github.com/pytorch/torchdistx&quot;&gt;here&lt;/a&gt;, and is a drop in replacement for AdamW optimizer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Activation checkpointing - &lt;em&gt;increasing throughput by trading compute for memory&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_12.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FSDP supports activation checkpointing once the model has been sharded&lt;/strong&gt;, and makes it easy to implement. The graph above shows ~4x throughput improvement using activation checkpointing.&lt;/p&gt;

&lt;p&gt;Activation checkpointing is where the intermediate activations are freed during the forward pass, and a checkpoint is left as a placeholder. This generally increases available GPU memory by over 30%.&lt;/p&gt;

&lt;p&gt;The tradeoff is that during the backward pass, these previously removed intermediate activations must be re-calculated again using information in the checkpoint (duplicate compute), but by leveraging the increased GPU memory, one can increase the batch size such that the net throughput can increase substantially.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# verify we have FSDP activation support ready by importing:
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed.algorithms._checkpoint.checkpoint_wrapper&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;checkpoint_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;CheckpointImpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;apply_activation_checkpointing_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The steps required to implement activation checkpointing is to first import the FSDP checkpointing functions. We need declare our checkpointer wrapper type which is non-reentrant and create a check function to identify which layer to wrap as follows&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;non_reentrant_wrapper&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checkpoint_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;offload_to_cpu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checkpoint_impl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CheckpointImpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NO_REENTRANT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;check_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;submodule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submodule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T5Block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;apply_activation_checkpointing_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;checkpoint_wrapper_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;non_reentrant_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;check_fn&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Important note - this must be run after the model has been initialized with FSDP.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, hopefully you’ve seen how some initial tuning with FSDP options can have a large impact on your training performance.&lt;/p&gt;

&lt;p&gt;With that, we turn our attention from how to scale within FSDP, to how to scale your server hardware for FSDP using AWS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Large Scale Training with FSDP on AWS - &lt;em&gt;For multi-node prioritize high speed network&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AWS provides several services that can be used to run distributed training with FSDP: &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing&quot;&gt;Amazon EC2 Accelerated Computing instances&lt;/a&gt;, AWS &lt;a href=&quot;https://aws.amazon.com/hpc/parallelcluster/&quot;&gt;ParallelCluster&lt;/a&gt;, and Amazon &lt;a href=&quot;https://aws.amazon.com/sagemaker/features/?nc=sn&amp;amp;loc=2&quot;&gt;Sagemaker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this series of blog posts, we used &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/p4/&quot;&gt;Amazon EC2 p4d&lt;/a&gt; instances in a single-instance multi-GPU configuration and in a multi-instance configuration using AWS &lt;a href=&quot;https://aws.amazon.com/hpc/parallelcluster/&quot;&gt;ParallelCluster&lt;/a&gt; and SageMaker in order to run our training jobs.&lt;/p&gt;

&lt;p&gt;Here, we’ll focus specifically on AWS parallel cluster and provide an overview of how to utilize it for training purposes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AWS ParallelCluster Setup&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AWS ParallelCluster is an open source, cluster management tool that makes it easy for you to deploy and manage High Performance Computing (HPC) clusters on AWS.  AWS ParallelCluster uses yaml configuration files to provision all the necessary resources. It also supports multiple instance types, job submission queues, shared file systems like &lt;a href=&quot;https://aws.amazon.com/efs/?trk=3c5ce89c-8865-47a3-bec3-f6820351aa6d&amp;amp;sc_channel=ps&amp;amp;sc_campaign=acquisition&amp;amp;sc_medium=ACQ-P|PS-GO|Non-Brand|Desktop|SU|Storage|Solution|US|EN|DSA&amp;amp;ef_id=Cj0KCQjwuaiXBhCCARIsAKZLt3l6dtldpE152xuxTMa3mbUbaqtTXwsBdfDRIzCL8cw3NO5DO_y1vOgaAj1pEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!579408162404!!!g!!&quot;&gt;Amazon EFS&lt;/a&gt; (NFS) or &lt;a href=&quot;https://aws.amazon.com/fsx/lustre/?refid=3c5ce89c-8865-47a3-bec3-f6820351aa6d&quot; target=&quot;_blank&quot;&gt;Amazon FSx for Lustre&lt;/a&gt;, and job schedulers like AWS Batch and Slurm.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_13.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Workflow on Clusters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The high level idea is to have a cluster that has a head node which controls the compute nodes. The actual training job runs on the compute nodes. Overall steps to run a training job on a cluster are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set up an AWS ParallelCuster (we discuss below)&lt;/li&gt;
  &lt;li&gt;Connect to the head node, and import the training code/ setup the environment.&lt;/li&gt;
  &lt;li&gt;Pull the data and place it in a shared folder that compute nodes can access (FSx Lustre drive).&lt;/li&gt;
  &lt;li&gt;Run the training job using a job scheduler (in this case Slurm).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Setup AWS ParallelCuster&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To setup AWS ParallelCluster,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Deploy a network stack.&lt;/strong&gt; This step is optional since you could use your account default VPC and let AWS ParallelCluster create your subnets and security groups. However, we prefer to compartmentalize our desired network infrastructure and do this deployment via a CloudFormation stack.&lt;/p&gt;

    &lt;p&gt;Since we deploy a public and a private subnet, we want to create them into an Availability Zone that contains our target instances, in this case p4d. We consult their availability in the region we use (us-east-1) through the following AWS CLI command:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws ec2 describe-instance-type-offerings --location-type availability-zone \ --filters Name=instance-type,Values=p4d.24xlarge --region us-east-1 --output table&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;We see three availability zones containing p4d instances, we pick one of them (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-east-1c&lt;/code&gt;, yours may be different) when deploying our network stack. This can be done with the AWS Console or the AWS CLI. In our case we use the latter as follows&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws cloudformation create-stack --stack-name VPC-Large-Scale --capabilities CAPABILITY_IAM --template-body file://VPC-Large-Scale.yaml --parameters ParameterKey=SubnetsAZ,ParameterValue=us-east-1c&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;CloudFormation will deploy our new VPC, subnets, security groups and endpoints on our behalf. Once done, you can retrieve the IDs of the public and private subnets by querying the stack outputs and the values &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PublicSubnet&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PrivateSubnet&lt;/code&gt;.&lt;/p&gt;

    &lt;p&gt;For example, using the AWS CLI for the private subnet:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws cloudformation describe-stacks --stack-name VPC-Large-Scale --query &quot;Stacks[0].Outputs[?OutputKey=='PrivateSubnet'].OutputValue&quot; --output text&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Create ParallelCluster,&lt;/strong&gt; The cluster configuration file specifies the resources for our cluster. These resources include instance type for Head node, compute nodes, access to S3 buckets, shared storage where our data will be located. We will use Amazon FSx for Lustre that offers a fully managed shared storage service with &lt;a href=&quot;https://en.wikipedia.org/wiki/Lustre_(file_system)&quot;&gt;Lustre&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://github.com/lessw2020/t5_11/blob/main/hpc-cluster/cluster.yaml&quot;&gt;Here&lt;/a&gt; is an example of a cluster configuration file. We can use AWs ParallelCluster CLI to create the cluster. Please note that the private and public subnet IDs will need to be replaced by the ones you retrieved earlier. You will be able to control the cluster using the AWS ParallelCluster CLI to start, stop, pause, etc.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pcluster create-cluster --cluster-name my-hpc-cluster --cluster-configuration cluster.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SSH to Head node -&lt;/strong&gt; once the cluster is ready, we can connect to the Head node using the SSH protocol, pull our training code with and place the data in the shared storage specified in the cluster configuration file.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pcluster ssh --cluster-name cluster -i your-key_pair
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Launch the training job -&lt;/strong&gt; now that we have the data and training code, we can launch the slurm job for training. Here is an &lt;a href=&quot;https://github.com/lessw2020/t5_11/blob/main/hpc-cluster/modified-bert.slurm&quot;&gt;example&lt;/a&gt; of a slurm script to launch the job using torchrun.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More details on how to set up the cluster is out of the scope of this post, however we will have a separate post on it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s next?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With this post we provided a high level overview of FSDP and how it efficiently scales distributed AI training. The flowchart included will help provide a checklist for you to review tuning options discussed such as the transformer wrapper and activation checkpointing.&lt;/p&gt;

&lt;p&gt;In the next posts, we will continue with the T5 model and go deeper into each of the topics above, specifically with sharding strategy and other optimizations to provide more insight and details. For now, a good reference for the sharding strategy is in our video tutorial &lt;a href=&quot;https://www.youtube.com/watch?v=a3iW6Cggccw&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=5&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;If you have questions or find an issue, please find the authors &lt;a href=&quot;https://www.linkedin.com/in/less-wright-22b59017/&quot;&gt;Less&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/hamid-nazeri/&quot;&gt;Hamid&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/geetachauhan/&quot;&gt;Geeta&lt;/a&gt; or open an issue on&lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt; PyTorch github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Special thanks to:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pytorch Distributed team, Shen Li, Rohan Varma, Yanli Zhao, Andrew Gu, Anjali Sridhar, Ana Simoes, Pierre-Yves Aquilanti, Sundar Ranganathan, and the broader AWS team for supporting us with providing infrastructure and technical support for running the large scale experiments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&quot;&gt;FSDP video series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;Getting started with FSDP&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html&quot;&gt;Advanced tutorial on FSDP&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html?highlight=fsdp#module-torch.distributed.fsdp&quot;&gt;API documentation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;style&gt;

    td{
        border: 1px solid black;
    }
    
    article.pytorch-article table tr td:first-of-type{
        padding: 0.3125rem;
    }

    article.pytorch-article table td {
    padding: 0.3125rem;
    }
}

&lt;/style&gt;</content>

      
      
      
      
      

      <author>
          <name>Less Wright, Hamid Shojanazeri, Geeta Chauhan</name>
        
        
      </author>

      

      

      
        <summary type="html">Cutting-edge AI models are becoming extremely large. The cost and overhead of training these models is increasing rapidly, and involves large amounts of engineering and guesswork to find the right training regime. FSDP reduces these costs significantly by enabling you to train much larger models with the same amount of resources. FSDP lowers the memory footprint on your GPUs, and is usable via a lightweight configuration that requires substantially less effort, typically with just a few lines of code.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling PyTorch FSDP for Training Foundation Models on IBM Cloud</title>
      <link href="https://pytorch.org/blog/scaling-pytorch-fsdp-for-training-foundation-models-on-ibm-cloud/" rel="alternate" type="text/html" title="Scaling PyTorch FSDP for Training Foundation Models on IBM Cloud" />
      <published>2022-12-15T00:00:00-08:00</published>
      <updated>2022-12-15T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/scaling-pytorch-fsdp-for-training-foundation-models-on-ibm-cloud</id>
      <content type="html" xml:base="https://pytorch.org/blog/scaling-pytorch-fsdp-for-training-foundation-models-on-ibm-cloud/">&lt;p&gt;Large model training using a cloud native approach is of growing interest for many enterprises given the emergence and success of &lt;a href=&quot;https://research.ibm.com/blog/what-are-foundation-models&quot;&gt;foundation models&lt;/a&gt;. Some AI practitioners may assume that the only way they can achieve high GPU utilization for distributed training jobs is to run them on HPC systems, such as those inter-connected with Infiniband and may not consider Ethernet connected systems. We demonstrate how the latest distributed training technique, Fully Sharded Data Parallel (FSDP) from PyTorch, successfully scales to models of size 10B+ parameters using commodity Ethernet networking in IBM Cloud.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-fsdp-scaling&quot;&gt;PyTorch FSDP Scaling&lt;/h2&gt;

&lt;p&gt;As models get larger, the standard techniques for data parallel training work only if the GPU can hold a full replica of the model, along with its training state (optimizer, activations, etc.). However, GPU memory increases have not kept up with the model size increases and new techniques for training such models have emerged (e.g., Fully Sharded Data Parallel, &lt;a href=&quot;https://www.deepspeed.ai/&quot;&gt;DeepSpeed&lt;/a&gt;), which allow us to efficiently distribute the model and data over multiple GPUs during training. In this blog post, we demonstrate a path to achieve remarkable scaling of model training to 64 nodes (512 GPUs) using PyTorch native FSDP APIs as we increase model sizes to 11B.&lt;/p&gt;

&lt;h3 id=&quot;what-is-fully-sharded-data-parallel&quot;&gt;What is Fully Sharded Data Parallel?&lt;/h3&gt;

&lt;p&gt;FSDP extends the distributed data parallel training (DDP) approach by sharding model parameters, gradient and optimizer states into K FSDP units, determined by using a wrapping policy. FSDP achieves large model training efficiency in terms of resources and performance by significantly reducing the memory footprint on each GPU and overlapping computation and communication.&lt;/p&gt;

&lt;p&gt;Resource efficiency is achieved with memory footprint reduction by having all GPUs own a portion of each FSDP unit. To process a given FSDP unit, all GPUs share their locally owned portion via all_gather communication calls.&lt;/p&gt;

&lt;p&gt;Performance efficiency is accomplished by overlapping all_gather communication calls for upcoming FSDP units with computation of the current FSDP unit. Once the current FSDP unit has been processed, the non-locally owned parameters are dropped, freeing memory for the upcoming FSDP units.  This process achieves training efficiency by the overlap of computation and communication, while also reducing the peak memory needed by each GPU.&lt;/p&gt;

&lt;p&gt;In what follows, we demonstrate how FSDP allows us to keep hundreds of GPUs highly utilized throughout a distributed training job, while running over standard Ethernet networking (system description towards the end of the blog). We chose the T5 architecture for our experiments and leveraged the code from the &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/FSDP_Workshop&quot;&gt;FSDP workshop&lt;/a&gt;. In each of our experiments, we start with a single node experiment to create a baseline and report the metric seconds/iteration normalized by the batch size as well as compute the teraflops based on the &lt;a href=&quot;https://cs.stanford.edu/~matei/papers/2021/sc_megatron_lm.pdf&quot;&gt;Megatron-LM paper&lt;/a&gt; (see Appendix for details of teraflop computation for T5). Our experiments aim to maximize the batch size (while avoiding cudaMalloc retries) to take full advantage of overlap in computation and communications, as discussed below. Scaling is defined as the ratio of the seconds/iteration normalized by batch size for N nodes versus a single node, representing how well we can utilize the additional GPUs as more nodes are added.&lt;/p&gt;

&lt;h3 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h3&gt;

&lt;p&gt;Our first set of experiments using the T5-3B configuration (mixed precision with BF16, activation checkpointing, and transformer wrapping policy) demonstrated scaling efficiency of 95% as we increased the number of GPUs from 8 to 512 (1 to 64 nodes, respectively). We achieved these results without any modifications to the existing FSDP APIs. We observed that, for this scale, over Ethernet based network, there is sufficient bandwidth to enable continuous overlap of communication and computation.&lt;/p&gt;

&lt;p&gt;However, when we increased the T5 model size to 11B, the scaling efficiency declined substantially to 20%. The PyTorch profiler shows that overlap of communication and computation was very limited. Further investigation into the network bandwidth usage revealed that the poor overlap is being caused by latency in the communication of individual packets and not the bandwidth required (in fact, our peak bandwidth utilization is 1/4th of that available). This led us to hypothesize that if we can increase the compute time by increasing the batch size, we can better overlap communication and computation. However, given we are already at maximum GPU memory allocation, we must identify opportunities to rebalance the memory allocation to allow for increase in batch size. We identified that the model state was being allocated a lot more memory than was needed. The primary function of these reservations is to have pre-reserved memory ready to aggressively send/receive tensors during the communication periods and too few buffers can result in increased wait times, whereas too many buffers result in smaller batch sizes.&lt;/p&gt;

&lt;p&gt;To achieve better efficiency, the PyTorch distributed team introduced a new control knob, the rate_limiter which controls how much memory is allocated for send/receive of tensors, alleviating the memory pressure and providing room for higher batch sizes.  In our case, the rate_limiter could increase the batch size from 20 to 50, thus increasing compute time by 2.5x and allowing for much greater overlap of communication and computation. With this fix, we increased the scaling efficiency to &amp;gt;75% (at 32 nodes)!&lt;/p&gt;

&lt;p&gt;Continued investigation into the factors limiting scaling efficiency uncovered that the rate limiter was creating a recurring pipeline bubble of GPU idle time. This was due to the rate limiter using a block and flush approach for the allocation and release of each set of memory buffers. By waiting for the entire block to complete before initiating a new all_gather, the GPU was idling at the start of each block, while waiting for the new set of all_gather parameters to arrive. This bubble was alleviated by moving to a sliding window approach. Upon the completion of a single all_gather step and its computation (rather than a block of them), the memory is freed and the next all_gather is immediately issued in a much more uniform manner.  This improvement eliminated the pipeline bubble and boosted the scaling efficiencies to &amp;gt;90% (at 32 nodes).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-pytorch-fsdp-image1-IBM_scaling_FSDP_visual_new.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 1: Scaling of T5-XL (3B) and T5-XXL (11B) from 1 node to 64 nodes
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-pytorch-fsdp-image2-tflops_per_second_new.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 2: TFLOPs/sec usage for T5-XL(3B) and T5-XXL (11B) as we increase number of nodes
&lt;/p&gt;

&lt;h2 id=&quot;ibm-cloud-ai-system-and-middleware&quot;&gt;IBM Cloud AI System and Middleware&lt;/h2&gt;

&lt;p&gt;The AI infrastructure used for this work is a large-scale AI system on IBM Cloud consisting of nearly 200 nodes, each node with 8 NVIDIA A100 80GB cards, 96 vCPUs, and 1.2TB CPU RAM. The GPU cards within a node are connected via NVLink with a card-to-card bandwidth of 600GBps. Nodes are connected by 2 x 100Gbps Ethernet links with SRIOV based TCP/IP stack, providing a usable bandwidth of 120Gbps.&lt;/p&gt;

&lt;p&gt;The IBM Cloud AI System has been production-ready since May of 2022 and is configured with the OpenShift container platform to run AI workloads. We also built a software stack for production AI workloads that provide end-to-end tools for training workloads. The middleware leverages Ray for pre and post processing workloads and PyTorch for training of models. We also integrate a Kubernetes native scheduler, MCAD, that manages multiple jobs with job queuing, gang scheduling, prioritization, and quota management. A multi-NIC CNI discovers all available network interfaces and handles them as a single NIC pool enabling optimized use of the network interfaces in Kubernetes. Finally, CodeFlare CLI supports a single pane for observability of the full stack using a desktop CLI (e.g., GPU utilization, application metrics like loss, gradient norm).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/scaling-pytorch-fsdp-image3-cli-and-dashboard.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 3: Foundation Model Middleware Stack
&lt;/p&gt;

&lt;h3 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h3&gt;

&lt;p&gt;In conclusion, we demonstrated how we can achieve remarkable scaling of FSDP APIs over non-InfiniBand networks. We identified the bottleneck that had limited scaling to less than 20% efficiency for 11B parameter model training.  After identifying the issue, we were able to correct this with a new rate limiter control to ensure a more optimal balance of reserved memory and communication overlap relative to compute time. With this improvement, we were able to achieve 90% scaling efficiency (a 4.5x improvement), at 256 GPUs and 80% at 512 GPUs for training of the 11B parameter model. In addition, the 3B parameter model scales extremely well with 95% efficiency even as we increase the number of GPUs to 512.&lt;/p&gt;

&lt;p&gt;This is a first in the industry to achieve such scaling efficiencies for up to 11B parameter models using Kubernetes with vanilla Ethernet and PyTorch native FSDP API’s. This improvement enables users to train huge models on a Hybrid Cloud platform in a cost efficient and sustainable manner.&lt;/p&gt;

&lt;p&gt;We plan on continuing to investigate scaling with decoder only models and increasing the size of these models to 100B+ parameters. From a system design perspective, we are exploring capabilities such as RoCE and GDR that can improve latencies of communications over Ethernet networks.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;This blog was possible because of contributions from both PyTorch Distributed and IBM Research teams.&lt;/p&gt;

&lt;p&gt;From the PyTorch Distributed team, we would like to thank Less Wright, Hamid Shojanazeri, Geeta Chauhan, Shen Li, Rohan Varma, Yanli Zhao, Andrew Gu, Anjali Sridhar, Chien-Chin Huang, and Bernard Nguyen.&lt;/p&gt;

&lt;p&gt;From the IBM Research team, we would like to thank Linsong Chu, Sophia Wen, Lixiang (Eric) Luo, Marquita Ellis, Davis Wertheimer, Supriyo Chakraborty, Raghu Ganti, Mudhakar Srivatsa, Seetharami Seelam, Carlos Costa, Abhishek Malvankar, Diana Arroyo, Alaa Youssef, Nick Mitchell.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h4 id=&quot;teraflop-computation&quot;&gt;Teraflop computation&lt;/h4&gt;

&lt;p&gt;The T5-XXL (11B) architecture has two types of T5 blocks, one is an encoder and the second is a decoder. Following the approach of Megatron-LM, where each matrix multiplication requires 2m×k×n FLOPs, where the first matrix is of size m×k and the second is k×n. The encoder block consists of self-attention and feed forward layers, whereas the decoder block consists of self-attention, cross-attention, and feed forward layers.&lt;/p&gt;

&lt;p&gt;The attention (both self and cross) block consists of a QKV projection, which requires 6Bsh&lt;sup&gt;2&lt;/sup&gt; operations, an attention matrix computation requiring 2Bs&lt;sup&gt;2&lt;/sup&gt;h operations, an attention over values which needs 2Bs&lt;sup&gt;2&lt;/sup&gt;h computations, and the post-attention linear projection requires 2Bsh&lt;sup&gt;2&lt;/sup&gt; operations. Finally, the feed forward layer requires 15Bsh&lt;sup&gt;2&lt;/sup&gt; operations.&lt;/p&gt;

&lt;p&gt;The total for an encoder block is 23Bsh&lt;sup&gt;2&lt;/sup&gt;+4Bs&lt;sup&gt;2&lt;/sup&gt;h, whereas for a decoder block, it comes to 31Bsh&lt;sup&gt;2&lt;/sup&gt;+8Bs&lt;sup&gt;2&lt;/sup&gt;h. With a total of 24 encoder and 24 decoder blocks and 2 forward passes (as we discard the activations) and one backward pass (equivalent to two forward passes), the final FLOPs computation comes to be 96×(54Bsh&lt;sup&gt;2&lt;/sup&gt;+ 12Bs&lt;sup&gt;2&lt;/sup&gt;h) + 6BshV. Here, B is the batch size per GPU, s is sequence length, h is hidden state size, and V is vocabulary size. 
We repeat a similar computation for T5-XL (3B) architecture, which is slightly different.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Linsong Chu, Less Wright, Hamid Shojanazeri, Sophia Wen, Raghu Ganti, Geeta Chauhan</name>
        
        
      </author>

      

      

      
        <summary type="html">Large model training using a cloud native approach is of growing interest for many enterprises given the emergence and success of foundation models. Some AI practitioners may assume that the only way they can achieve high GPU utilization for distributed training jobs is to run them on HPC systems, such as those inter-connected with Infiniband and may not consider Ethernet connected systems. We demonstrate how the latest distributed training technique, Fully Sharded Data Parallel (FSDP) from PyTorch, successfully scales to models of size 10B+ parameters using commodity Ethernet networking in IBM Cloud.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Hugging Face and TIMM models with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/" rel="alternate" type="text/html" title="Accelerating Hugging Face and TIMM models with PyTorch 2.0" />
      <published>2022-12-02T00:00:00-08:00</published>
      <updated>2022-12-02T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/">&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; makes it easy to experiment with different compiler backends to make PyTorch code faster with a single line decorator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;. It works either directly over an nn.Module as a drop-in replacement for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.jit.script()&lt;/code&gt; but without requiring you to make any source code changes. We expect this one line code change to provide you with between 30%-2x training time speedups on the vast majority of models that you’re already running.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;n&quot;&gt;opt_module&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;torch.compile supports arbitrary PyTorch code, control flow, mutation and comes with experimental support for dynamic shapes. We’re so excited about this development that we call it PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;What makes this announcement different for us is we’ve already benchmarked some of the most popular open source PyTorch models and gotten substantial speedups ranging from 30% to 2x &lt;a href=&quot;https://github.com/pytorch/torchdynamo/issues/681&quot;&gt;https://github.com/pytorch/torchdynamo/issues/681&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are no tricks here, we’ve pip installed popular libraries like &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;https://github.com/huggingface/transformers&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/accelerate&quot;&gt;https://github.com/huggingface/accelerate&lt;/a&gt; and &lt;a href=&quot;https://github.com/rwightman/pytorch-image-models&quot;&gt;https://github.com/rwightman/pytorch-image-models&lt;/a&gt; and then ran torch.compile() on them and that’s it.&lt;/p&gt;

&lt;p&gt;It’s rare to get both performance and convenience, but this is why the core team finds PyTorch 2.0 so exciting. The Hugging Face team is also excited, in their words:&lt;/p&gt;

&lt;p&gt;Ross Wightman the primary maintainer of TIMM: “PT 2.0 works out of the box with majority of timm models for inference and train workloads and no code changes”&lt;/p&gt;

&lt;p&gt;Sylvain Gugger the primary maintainer of transformers and accelerate: “With just one line of code to add, PyTorch 2.0 gives a speedup between 1.5x and 2.x in training Transformers models. This is the most exciting thing since mixed precision training was introduced!”&lt;/p&gt;

&lt;p&gt;This tutorial will show you exactly how to replicate those speedups so you can be as excited as to PyTorch 2.0 as we are.&lt;/p&gt;

&lt;h2 id=&quot;requirements-and-setup&quot;&gt;Requirements and Setup&lt;/h2&gt;

&lt;p&gt;For GPU (newer generation GPUs will see drastically better performance)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install numpy --pre torch --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu117

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For CPU&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip3 install --pre torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optional: Verify Installation&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/pytorch/pytorch
cd tools/dynamo
python verify_dynamo.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Optional: Docker installation&lt;/p&gt;

&lt;p&gt;We also provide all the required dependencies in the PyTorch nightly
binaries which you can download with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull ghcr.io/pytorch/pytorch-nightly

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And for ad hoc experiments just make sure that your container has access
to all your GPUs&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run --gpus all -it ghcr.io/pytorch/pytorch-nightly:latest /bin/bash

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;

&lt;h3 id=&quot;a-toy-exmaple&quot;&gt;a toy exmaple&lt;/h3&gt;

&lt;p&gt;Let’s start with a simple example and make things more complicated step
by step. Please note that you’re likely to see more significant speedups the newer your GPU is.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;new_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inductor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This example won’t actually run faster but it’s educational.&lt;/p&gt;

&lt;p&gt;example that features &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cos()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.sin()&lt;/code&gt; which are examples of pointwise ops as in they operate element by element on a vector. A more famous pointwise op you might actually want to use would be something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.relu()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Pointwise ops in eager mode are suboptimal because each one would need to read a tensor from memory, make some changes and then write back those changes.&lt;/p&gt;

&lt;p&gt;The single most important optimization that PyTorch 2.0 does for you is fusion.&lt;/p&gt;

&lt;p&gt;So back to our example we can turn 2 reads and 2 writes into 1 read and 1 write which is crucial especially for newer GPUs where the bottleneck is memory bandwidth (how quickly you can send data to a GPU) instead of compute (how quickly your GPU can crunch floating point operations)&lt;/p&gt;

&lt;p&gt;The second most important optimization that PyTorch 2.0 does for you is CUDA graphs&lt;/p&gt;

&lt;p&gt;CUDA graphs help eliminate the overhead from launching individual kernels from a python program.&lt;/p&gt;

&lt;p&gt;torch.compile() supports many different backends but one that we’re particularly excited about is Inductor which generates Triton kernels &lt;a href=&quot;https://github.com/openai/triton&quot;&gt;https://github.com/openai/triton&lt;/a&gt; which are written in Python yet outperform the vast majority of handwritten CUDA kernels. Suppose our example above was called trig.py we can actually inspect the code generated triton kernels by running.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TORCHINDUCTOR_TRACE=1 python trig.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pointwise&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size_hints&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__file__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'signature'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'*fp32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'*fp32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'i32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'device'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'constants'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'configs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;instance_descriptor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;divisible_by_16&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;equal_to_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())]})&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triton&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_ptr0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_ptr0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xnumel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XBLOCK&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constexpr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xnumel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xoffset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;program_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XBLOCK&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xindex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xoffset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XBLOCK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XBLOCK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xmask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xindex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xnumel&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xindex&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tmp0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_ptr0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tmp1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tmp2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;store&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_ptr0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XBLOCK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And you can verify that fusing the two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sins&lt;/code&gt; did actually occur because the two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sin&lt;/code&gt; operations occur within a single Triton kernel and the temporary variables are held in registers with very fast access.&lt;/p&gt;

&lt;h3 id=&quot;a-real-model&quot;&gt;a real model&lt;/h3&gt;

&lt;p&gt;As a next step let’s try a real model like resnet50 from the PyTorch hub.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pytorch/vision:v0.10.0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'resnet18'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inductor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you actually run you may be surprised that the first run is slow and that’s because the model is being compiled. Subsequent runs will be faster so it’s common practice to warm up your model before you start benchmarking it.&lt;/p&gt;

&lt;p&gt;You may have noticed how we also passed in the name of a compiler explicitly here with “inductor” but it’s not the only available backend, you can run in a REPL &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch._dynamo.list_backends()&lt;/code&gt; to see the full list of available backends. For fun you should try out &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aot_cudagraphs&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvfuser&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;hugging-face-models&quot;&gt;Hugging Face models&lt;/h3&gt;

&lt;p&gt;Let’s do something a bit more interesting now, our community frequently
uses pretrained models from transformers &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;https://github.com/huggingface/transformers&lt;/a&gt; or TIMM &lt;a href=&quot;https://github.com/rwightman/pytorch-image-models&quot;&gt;https://github.com/rwightman/pytorch-image-models&lt;/a&gt; and one of our design goals for PyTorch 2.0 was that any new compiler stack needs to work out of the box with the vast majority of models people actually run.&lt;/p&gt;

&lt;p&gt;So we’re going to directly download a pretrained model from the Hugging Face hub and optimize it&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Copy pasted from here https://huggingface.co/bert-base-uncased
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert-base-uncased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert-base-uncased&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# This is the only line of code that we changed
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Replace me by any text you'd like.&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoded_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you remove the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;to(device=&quot;cuda:0&quot;)&lt;/code&gt; from the model and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;encoded_input&lt;/code&gt; then PyTorch 2.0 will generate C++ kernels that will be optimized for running on your CPU. You can inspect both Triton or C++ kernels for BERT, they’re obviously more complex than the trigonometry example we had above but you can similarly skim it and understand if you understand PyTorch.&lt;/p&gt;

&lt;p&gt;The same code also works just fine if used with &lt;a href=&quot;https://github.com/huggingface/accelerate&quot;&gt;https://github.com/huggingface/accelerate&lt;/a&gt; and DDP&lt;/p&gt;

&lt;p&gt;Similarly let’s try out a TIMM example&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;timm&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'resnext101_32x8d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inductor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our goal with PyTorch was to build a breadth-first compiler that would speed up the vast majority of actual models people run in open source. The Hugging Face Hub ended up being an extremely valuable benchmarking tool for us, ensuring that any optimization we work on actually helps accelerate models people want to run.&lt;/p&gt;

&lt;p&gt;So please try out PyTorch 2.0, enjoy the free perf and if you’re not seeing it then please open an issue and we will make sure your model is supported &lt;a href=&quot;https://github.com/pytorch/torchdynamo/issues&quot;&gt;https://github.com/pytorch/torchdynamo/issues&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After all, we can’t claim we’re created a breadth-first unless YOUR models actually run faster.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Mark Saroufim</name>
        
        
      </author>

      

      

      
        <summary type="html">torch.compile() makes it easy to experiment with different compiler backends to make PyTorch code faster with a single line decorator torch.compile(). It works either directly over an nn.Module as a drop-in replacement for torch.jit.script() but without requiring you to make any source code changes. We expect this one line code change to provide you with between 30%-2x training time speedups on the vast majority of models that you’re already running.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Get Started with PyTorch 2.0 Summary and Overview</title>
      <link href="https://pytorch.org/blog/getting-started-with-pytorch-2.0/" rel="alternate" type="text/html" title="Get Started with PyTorch 2.0 Summary and Overview" />
      <published>2022-12-02T00:00:00-08:00</published>
      <updated>2022-12-02T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/getting-started-with-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/getting-started-with-pytorch-2.0/">&lt;p&gt;Introducing PyTorch 2.0, our first steps toward the next generation 2-series release of PyTorch. Over the last few years we have innovated and iterated from PyTorch 1.0 to the most recent 1.13 and moved to the newly formed PyTorch Foundation, part of the Linux Foundation.&lt;/p&gt;

&lt;p&gt;To complement the PyTorch 2.0 announcement and conference, we have also posted a comprehensive introduction and technical overview within the Get Started menu at &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0&quot;&gt;https://pytorch.org/get-started/pytorch-2.0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also wanted to ensure you had all the information to quickly leverage PyTorch 2.0 in your models so we added the technical requirements, tutorial, user experience, Hugging Face benchmarks and FAQs to get you started today!&lt;/p&gt;

&lt;p&gt;Finally we are launching a new “Ask the Engineers: 2.0 Live Q&amp;amp;A” series that allows you to go deeper on a range of topics with PyTorch subject matter experts. We hope this content is helpful for the entire community and level of users/contributors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0&quot;&gt;https://pytorch.org/get-started/pytorch-2.0&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">Introducing PyTorch 2.0, our first steps toward the next generation 2-series release of PyTorch. Over the last few years we have innovated and iterated from PyTorch 1.0 to the most recent 1.13 and moved to the newly formed PyTorch Foundation, part of the Linux Foundation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Optimizing Production PyTorch Models’ Performance with Graph Transformations</title>
      <link href="https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/" rel="alternate" type="text/html" title="Optimizing Production PyTorch Models’ Performance with Graph Transformations" />
      <published>2022-11-28T00:00:00-08:00</published>
      <updated>2022-11-28T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations</id>
      <content type="html" xml:base="https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;PyTorch supports two execution modes [1]: eager mode and graph mode. In eager mode, operators in a model are immediately executed as they are encountered. In contrast, in graph mode, operators are first synthesized into a graph, which will then be compiled and executed as a whole. Eager mode is easier to use, more suitable for ML researchers, and hence is the default mode of execution. On the other hand, graph mode typically delivers higher performance and hence is heavily used in production.&lt;/p&gt;

&lt;p&gt;Specifically, graph mode enables operator fusion [2], wherein one operator is merged with another to reduce/localize memory reads as well as total kernel launch overhead. Fusion can be horizontal—taking a single operation (e.g., BatchNorm) that is independently applied to many operands and merging those operands into an array; and vertical—merging a kernel with another kernel that consumes the output of the first kernel (e.g., Convolution followed by ReLU).&lt;/p&gt;

&lt;p&gt;Torch.FX [3, 4] (abbreviated as FX) is a publicly available toolkit as part of the PyTorch package that supports graph mode execution. In particular, it (1) captures the graph from a PyTorch program and (2) allows developers to write transformations on the captured graph. It is used inside Meta to optimize the training throughput of production models. By introducing a number of FX-based optimizations developed at Meta, we demonstrate the approach of using graph transformation to optimize PyTorch’s performance for production.&lt;/p&gt;

&lt;h2 id=&quot;2-background&quot;&gt;2. Background&lt;/h2&gt;

&lt;p&gt;Embedding tables are ubiquitous in recommendation systems. Section 3 will discuss three FX transformations that optimize accesses to embedding tables. In this section, we provide some background on FX (Section 2.1) and embedding tables (Section 2.2).&lt;/p&gt;

&lt;h3 id=&quot;21-fx&quot;&gt;2.1 FX&lt;/h3&gt;

&lt;p&gt;Figure 1 is a simple example adopted from [3] which illustrates using FX to transform a PyTorch program. It contains three steps: (1) capturing the graph from a program, (2) modifying the graph (in this example, all uses of RELU are replaced by GELU), and (3) generating a new program from the modified graph.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-1.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1: A FX example which replaces all uses of RELU by GELU in a PyTorch module.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The FX API [4] provides many more functionalities for inspecting and transforming PyTorch program graphs.&lt;/p&gt;

&lt;h3 id=&quot;22-embedding-tables&quot;&gt;2.2 Embedding Tables&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-2.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2: Illustration of an embedding table for a sparse feature with batch size = 1&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a recommendation system, sparse features (e.g., User ID, Story ID) are represented by embedding tables. An embedding table E is an HxD matrix, where H is the hash size, D is the embedding dimension. Each row of E is a vector of floats. Feature hashing [5] is used to map a sparse feature to a list of indices to E, say [S&lt;sub&gt;1&lt;/sub&gt;,S&lt;sub&gt;2&lt;/sub&gt;, …, S&lt;sub&gt;k&lt;/sub&gt;], where 0&amp;lt;=S&lt;sub&gt;i&lt;/sub&gt;&amp;lt;H. Its output value is computed as f(E[S&lt;sub&gt;1&lt;/sub&gt;], E[S&lt;sub&gt;2&lt;/sub&gt;], …, E[S&lt;sub&gt;k&lt;/sub&gt;]), where E[S&lt;sub&gt;i&lt;/sub&gt;] is the vector at row S&lt;sub&gt;i&lt;/sub&gt;, and f is called the pooling function, which is typically one of the following functions: sum, average, maximum. See Figure 2 for an illustration.&lt;/p&gt;

&lt;p&gt;To fully utilize the GPU, sparse features are usually processed in a batch. Each entity in a batch has its own list of indices. If a batch has B entities, a naive representation has B lists of indices. A more compact representation is to combine the B lists of indices into a single list of indices and add a list of the lengths of indices (one length for each entity in the batch). For example, if a batch has 3 entities whose lists of indices are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Entity 1: indices = [10, 20]&lt;/li&gt;
  &lt;li&gt;Entity 2: indices = [5, 9, 77, 81]&lt;/li&gt;
  &lt;li&gt;Entity 3: indices = [15, 20, 45]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then the indices and lengths for the entire batch will be:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Indices = [10, 20, 5, 9, 77, 81, 15, 20, 45]&lt;/li&gt;
  &lt;li&gt;Lengths = [2, 4, 3]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And the output of the embedding table lookup for the whole batch is a BxD matrix.&lt;/p&gt;

&lt;h2 id=&quot;3-three-fx-transformations&quot;&gt;3. Three FX Transformations&lt;/h2&gt;

&lt;p&gt;We have developed three FX transformations that accelerate accesses to embedding tables. Section 3.1 discusses a transformation that combines multiple small input tensors into a single big tensor; Section 3.2 a transformation that fuses multiple, parallel compute chains into a single compute chain; and Section 3.3 a transformation that overlaps communication with computation.&lt;/p&gt;

&lt;h3 id=&quot;31-combining-input-sparse-features&quot;&gt;3.1 Combining Input Sparse Features&lt;/h3&gt;

&lt;p&gt;Recall that an input sparse feature in a batch is represented by two lists: a list of indices and a list of B lengths, where B is the batch size. In PyTorch, these two lists are implemented as two tensors. When a PyTorch model is run on a GPU, embedding tables are commonly stored in the GPU memory (which is closer to the GPU and has much higher read/write bandwidth than the CPU memory). To use an input sparse feature, its two tensors need to be first copied from CPU to GPU. Nevertheless, per host-to-device memory copying requires a kernel launch, which is relatively expensive compared to the actual data transfer time. If a model uses many input sparse features, this copying could become a performance bottleneck (e.g., 1000 input sparse features would require copying 2000 tensors from host to device).&lt;/p&gt;

&lt;p&gt;An optimization that reduces the number of host-to-device memcpy is to combine multiple input sparse features before sending them to the device. For instance, given the following three input features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feature_A: indices = [106, 211, 7], lengths = [2, 1]&lt;/li&gt;
  &lt;li&gt;Feature_B: indices = [52, 498, 616, 870, 1013], lengths = [3, 2]&lt;/li&gt;
  &lt;li&gt;Feature_C: indices = [2011, 19, 351, 790], lengths = [1, 3]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The combined form is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Features_A_B_C: indices = [106, 211, 7, 52, 498, 616, 870, 1013, 2011, 19, 351, 790], lengths = [2, 1, 3, 2, 1, 3]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, instead of copying 3x2=6 tensors from host to device, we only need to copy 2 tensors.&lt;/p&gt;

&lt;p&gt;Figure 3(b) describes an implementation of this optimization, which has two components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On the CPU side: The input pipeline is modified to combine all the indices of sparse features into a single tensor and similarly all the lengths into another tensor. Then the two tensors are copied to the GPU.&lt;/li&gt;
  &lt;li&gt;On the GPU side: Using FX, we insert a Permute_and_Split op into the model graph to recover the indices and lengths tensors of individual features from the combined tensors, and route them to the corresponding nodes downstream.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-3a.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(a). &lt;strong&gt;Without the optimization&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-3b.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(b). &lt;strong&gt;With the optimization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3: Combining input sparse features&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-horizontal-fusion-of-computation-chains-started-with-accesses-to-embedding-tables&quot;&gt;3.2 Horizontal fusion of computation chains started with accesses to embedding tables&lt;/h3&gt;

&lt;p&gt;In a production model, it is fairly common to have 10s of embedding tables residing on each GPU. For performance reasons, lookups to these tables are grouped together so that their outputs are concatenated in a single big tensor (see the red part in Figure 4(a)). To apply computations to individual feature outputs, a Split op is used to divide the big tensors into N smaller tensors (where N is the number of features) and then the desired computations are applied to each tensor. This is shown in Figure 4(a), where the computation applied to each feature output O is Tanh(LayerNorm(O)). All the computation results are concatenated back to a big tensor, which is then passed to downstream ops (Op1 in Figure 4(a)).&lt;/p&gt;

&lt;p&gt;The main runtime cost here is the GPU kernel launch overhead. For instance, the number of GPU kernel launches in Figure 4(a) is 2*N + 3 (each oval in the figure is a GPU kernel). This could become a performance issue because execution times of LayerNorm and Tanh on the GPU are short compared to their kernel launch times. In addition, the Split op may create an extra copy of the embedding output tensor, consuming additional GPU memory.&lt;/p&gt;

&lt;p&gt;We use FX to implement an optimization called horizontal fusion which dramatically reduces the number of GPU kernel launches (in this example, the optimized number of GPU kernel launches is 5, see Figure 4(b)). Instead of doing an explicit Split, we use the Add_middle_dim op to reshape the 2D embedding tensor of shape (B, NxD) to a 3D tensor of shape (B, N, D). Then a single LayerNorm is applied to the last dimension of it. Then a single Tanh is applied to the result of the LayerNorm. At the end, we use the Remove_middle_dim op to reshape the Tanh’s result back to a 2D tensor. In addition, since Add_middle_dim and Remove_middle_dim only reshape the tensor without creating an extra copy, the amount of GPU memory consumption could be reduced as well.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-4a.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(a). &lt;strong&gt;Without the optimization&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-4b.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;(b). &lt;strong&gt;With the optimization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4: Horizontal fusion&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;33-overlapping-computation-with-communication&quot;&gt;3.3 Overlapping Computation with Communication&lt;/h3&gt;

&lt;p&gt;Training of a production recommendation model is typically done on a distributed GPU system. Since the capacity of the device memory per GPU is not big enough to hold all the embedding tables in the model, they need to be distributed among the GPUs.&lt;/p&gt;

&lt;p&gt;Within a training step, a GPU needs to read/write feature values from/to the embedding tables on the other GPUs. This is known as all-to-all communication [6] and can be a major performance bottleneck.&lt;/p&gt;

&lt;p&gt;We use FX to implement a transformation that can overlap computation with all-to-all communication. Figure 5(a) shows the example of a model graph which has embedding table accesses (EmbeddingAllToAll) and other ops. Without any optimization, they are sequentially executed on a GPU stream, as shown in Figure 5(b). Using FX, we break EmbeddingAllToAll into EmbeddingAllToAll_Request and EmbeddingAllToAll_Wait, and schedule independent ops in between them.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-5a.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a) Model graph&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-5b.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(b) Original execution order&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/blog1-fig-5c.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(c)Optimized execution order&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5: Overlapping Computation with Communication&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;34-summary&quot;&gt;3.4 Summary&lt;/h3&gt;

&lt;p&gt;Table 1 summarizes the optimizations discussed in this section and the corresponding performance bottlenecks addressed.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimization&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Performance Bottleneck Addressed&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Combining Input Sparse Features
   &lt;/td&gt;
   &lt;td&gt;Host-to-device memory copy
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Horizontal fusion
   &lt;/td&gt;
   &lt;td&gt;GPU kernel launch overhead
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Overlapping Computation with Communication
   &lt;/td&gt;
   &lt;td&gt;Embedding all-to-all access time
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 1: Summary of the optimizations and the performance bottlenecks addressed&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We have also developed other FX transformations which are not discussed in this section due to space limitations.&lt;/p&gt;

&lt;p&gt;To discover which models would benefit from these transformations, we analyzed the performance data collected by MAIProf [7] from the models that run at Meta’s data centers. Altogether, these transformations provide up to 2-3x of speedups compared to eager mode on a set of production models.&lt;/p&gt;

&lt;h2 id=&quot;4-concluding-remarks&quot;&gt;4. Concluding Remarks&lt;/h2&gt;

&lt;p&gt;The graph mode in PyTorch is preferred over the eager mode for production use for performance reasons. FX is a powerful tool for capturing and optimizing the graph of a PyTorch program. We demonstrate three FX transformations that are used to optimize production recommendation models inside Meta. We hope that this blog can motivate other PyTorch model developers to use graph transformations to boost their models’ performance.&lt;/p&gt;

&lt;p&gt;References&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://pytorch.org/features/&quot;&gt;End-to-end Machine Learning Framework&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/abs/2108.13342&quot;&gt;DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://arxiv.org/pdf/2112.08429.pdf&quot;&gt;Torch.FX: Practical Program Capture and Transformation for Deep Learning In Python&lt;/a&gt;, MLSys 2022.&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;Torch.fx—PyTorch 1.12 documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://alex.smola.org/papers/2009/Weinbergeretal09.pdf&quot;&gt;Feature Hashing for Large Scale Multitask Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/&quot;&gt;NVIDIA Collective Communication Library Documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&quot;https://pytorch.org/blog/performance-debugging-of-production-pytorch-models-at-meta/&quot;&gt;Performance Debugging of Production PyTorch Models at Meta&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jade Nie, CK Luk, Xiaodong Wang, Jackie (Jiaqi) Xu</name>
        
        
      </author>

      

      

      
        <summary type="html">1. Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Efficient Multi-Objective Neural Architecture Search with Ax</title>
      <link href="https://pytorch.org/blog/effective-multi-objective-nueral-architecture/" rel="alternate" type="text/html" title="Efficient Multi-Objective Neural Architecture Search with Ax" />
      <published>2022-11-22T00:00:00-08:00</published>
      <updated>2022-11-22T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/effective-multi-objective-nueral-architecture</id>
      <content type="html" xml:base="https://pytorch.org/blog/effective-multi-objective-nueral-architecture/">&lt;h2 id=&quot;tldr&quot;&gt;tl;dr&lt;/h2&gt;

&lt;p&gt;Multi-Objective Optimization in Ax enables efficient exploration of tradeoffs (e.g. between model performance and model size or latency) in Neural Architecture Search. This method has been successfully applied at Meta for a variety of products such as On-Device AI. In this post, we provide an &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ax_multiobjective_nas_tutorial.html&quot;&gt;end-to-end&lt;/a&gt; tutorial that allows you to try it out yourself.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Neural networks continue to grow in both size and complexity. Developing state-of-the-art architectures is often a cumbersome and time-consuming process that requires both domain expertise and large engineering efforts. In an attempt to overcome these challenges, several Neural Architecture Search (NAS) approaches have been proposed to automatically design well-performing architectures without requiring a human in-the-loop.&lt;/p&gt;

&lt;p&gt;Despite being very sample-inefficient, naïve approaches like random search and grid search are still popular for both hyperparameter optimization and NAS (a &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-02447823/document&quot;&gt;study&lt;/a&gt; conducted at NeurIPS 2019 and ICLR 2020 found that 80% of NeurIPS papers and 88% of ICLR papers tuned their ML model hyperparameters using manual tuning, random search, or grid search). But as models are often time-consuming to train and may require large amounts of computational resources, minimizing the number of configurations that are evaluated is important.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ax.dev/&quot;&gt;Ax&lt;/a&gt; is a general tool for black-box optimization that allows users to explore large search spaces in a sample-efficient manner using &lt;a href=&quot;http://proceedings.mlr.press/v133/turner21a/turner21a.pdf&quot;&gt;state-of-the art algorithms such as Bayesian Optimization&lt;/a&gt;. At Meta, Ax is used in a variety of domains, including hyperparameter tuning, NAS, identifying optimal product settings through large-scale A/B testing, infrastructure optimization, and designing cutting-edge AR/VR hardware.&lt;/p&gt;

&lt;p&gt;In many NAS applications, there is a natural tradeoff between multiple metrics of interest. For instance, when deploying models on-device we may want to maximize model performance (e.g., accuracy), while simultaneously minimizing competing metrics such as power consumption, inference latency, or model size, in order to satisfy deployment constraints. In many cases, we have been able to reduce computational requirements or latency of predictions substantially by accepting a small degradation in model performance (in some cases we were able to both increase accuracy and reduce latency!). Principled methods for exploring such tradeoffs efficiently are key enablers of &lt;a href=&quot;https://arxiv.org/abs/2111.00364&quot;&gt;Sustainable AI&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;At Meta, we have successfully used &lt;a href=&quot;https://research.facebook.com/blog/2021/07/optimizing-model-accuracy-and-latency-using-bayesian-multi-objective-neural-architecture-search/&quot;&gt;multi-objective Bayesian NAS&lt;/a&gt; in Ax to explore such tradeoffs. Our methodology is being used routinely for optimizing AR/VR on-device ML models. Beyond NAS applications, we have also developed &lt;a href=&quot;https://arxiv.org/pdf/2109.10964.pdf&quot;&gt;MORBO&lt;/a&gt; which is a method for high-dimensional multi-objective optimization that can be used to optimize optical systems for augmented reality (AR).&lt;/p&gt;

&lt;h2 id=&quot;fully-automated-multi-objective-nas-with-ax&quot;&gt;Fully automated Multi-Objective NAS with Ax&lt;/h2&gt;

&lt;p&gt;Ax’s Scheduler allows running experiments asynchronously in a closed-loop fashion by continuously deploying trials to an external system, polling for results, leveraging the fetched data to generate more trials, and repeating the process until a stopping condition is met. No human intervention or oversight is required. Features of the Scheduler include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Customizability of parallelism, failure tolerance, and many other settings;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A large selection of state-of-the-art optimization algorithms;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Saving in-progress experiments (to a SQL DB or json) and resuming an experiment from storage;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Easy extensibility to new backends for running trial evaluations remotely.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following illustration from the &lt;a href=&quot;https://ax.dev/tutorials/scheduler.html&quot;&gt;Ax scheduler tutorial&lt;/a&gt; summarizes how the scheduler interacts with any external system used to run trial evaluations:&lt;/p&gt;

&lt;!-- image goes here  --&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/MOO-NAS-blog-img1-ax_scheduler_illustration.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;To run automated NAS with the Scheduler, the main things we need to do are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Define a &lt;a href=&quot;https://github.com/facebook/Ax/blob/main/ax/core/runner.py#L21&quot;&gt;Runner&lt;/a&gt;, which is responsible for sending off a model with a particular architecture to be trained on a platform of our choice (like Kubernetes, or maybe just a Docker image on our local machine). In the tutorial below, we use TorchX for handling deployment of training jobs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Define a &lt;a href=&quot;https://github.com/facebook/Ax/blob/main/ax/core/metric.py#L21&quot;&gt;Metric&lt;/a&gt;, which is responsible for fetching the objective metrics (such as accuracy, model size, latency) from the training job. In our tutorial, we use Tensorboard to log data, and so can use the Tensorboard metrics that come bundled with Ax.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tutorial&quot;&gt;Tutorial&lt;/h2&gt;

&lt;p&gt;In our tutorial we show how to use Ax to run multi-objective NAS for a simple neural network model on the popular MNIST dataset. While the underlying methodology can be used for more complicated models and larger datasets, we opt for a tutorial that is easily runnable end-to-end on a laptop in less than an hour. In our example, we will tune the widths of two hidden layers, the learning rate, the dropout probability, the batch size, and the number of training epochs. The goal is to trade off performance (accuracy on the validation set) and model size (the number of model parameters) using &lt;a href=&quot;https://proceedings.neurips.cc/paper/2021/file/11704817e347269b7254e744b5e22dac-Paper.pdf&quot;&gt;multi-objective Bayesian optimization&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The tutorial makes use of the following PyTorch libraries:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/PyTorchLightning/pytorch-lightning&quot;&gt;PyTorch Lightning&lt;/a&gt; (specifying the model and training loop)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/torchx&quot;&gt;TorchX&lt;/a&gt; (for running training jobs remotely / asynchronously)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/botorch&quot;&gt;BoTorch&lt;/a&gt; (the Bayesian optimization library that powers Ax’s algorithms)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The complete runnable example is available as a &lt;strong&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ax_multiobjective_nas_tutorial.html&quot;&gt;PyTorch Tutorial&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;The final results from the NAS optimization performed in the tutorial can be seen in the tradeoff plot below. Here, each point corresponds to the result of a trial, with the color representing its iteration number, and the star indicating the reference point defined by the thresholds we imposed on the objectives. We see that our method was able to successfully explore the trade-offs between validation accuracy and number of parameters and found both large models with high validation accuracy as well as small models with lower validation accuracy. Depending on the performance requirements and model size constraints, the decision maker can now choose which model to use or analyze further.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/MOO-NAS-blog-img2-pareto_frontier_plot.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;visualizations&quot;&gt;Visualizations&lt;/h3&gt;

&lt;p&gt;Ax provides a number of visualizations that make it possible to analyze and understand the results of an experiment. Here, we will focus on the performance of the Gaussian process models that model the unknown objectives, which are used to help us discover promising configurations faster. Ax makes it easy to better understand how accurate these models are and how they perform on unseen data via leave-one-out cross-validation. In the figures below, we see that the model fits look quite good - predictions are close to the actual outcomes, and predictive 95% confidence intervals cover the actual outcomes well. Additionally, we observe that the model size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(num_params)&lt;/code&gt; metric is much easier to model than the validation accuracy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(val_acc)&lt;/code&gt; metric.&lt;/p&gt;

&lt;!-- another image  --&gt;

&lt;style&gt;

    .cross-validation-container{
        display:flex; 
        flex-direction:row; 
    }

&lt;/style&gt;

&lt;div class=&quot;cross-validation-container&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/MOO-NAS-blog-img3-cv_plot_val_acc.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/MOO-NAS-blog-img4-cv_plot_num_params.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We showed how to run a fully automated multi-objective Neural Architecture Search using Ax.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using the Ax Scheduler, we were able to run the optimization automatically in a fully asynchronous fashion - this can be done locally (as done in the tutorial) or by deploying trials remotely to a cluster (simply by changing the TorchX scheduler configuration).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The state-of-the-art multi-objective Bayesian optimization algorithms available in Ax allowed us to efficiently explore the tradeoffs between validation accuracy and model size.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advanced-functionality&quot;&gt;Advanced Functionality&lt;/h2&gt;

&lt;p&gt;Ax has a number of other advanced capabilities that we did not discuss in our tutorial. Among these are the following:&lt;/p&gt;

&lt;h3 id=&quot;early-stopping&quot;&gt;Early Stopping&lt;/h3&gt;

&lt;p&gt;When evaluating a new candidate configuration, partial learning curves are typically available while the NN training job is running. We can use the information contained in the partial curves to identify under-performing trials to stop early in order to free up computational resources for more promising candidates. While not demonstrated in the above tutorial, Ax supports early stopping out-of-the-box - see our &lt;a href=&quot;https://ax.dev/versions/latest/tutorials/early_stopping/early_stopping.html&quot;&gt;early stopping tutorial&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;high-dimensional-search-spaces&quot;&gt;High-dimensional search spaces&lt;/h3&gt;

&lt;p&gt;In our tutorial, we used Bayesian optimization with a standard Gaussian process in order to keep the runtime low. However, these models typically scale to only about 10-20 tunable parameters. Our new SAASBO method (&lt;a href=&quot;https://proceedings.mlr.press/v161/eriksson21a/eriksson21a.pdf&quot;&gt;paper&lt;/a&gt;, &lt;a href=&quot;https://ax.dev/tutorials/saasbo.html&quot;&gt;Ax tutorial&lt;/a&gt;, &lt;a href=&quot;https://botorch.org/tutorials/saasbo&quot;&gt;BoTorch tutorial&lt;/a&gt;) is very sample-efficient and enables tuning hundreds of parameters. SAASBO can easily be enabled by passing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_saasbo=True&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;choose_generation_strategy&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank the TorchX team (in particular Kiuk Chung and Tristan Rice) for their help with integrating TorchX with Ax, and the Adaptive Experimentation team @ Meta for their contributions to Ax and BoTorch.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://research.facebook.com/blog/2021/07/optimizing-model-accuracy-and-latency-using-bayesian-multi-objective-neural-architecture-search/&quot;&gt;D. Eriksson, P. Chuang, S. Daulton, M. Balandat. Optimizing model accuracy and latency using Bayesian multi-objective neural architecture search. Meta Research blog, July 2021.&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>David Eriksson, Max Balandat</name>
        
        
      </author>

      

      

      
        <summary type="html">tl;dr</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling Multimodal Foundation Models in TorchMultimodal with Pytorch Distributed</title>
      <link href="https://pytorch.org/blog/scaling-multimodal-foundation-models-in-torchmultimodal-with-pytorch-distributed/" rel="alternate" type="text/html" title="Scaling Multimodal Foundation Models in TorchMultimodal with Pytorch Distributed" />
      <published>2022-11-21T00:00:00-08:00</published>
      <updated>2022-11-21T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/scaling-multimodal-foundation-models-in-torchmultimodal-with-pytorch-distributed</id>
      <content type="html" xml:base="https://pytorch.org/blog/scaling-multimodal-foundation-models-in-torchmultimodal-with-pytorch-distributed/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In recent years, scaling model sizes has become a promising area of research. In the field of NLP, language models have gone from hundreds of millions of parameters (BERT) to hundreds of billions of parameters (GPT-3) demonstrating significant improvements on downstream tasks. The &lt;a href=&quot;https://arxiv.org/pdf/2001.08361.pdf&quot;&gt;scaling laws&lt;/a&gt; for large scale language models have also been studied extensively in the industry. A similar trend can be observed in the vision field, with the community moving to transformer based models (like &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;Vision Transformer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2111.06377.pdf&quot;&gt;Masked Auto Encoders&lt;/a&gt;) as well. It is clear that individual modalities - text, image, video - have benefited massively from recent advancements in scale, and frameworks have quickly adapted to accommodate larger models.&lt;/p&gt;

&lt;p&gt;At the same time, multimodality is becoming increasingly important in research with tasks like image-text retrieval, visual question-answering, visual dialog and text to image generation gaining traction in real world applications. Training large scale multimodal models is the natural next step and we already see several efforts in this area like &lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt;CLIP&lt;/a&gt; from OpenAI, &lt;a href=&quot;https://parti.research.google/&quot;&gt;Parti&lt;/a&gt; from Google and &lt;a href=&quot;https://arxiv.org/pdf/2201.07520.pdf&quot;&gt;CM3&lt;/a&gt; from Meta.&lt;/p&gt;

&lt;p&gt;In this blog, we present a case study demonstrating the scaling of &lt;a href=&quot;https://flava-model.github.io/&quot;&gt;FLAVA&lt;/a&gt; to 10B params using techniques from PyTorch Distributed. FLAVA is a vision and language foundation model, available in &lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/torchmultimodal/models/flava&quot;&gt;TorchMultimodal&lt;/a&gt;, which has shown competitive performance on both unimodal and multimodal benchmarks. We also give the relevant code pointers in this blog. The instructions for running an example script to scale FLAVA can be found &lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/examples/flava/native&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;scaling-flava-overview&quot;&gt;Scaling FLAVA Overview&lt;/h2&gt;

&lt;p&gt;FLAVA is a foundation multimodal model which consists of transformer based image and text encoders followed by a transformer-based multimodal fusion module. It is pretrained on both unimodal and multimodal data with a diverse set of losses. This includes masked language, image and multimodal modeling losses that require the model to reconstruct the original input from its context (self-supervised learning). It also uses image text matching loss over positive and negative examples of aligned image-text pairs as well as CLIP style contrastive loss. In addition to multimodal tasks (like image-text retrieval), FLAVA demonstrated competitive performance on unimodal benchmarks as well (GLUE tasks for NLP and image classification for vision).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/scaling-multimodal-image1-diagram-of-multimodal-flava-new.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The original FLAVA model has ~350M parameters and uses ViT-B16 configurations (from the &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;Vision Transformer paper&lt;/a&gt;) for image and text encoders. The multimodal fusion transformer follows the unimodal encoders but with half the number of layers. We explore increasing the size of each encoder to larger ViT variants.&lt;/p&gt;

&lt;p&gt;Another aspect of scaling is adding the ability to increase the batch size. FLAVA makes use of contrastive loss over in-batch negatives, which typically benefits from large batch size (as studied &lt;a href=&quot;https://openreview.net/pdf?id=U2exBrf_SJh&quot;&gt;here&lt;/a&gt;). The largest training efficiency or throughput is also generally achieved when operating near maximum possible batch sizes as determined by the amount of GPU memory available (also see the experiments section).&lt;/p&gt;

&lt;p&gt;The following table displays the different model configurations we experimented with. We also determine the maximum batch size that was able to fit in memory for each configuration in the experiments section.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Approx Model params&lt;/th&gt;
      &lt;th&gt;Hidden size&lt;/th&gt;
      &lt;th&gt;MLP size&lt;/th&gt;
      &lt;th&gt;Heads&lt;/th&gt;
      &lt;th&gt;Unimodal layers&lt;/th&gt;
      &lt;th&gt;Multimodal layers&lt;/th&gt;
      &lt;th&gt;Model size (fp32)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;350M (original)&lt;/td&gt;
      &lt;td&gt;768&lt;/td&gt;
      &lt;td&gt;3072&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;1.33GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;900M&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt;4096&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;3.48GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1.8B&lt;/td&gt;
      &lt;td&gt;1280&lt;/td&gt;
      &lt;td&gt;5120&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;6.66GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2.7B&lt;/td&gt;
      &lt;td&gt;1408&lt;/td&gt;
      &lt;td&gt;6144&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;10.3GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4.8B&lt;/td&gt;
      &lt;td&gt;1664&lt;/td&gt;
      &lt;td&gt;8192&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;18.1GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10B&lt;/td&gt;
      &lt;td&gt;2048&lt;/td&gt;
      &lt;td&gt;10240&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;38GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;optimization-overview&quot;&gt;Optimization overview&lt;/h2&gt;

&lt;p&gt;PyTorch offers several native techniques to efficiently scale models. In the following sections, we go over some of these techniques and show how they can be applied to scale up a FLAVA model to 10 billion parameters.&lt;/p&gt;

&lt;h2 id=&quot;distributed-data-parallel&quot;&gt;Distributed Data Parallel&lt;/h2&gt;

&lt;p&gt;A common starting point for distributed training is data parallelism. Data parallelism replicates the model across each worker (GPU), and partitions the dataset across the workers. Different workers process different data partitions in parallel and synchronize their gradients (via all reduce) before model weights are updated. The figure below showcases the flow (forward, backward, and weight update steps) for processing a single example for data parallelism:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/scaling-multimodal-image2-diagram-of-standard-data-parallel-training.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  Source: &lt;a href=&quot;https://engineering.fb.com/2021/07/15/open-source/fsdp/&quot;&gt;https://engineering.fb.com/2021/07/15/open-source/fsdp/&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;PyTorch provides a native API, &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html&quot;&gt;DistributedDataParallel&lt;/a&gt; (DDP) to enable data parallelism which can be used as a module wrapper as showcased below. Please see PyTorch Distributed &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.html#&quot;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;from torchmultimodal.models.flava.model import flava_model_for_pretraining
import torch
import torch.distributed as dist

model = flava_model_for_pretraining().cuda()
# Initialize PyTorch Distributed process groups
# Please see https://pytorch.org/tutorials/intermediate/dist_tuto.html for details
dist.init_process_group(backend=”nccl”)
# Wrap model in DDP
model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[torch.cuda.current_device()])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;fully-sharded-data-parallel&quot;&gt;Fully Sharded Data Parallel&lt;/h2&gt;

&lt;p&gt;GPU memory usage of a training application can roughly be broken down into model inputs, intermediate activations (needed for gradient computation), model parameters, gradients, and optimizer states. Scaling a model will typically increase each of these elements. Scaling a model with DDP can eventually result in out-of-memory issues when a single GPU’s memory becomes insufficient since it replicates the parameters, gradients, and optimizer states on all workers.&lt;/p&gt;

&lt;p&gt;To reduce this replication and save GPU memory, we can shard the model parameters, gradients, and optimizer states across all workers with each worker only managing a single shard. This technique was popularized by the &lt;a href=&quot;https://arxiv.org/abs/1910.02054&quot;&gt;ZeRO-3&lt;/a&gt; approach developed by Microsoft. A PyTorch-native implementation of this approach is available as &lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html&quot;&gt;FullyShardedDataParallel&lt;/a&gt; (FSDP) API, released as a beta feature in PyTorch 1.12. During a module’s forward and backward passes, FSDP unshards the model parameters as needed for computation (using all-gather) and reshards them after computation. It synchronizes gradients using the reduce-scatter collective to ensure sharded gradients are globally averaged. The forward and backward pass flow of a model wrapped in FSDP are detailed below:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/scaling-multimodal-image3-diagram-of-fully-shared-data-parallel-training.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  Source: &lt;a href=&quot;https://engineering.fb.com/2021/07/15/open-source/fsdp/&quot;&gt;https://engineering.fb.com/2021/07/15/open-source/fsdp/&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;To use FSDP, the submodules of a model need to be wrapped with the API to control when specific submodules are sharded or unsharded. FSDP provides an auto-wrapping API (see the &lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel&quot;&gt;auto_wrap_policy&lt;/a&gt; argument) that can be used out of the box as well as several &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/distributed/fsdp/wrap.py&quot;&gt;wrapping policies&lt;/a&gt; and the ability to &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/75c0e3a471c19b883feca15fd4ecfabedf746691/torch/distributed/fsdp/fully_sharded_data_parallel.py#L858&quot;&gt;write your own policy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following example demonstrates wrapping the FLAVA model with FSDP. We specify the auto-wrapping policy as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transformer_auto_wrap_policy&lt;/code&gt;. This will wrap individual transformer layers (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransformerEncoderLayer&lt;/code&gt;), the image transformer (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ImageTransformer&lt;/code&gt;), text encoder (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BERTTextEncoder&lt;/code&gt;) and multimodal encoder (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FLAVATransformerWithoutEmbeddings&lt;/code&gt;) as individual FSDP units. This uses a recursive wrapping approach for efficient memory management. For example, after an individual transformer layer’s forward or backward pass is finished, its parameters are discarded, freeing up memory thereby reducing peak memory usage.&lt;/p&gt;

&lt;p&gt;FSDP also provides a number of configurable options to tune the performance of applications. For example, in our use case, we illustrate the use of the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;limit_all_gathers&lt;/code&gt; flag, which prevents all-gathering model parameters too early thereby alleviating memory pressure on the application. We encourage users to experiment with this flag which can potentially improve the performance of applications with high active memory usage.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;import torch
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from torchmultimodal.models.flava.model import flava_model_for_pretraining
from torchmultimodal.models.flava.text_encoder import BertTextEncoder
from torchmultimodal.models.flava.image_encoder import ImageTransformer
from torchmultimodal.models.flava.transformer import FLAVATransformerWithoutEmbeddings
from torchmultimodal.modules.layers.transformer import TransformerEncoderLayer

model = flava_model_for_pretraining().cuda()
dist.init_process_group(backend=”nccl”)

model = FSDP(
               model,
               device_id=torch.cuda.current_device(),
               auto_wrap_policy=partial(
                   transformer_auto_wrap_policy,
                   transformer_layer_cls={
                       TransformerEncoderLayer,
                       ImageTransformer,
                       BERTTextEncoder,
                       FLAVATransformerWithoutEmbeddings
                   },
               ),
               limit_all_gathers=True,
           )
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;activation-checkpointing&quot;&gt;Activation Checkpointing&lt;/h2&gt;

&lt;p&gt;As discussed above, intermediate activations, model parameters, gradients, and optimizer states contribute to the overall GPU memory usage. FSDP can reduce memory consumption due to the latter three but does not reduce memory consumed by activations. Memory used by activations increases with increase in batch size or number of hidden layers. Activation checkpointing is a technique to decrease this memory usage by recomputing the activations during the backward pass instead of holding them in memory for a specific checkpointed module. For example, we observed ~4x reduction in the peak active memory after forward pass by applying activation checkpointing to the 2.7B parameter model.&lt;/p&gt;

&lt;p&gt;PyTorch offers a wrapper based activation checkpointing API. In particular, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkpoint_wrapper&lt;/code&gt; allows users to wrap an individual module with checkpointing, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apply_activation_checkpointing&lt;/code&gt; allows users to specify a policy with which to wrap modules within an overall module with checkpointing. Both these APIs can be applied to most models as they do not require any modifications to the model definition code. However, if more granular control over checkpointed segments, such as checkpointing specific functions within a module, is required, the functional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.utils.checkpoint&lt;/code&gt; &lt;a href=&quot;https://pytorch.org/docs/stable/checkpoint.html&quot;&gt;API&lt;/a&gt; can be leveraged, although this requires modification to the model code. The application of the activation checkpointing wrapper to individual FLAVA transformer layers (denoted by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TransformerEncoderLayer&lt;/code&gt;) is shown below. For a thorough description of activation checkpointing, please see the description in the &lt;a href=&quot;https://pytorch.org/docs/stable/checkpoint.html&quot;&gt;PyTorch documentation&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Python&quot;&gt;from torchmultimodal.models.flava.model import flava_model_for_pretraining
from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import apply_activation_checkpointing, checkpoint_wrapper, CheckpointImpl
from torchmultimodal.modules.layers.transformer import TransformerEncoderLayer

model = flava_model_for_pretraining()
checkpoint_tformer_layers_policy = lambda submodule: isinstance(submodule, TransformerEncoderLayer)

apply_activation_checkpointing(
               model,
               checkpoint_wrapper_fn=checkpoint_wrapper,
               check_fn=checkpoint_tformer_layers_policy,
           )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Used together, wrapping FLAVA transformer layers with activation checkpointing and wrapping the overall model with FSDP as demonstrated above, we are able to scale FLAVA to 10B parameters.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;We conduct an empirical study about the impact of the different optimizations from the previous section on system performance. For all our experiments, we use a single node with 8 A100 40GB GPUs and run the pretraining for 1000 iterations. All runs also used PyTorch’s &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;automatic mixed precision&lt;/a&gt; with the bfloat16 data type. &lt;a href=&quot;https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices&quot;&gt;TensorFloat32&lt;/a&gt; format is also enabled to improve matmul performance on the A100. We define throughput as the average number of items (text or image) processed per second (we ignore the first 100 iterations while measuring throughput to account for warmup). We leave training to convergence and its impact on downstream task metrics as an area for future study.&lt;/p&gt;

&lt;p&gt;Figure 1 plots the throughput for each model configuration and optimization, both with a local batch size of 8 and then with the maximum batch size possible on 1 node. Absence of a data point for a model variant for an optimization indicates that the model could not be trained on a single node.&lt;/p&gt;

&lt;p&gt;Figure 2 plots the maximum possible batch size per worker for each optimization. We observe a few things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Scaling model size: DDP is only able to fit the 350M and 900M model on a node. With FSDP, due to memory savings, we are able to train ~3x bigger models compared to DDP  (i.e. the 1.8B and 2.7B variants). Combining activation checkpointing (AC) with FSDP enables training even bigger models, on the order of ~10x compared to DDP (i.e. 4.8B and 10B variants)&lt;/li&gt;
  &lt;li&gt;Throughput:
    &lt;ul&gt;
      &lt;li&gt;For smaller model sizes, at a constant batch size of 8, the throughput for DDP is slightly higher than or equal to FSDP, explainable by the additional communication required by FSDP. It is lowest for FSDP and AC combined together. This is because AC re-runs checkpointed forward passes during the backwards pass, trading off additional computation for memory savings. However, in the case of the 2.7B model, FSDP + AC actually has higher throughput compared to FSDP alone. This is because the 2.7B model with FSDP is operating close to the memory limit even at batch size 8 triggering CUDA malloc retries which tend to slow down training. AC helps with reducing the memory pressure and leads to no retries.&lt;/li&gt;
      &lt;li&gt;For DDP and FSDP + AC, the throughput increases with an increase in batch size for each model. For FSDP alone, this is true for smaller variants. However, with the 1.8B and 2.7B parameter models, we observe throughput degradation when increasing batch size. A potential reason for this, as noted above also, is that at the memory limit, PyTorch’s CUDA memory management may have to retry cudaMalloc calls and/or run expensive defragmentation steps to find free memory blocks to handle the workload’s memory requirements which can result in training slowdown.&lt;/li&gt;
      &lt;li&gt;For larger models that can only be trained with FSDP (1.8B, 2.7B, 4.8B) the setting with highest throughput achieved is with FSDP + AC scaling to the maximum batch size. For 10B, we observe nearly equal throughput for smaller and maximum batch size. This might be counterintuitive as AC results in increased computation and maxing out batch size potentially leads to expensive defragmentation operations due to operating at CUDA memory limit. However, for these large models, the increase in batch size is large enough to mask this overhead.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/scaling-multimodal-image4-graph-experiments-figure1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  Figure 1: Training throughput for different configurations
&lt;/p&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Batch size: FSDP alone enables slightly higher batch sizes compared to DDP. Using FSDP + AC enables ~3x batch size compared to DDP for the 350M param model and ~5.5x for 900M param model. Even for 10B, a max batch size of ~20 which is fairly decent. This essentially enables larger global batch size using fewer GPUs which is especially useful for contrastive learning tasks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/scaling-multimodal-image5-graph-experiments-figure-2.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  Figure 2: Max local batchsize possible for different configurations
&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As the world moves towards multimodal foundation models, scaling model parameters and efficient training is becoming an area of focus. The PyTorch ecosystem aims to accelerate innovation in this field by providing different tools to the research community, both for training and scaling multimodal models. With FLAVA, we laid out an example of scaling a model for multimodal understanding. In the future, we plan to add support for other kinds of models like the ones for multimodal generation and demonstrate their scaling factors. We also hope to automate many of these scaling and memory saving techniques (such as sharding and activation checkpointing) to reduce the amount of user experimentation needed to achieve the desired scale and maximum training throughput.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/introducing-torchmultimodal/&quot;&gt;Introducing TorchMultimodal - a library for accelerating exploration in Multimodal AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://deploy-preview-1186--pytorch-dot-org-preview.netlify.app/blog/introducing-torchmultimodal/&quot;&gt;FLAVA paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/&quot;&gt;Introducing Pytorch FSDP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Ankita De, Edward Wang (EcoF), Rohan Varma, Anjali Sridhar, Kartikay Khandelwal</name>
        
        
      </author>

      

      

      
        <summary type="html">Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing TorchMultimodal - a library for accelerating exploration in Multimodal AI</title>
      <link href="https://pytorch.org/blog/introducing-torchmultimodal/" rel="alternate" type="text/html" title="Introducing TorchMultimodal - a library for accelerating exploration in Multimodal AI" />
      <published>2022-11-17T00:00:00-08:00</published>
      <updated>2022-11-17T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/introducing-torchmultimodal</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-torchmultimodal/">&lt;p&gt;We are announcing TorchMultimodal Beta, a PyTorch domain library for training SoTA multi-task multimodal models at scale. The library provides composable building blocks (modules, transforms, loss functions) to accelerate model development, SoTA model architectures (FLAVA, MDETR, Omnivore) from published research, training and evaluation scripts, as well as notebooks for exploring these models. The library is under active development, and we’d love to hear your feedback! You can find more details on how to get started &lt;a href=&quot;https://github.com/facebookresearch/multimodal#installation&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-torchmultimodal&quot;&gt;Why TorchMultimodal?&lt;/h2&gt;

&lt;p&gt;Interest is rising around AI models that understand multiple input types (text, images, videos and audio signals), and optionally use this understanding to generate different forms of outputs (sentences, pictures, videos). Recent work from FAIR such as &lt;a href=&quot;https://arxiv.org/abs/2112.04482&quot;&gt;FLAVA&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2201.08377.pdf&quot;&gt;Omnivore&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2202.03555&quot;&gt;data2vec&lt;/a&gt; have shown that &lt;a href=&quot;https://ai.facebook.com/blog/advances-in-multimodal-understanding-research-at-meta-ai/&quot;&gt;multimodal models for understanding&lt;/a&gt; are competitive with unimodal counterparts, and in some cases are establishing the new state-of-the art. Generative models such as &lt;a href=&quot;https://ai.facebook.com/blog/generative-ai-text-to-video/&quot;&gt;Make-a-video&lt;/a&gt; and &lt;a href=&quot;https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/&quot;&gt;Make-a-scene&lt;/a&gt; are redefining what modern AI systems can do.&lt;/p&gt;

&lt;p&gt;As interest in multimodal AI has grown, researchers are looking for tools and libraries to quickly experiment with ideas, and build on top of the latest research in the field. While the PyTorch ecosystem has a rich repository of libraries and frameworks, it’s not always obvious how components from these interoperate with each other, or how they can be stitched together to build SoTA multimodal models.&lt;/p&gt;

&lt;p&gt;TorchMultimodal solves this problem by providing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Composable and easy-to-use building blocks&lt;/strong&gt; which researchers can use to accelerate model development and experimentation in their own workflows. These are designed to be modular, and can be easily extended to handle new modalities.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;End-to-end examples for training and evaluating the latest models from research.&lt;/strong&gt; These should serve as starting points for ongoing/future research, as well as examples for using advanced features such as integrating with FSDP and activation checkpointing for scaling up model and batch sizes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introducing-torchmultimodal&quot;&gt;Introducing TorchMultimodal&lt;/h2&gt;

&lt;p&gt;TorchMultimodal is a PyTorch domain library for training multi-task multimodal models at scale. In the repository, we provide:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/torchmultimodal&quot;&gt;Building Blocks&lt;/a&gt;&lt;/strong&gt;. A collection of modular and composable building blocks like models, fusion layers, loss functions, datasets and utilities. Some examples include:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/4d2236877467ff8f56aa1935dd92d7782751b135/torchmultimodal/modules/losses/contrastive_loss_with_temperature.py#L145&quot;&gt;Contrastive Loss with Temperature&lt;/a&gt;. Commonly used function for training models like CLIP and FLAVA. We also include variants such as &lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/4d2236877467ff8f56aa1935dd92d7782751b135/torchmultimodal/modules/losses/albef.py#L14&quot;&gt;ImageTextContrastiveLoss&lt;/a&gt; used in models like ALBEF.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/modules/layers/codebook.py#L31&quot;&gt;Codebook layers&lt;/a&gt; which compresses high dimensional data by nearest neighbor lookup in an embedding space and is a vital component of VQVAEs (provided as a &lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/4d2236877467ff8f56aa1935dd92d7782751b135/torchmultimodal/models/vqvae.py#L26&quot;&gt;model&lt;/a&gt; in the repository).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/modules/encoders/swin_transformer_3d_encoder.py#L76&quot;&gt;Shifted-window Attention&lt;/a&gt; window based multi-head self attention which is a vital component of encoders like Swin 3D Transformers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/4d2236877467ff8f56aa1935dd92d7782751b135/torchmultimodal/models/clip&quot;&gt;Components for CLIP.&lt;/a&gt; A popular model published by OpenAI which has proven to be extremely effective at learning text and image representations.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/4d2236877467ff8f56aa1935dd92d7782751b135/torchmultimodal/models/gpt.py&quot;&gt;Multimodal GPT.&lt;/a&gt; An abstraction that extends OpenAI’s &lt;a href=&quot;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;GPT&lt;/a&gt; architecture for multimodal generation when combined with the &lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/utils/generate.py#L33&quot;&gt;generation utility&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/modules/layers/attention.py#L134&quot;&gt;MultiHeadAttention&lt;/a&gt;. A critical component for attention-based models with support for fast auto-regressive decoding.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/examples&quot;&gt;Examples&lt;/a&gt;&lt;/strong&gt;. A collection of examples that show how to combine these building blocks with components and common infrastructure (Lightning, TorchMetrics) from across the PyTorch Ecosystem to replicate state-of-the-art models published in literature. We currently provide five examples, which include.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.04482&quot;&gt;FLAVA&lt;/a&gt; [&lt;a href=&quot;https://arxiv.org/abs/2112.04482&quot;&gt;paper&lt;/a&gt;]. Official code for the paper accepted at CVPR, including a tutorial on finetuning FLAVA.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/examples/mdetr&quot;&gt;MDETR&lt;/a&gt; [&lt;a href=&quot;https://arxiv.org/abs/2104.12763&quot;&gt;paper&lt;/a&gt;]. Collaboration with authors from NYU to provide an example which alleviates interoperability pain points in the PyTorch ecosystem, including a &lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/main/examples/mdetr/MDETRTutorial.ipynb&quot;&gt;notebook&lt;/a&gt; on using MDETR for phrase grounding and visual question answering.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/examples/omnivore&quot;&gt;Omnivore&lt;/a&gt; [&lt;a href=&quot;https://arxiv.org/abs/2204.08058&quot;&gt;paper&lt;/a&gt;]. First example in TorchMultimodal of a model which deals with Video and 3D data, including a &lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/main/examples/omnivore/omnivore_inference_demo.ipynb&quot;&gt;notebook&lt;/a&gt; for exploring the model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/examples/mugen&quot;&gt;MUGEN&lt;/a&gt; [&lt;a href=&quot;https://arxiv.org/abs/2204.08058&quot;&gt;paper&lt;/a&gt;]. Foundational work for auto-regressive &lt;a href=&quot;https://colab.research.google.com/drive/1C3ZbH_l19g_KqW3CPeX2-8Q2sOUCpmZo?usp=sharing&quot;&gt;generation&lt;/a&gt; and &lt;a href=&quot;https://colab.research.google.com/drive/1gZfz1jsy79CNCK9t2_r43yt3z7v-w4HS?usp=sharing&quot;&gt;retrieval&lt;/a&gt;, including demos for text-video generation and retrieval with a large-scale synthetic dataset enriched from OpenAI &lt;a href=&quot;https://github.com/openai/coinrun&quot;&gt;coinrun&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/multimodal/tree/main/examples/albef&quot;&gt;ALBEF&lt;/a&gt; [&lt;a href=&quot;https://arxiv.org/abs/2107.07651&quot;&gt;paper&lt;/a&gt;] Code for the model, including a &lt;a href=&quot;https://github.com/facebookresearch/multimodal/blob/main/examples/albef/vqa_with_albef.ipynb&quot;&gt;notebook&lt;/a&gt; for using this model for Visual Question Answering.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following code snippet showcases an example usage of several TorchMultimodal components related to CLIP:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# instantiate clip transform
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CLIPTransform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# pass the transform to your dataset. Here we use coco captions
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CocoCaptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;annFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# instantiate model. Here we use clip with vit-L as the image encoder
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clip_vit_l14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# define loss and other things needed for training
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clip_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ContrastiveLossWithTemperature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# write your train loop
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;image_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contrastive_loss_with_temperature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Apart from the code, we are also &lt;strong&gt;releasing a tutorial for fine-tuning multimodal foundation models, and a blog post (with code pointers) on how to scale up such models using techniques from PyTorch Distributed (FSDP and activation checkpointing)&lt;/strong&gt;. We hope such examples and tutorials will serve to demystify a number of advanced features available in the PyTorch ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;While this is an exciting launch, there’s a lot more to come. The library is under development and we are working on adding some of the exciting developments in the space of diffusion models, and examples to showcase common trends from research. As you explore and use the library, we’d love to hear any feedback you might have! You can find more details on how to get started &lt;a href=&quot;https://github.com/facebookresearch/multimodal#installation&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;team&quot;&gt;Team&lt;/h2&gt;

&lt;p&gt;The primary contributors and developers of TorchMultimodal include Ankita De, Evan Smothers, Kartikay Khandelwal, Lan Gong, Laurence Rouesnel, Nahiyan Malik, Rafi Ayub and Yosua Michael Maranatha.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kartikay Khandelwal, Ankita De</name>
        
        
      </author>

      

      

      
        <summary type="html">We are announcing TorchMultimodal Beta, a PyTorch domain library for training SoTA multi-task multimodal models at scale. The library provides composable building blocks (modules, transforms, loss functions) to accelerate model development, SoTA model architectures (FLAVA, MDETR, Omnivore) from published research, training and evaluation scripts, as well as notebooks for exploring these models. The library is under active development, and we’d love to hear your feedback! You can find more details on how to get started here.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Enterprise Support Program Update</title>
      <link href="https://pytorch.org/blog/pytorch-enterprise-support-update/" rel="alternate" type="text/html" title="PyTorch Enterprise Support Program Update" />
      <published>2022-11-10T00:00:00-08:00</published>
      <updated>2022-11-10T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch-enterprise-support-update</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-enterprise-support-update/">&lt;p&gt;On May 25, 2021, we announced the &lt;a href=&quot;https://pytorch.org/blog/announcing-pytorch-enterprise/&quot;&gt;PyTorch Enterprise Support Program&lt;/a&gt; (ESP) that enabled providers to develop and offer tailored enterprise-grade support to their customers.&lt;/p&gt;

&lt;p&gt;The program enabled Program certified service providers to develop and offer tailored enterprise-grade support to their customers through contribution of hotfixes and other improvements requested by PyTorch enterprise users who were developing models in production at scale for mission-critical applications. However, as we evaluate community feedback, we found ongoing ESP support was not necessary at this time and will immediately divert these resources to other areas to improve the user experience for the entire community.&lt;/p&gt;

&lt;p&gt;Today, we are removing the PyTorch long-term support (LTS 1.8.2) download link from the “Get Started” page from the “&lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;Start Locally&lt;/a&gt;” download option in order to simplify the user experience. One can download PyTorch v1.8.2 in &lt;a href=&quot;/get-started/previous-versions/#v182-with-lts-support&quot;&gt;previous versions&lt;/a&gt;. Please note that it is only supported for Python while it is being deprecated. If there are any updates to ESP/LTS, we will update future blogs.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/Pytorch-Enterprise-Support-Img1.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Please reach out to &lt;a href=&quot;mailto:marketing@pytorch.org&quot;&gt;marketing@pytorch.org&lt;/a&gt; with any questions.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">On May 25, 2021, we announced the PyTorch Enterprise Support Program (ESP) that enabled providers to develop and offer tailored enterprise-grade support to their customers.</summary>
      

      
      
    </entry>
  
</feed>


