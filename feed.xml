<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-03-20T18:01:45-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Accelerated Diffusers with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/accelerated-diffusers-pt-20/" rel="alternate" type="text/html" title="Accelerated Diffusers with PyTorch 2.0" />
      <published>2023-03-16T00:00:00-07:00</published>
      <updated>2023-03-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-diffusers-pt-20</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-diffusers-pt-20/">&lt;p&gt;PyTorch 2.0 has just been released. Its flagship new feature is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;, a one-line code change that promises to automatically improve performance across codebases. We have previously &lt;a href=&quot;https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/&quot;&gt;checked on that promise in Hugging Face Transformers and TIMM models&lt;/a&gt;, and delved deep into its &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;motivation, architecture and the road ahead&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As important as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is, there’s much more to PyTorch 2.0. Notably, PyTorch 2.0 incorporates several strategies to accelerate transformer blocks, and these improvements are very relevant for diffusion models too. Techniques such as &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt;, for example, have become very popular in the diffusion community thanks to their ability to significantly speed up Stable Diffusion and achieve larger batch sizes, and they are now part of PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;In this post we discuss how attention layers are optimized in PyTorch 2.0 and how these optimization are applied to the popular &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;🧨 Diffusers library&lt;/a&gt;. We finish with a benchmark that shows how the use of PyTorch 2.0 and Diffusers immediately translates to significant performance improvements across different hardware.&lt;/p&gt;

&lt;h2 id=&quot;accelerating-transformer-blocks&quot;&gt;Accelerating transformer blocks&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 includes a &lt;em&gt;scaled dot-product attention&lt;/em&gt; function as part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional&lt;/code&gt;. This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. Before PyTorch 2.0, you had to search for third-party implementations and install separate packages in order to take advantage of memory optimized algorithms, such as FlashAttention. The available implementations are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;FlashAttention, from the official &lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;FlashAttention project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Memory-Efficient Attention, from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A native C++ implementation suitable for non-CUDA devices or when high-precision is required.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these methods are available by default, and PyTorch will try to select the optimal one automatically through the use of the new scaled dot-product attention (SDPA) API. You can also individually toggle them for finer-grained control, see &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention&quot;&gt;the documentation&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;using-scaled-dot-product-attention-in-diffusers&quot;&gt;Using scaled dot-product attention in diffusers&lt;/h2&gt;

&lt;p&gt;The incorporation of Accelerated PyTorch 2.0 Transformer attention to the Diffusers library was achieved through the use of the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/api/models#diffusers.UNet2DConditionModel.set_attn_processor&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_attn_processor&lt;/code&gt; method&lt;/a&gt;, which allows for pluggable attention modules to be configured. In this case, a &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57/src/diffusers/models/cross_attention.py#L469&quot;&gt;new attention processor was created&lt;/a&gt;, which is &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57bb7a9ee13af4a08492e524b0a145a2c5/src/diffusers/models/cross_attention.py#L105&quot;&gt;enabled by default when PyTorch 2.0 is available&lt;/a&gt;. For clarity, this is how you could enable it manually (but it’s usually not necessary since diffusers will automatically take care of it):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionPipeline
from diffusers.models.cross_attention import AttnProcessor2_0

pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;)
pipe.to(&quot;cuda&quot;)
pipe.unet.set_attn_processor(AttnProcessor2_0())

prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
image = pipe(prompt).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;stable-diffusion-benchmark&quot;&gt;Stable Diffusion Benchmark&lt;/h2&gt;

&lt;p&gt;We ran a number of tests using accelerated dot-product attention from PyTorch 2.0 in Diffusers. We installed diffusers from pip and used nightly versions of PyTorch 2.0, since our tests were performed before the official release. We also used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.set_float32_matmul_precision('high')&lt;/code&gt; to enable additional fast matrix multiplication algorithms.&lt;/p&gt;

&lt;p&gt;We compared results with the traditional attention implementation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diffusers&lt;/code&gt; (referred to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vanilla&lt;/code&gt; below) as well as with the best-performing solution in pre-2.0 PyTorch: PyTorch 1.13.1 with the xFormers package (v0.0.16) installed.&lt;/p&gt;

&lt;p&gt;Results were measured without compilation (i.e., no code changes at all), and also with a single call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to wrap the UNet module. We did not compile the image decoder because most of the time is spent in the 50 denoising iterations that run UNet evaluations.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float32&quot;&gt;Results in float32&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig1-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float32&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following figures explore performance improvement vs batch size for various representative GPUs belonging to different generations. We collected data for each combination until we reached maximum memory utilization. Vanilla attention runs out of memory earlier than xFormers or PyTorch 2.0, which explains the missing bars for larger batch sizes. Similarly, A100 (we used the 40 GB version) is capable of running batch sizes of 64, but the other GPUs could only reach 32 in our tests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig2-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig3-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig4-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig5-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (V100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We found very significant performance improvements over vanilla attention across the board, without even using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;. An out of the box installation of PyTorch 2.0 and diffusers yields about 50% speedup on A100 and between 35% and 50% on 4090 GPUs, depending on batch size. Performance improvements are more pronounced for modern CUDA architectures such as Ada (4090) or Ampere (A100), but they are still very significant for older architectures still heavily in use in cloud services.&lt;/p&gt;

&lt;p&gt;In addition to faster speeds, the accelerated transformers implementation in PyTorch 2.0 allows much larger batch sizes to be used. A single 40GB A100 GPU runs out of memory with a batch size of 10, and 24 GB high-end consumer cards such as 3090 and 4090 cannot generate 8 images at once. Using PyTorch 2.0 and diffusers we could achieve batch sizes of &lt;strong&gt;48&lt;/strong&gt; for 3090 and 4090, and &lt;strong&gt;64&lt;/strong&gt; for A100. This is of great significance for cloud services and applications, as they can efficiently process more images at a time.&lt;/p&gt;

&lt;p&gt;When compared with PyTorch 1.13.1 + xFormers, the new accelerated transformers implementation is still faster and requires no additional packages or dependencies. In this case we found moderate speedups of up to 2% on datacenter cards such as A100 or T4, but performance was great on the two last generations of consumer cards: up to 20% speed improvement on 3090 and between 10% and 45% on 4090, depending on batch size.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is used, we get an additional performance boost of (typically) 2% and 3% over the previous improvements. As compilation takes some time, this is better geared towards user-facing inference services or training.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float16&quot;&gt;Results in float16&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig6-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float16&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig7-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig8-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig9-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float16&lt;/code&gt; inference, the performance improvements of the accelerated transformers implementation in PyTorch 2.0 are between 20% and 28% over standard attention, across all the GPUs we tested, except for the 4090, which belongs to the more modern Ada architecture. This GPU benefits from a dramatic performance improvement when using PyTorch 2.0 nightlies. With respect to optimized SDPA vs xFormers, results are usually on par for most GPUs, except again for the 4090. Adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to the mix boosts performance a few more percentage points across the board.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 comes with multiple features to optimize the crucial components of the foundational transformer block, and they can be further improved with the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. These optimizations lead to significant memory and time improvements for diffusion models, and remove the need for third-party library installations.&lt;/p&gt;

&lt;p&gt;To take advantage of these speed and memory improvements all you have to do is upgrade to PyTorch 2.0 and use diffusers &amp;gt;= 0.13.0.&lt;/p&gt;

&lt;p&gt;For more examples and in-detail benchmark numbers, please also have a look at the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/torch2.0&quot;&gt;Diffusers with PyTorch 2.0&lt;/a&gt; docs.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The authors are grateful to the PyTorch team for creating such excellent software.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pedro Cuenca, Patrick von Platen, Suraj Patil</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 has just been released. Its flagship new feature is torch.compile(), a one-line code change that promises to automatically improve performance across codebases. We have previously checked on that promise in Hugging Face Transformers and TIMM models, and delved deep into its motivation, architecture and the road ahead.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">New Library Updates in PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/" rel="alternate" type="text/html" title="New Library Updates in PyTorch 2.0" />
      <published>2023-03-15T00:00:00-07:00</published>
      <updated>2023-03-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/new-library-updates-in-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We are bringing a number of improvements to the current PyTorch libraries, alongside the &lt;a href=&quot;/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0 release&lt;/a&gt;. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.&lt;/p&gt;

&lt;p&gt;Along with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. Please find the list of the latest stable versions and updates below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Latest Stable Library Versions (&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;Full List&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchArrow 0.1.0
   &lt;/td&gt;
   &lt;td&gt;TorchRec 0.4.0
   &lt;/td&gt;
   &lt;td&gt;TorchVision 0.15
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchAudio 2.0
   &lt;/td&gt;
   &lt;td&gt;TorchServe 0.7.1
   &lt;/td&gt;
   &lt;td&gt;TorchX 0.4.0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchData 0.6.0
   &lt;/td&gt;
   &lt;td&gt;TorchText 0.15.0
   &lt;/td&gt;
   &lt;td&gt;PyTorch on XLA Devices 1.14
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;prior versions&lt;/a&gt; or (unstable) nightlies, click on versions in the top left menu above ‘Search Docs’.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio&quot;&gt;TorchAudio&lt;/h2&gt;

&lt;h3 id=&quot;beta-data-augmentation-operators&quot;&gt;[Beta] Data augmentation operators&lt;/h3&gt;

&lt;p&gt;The release adds several data augmentation operators under torchaudio.functional and torchaudio.transforms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;torchaudio.functional.add_noise&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.convolve&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.deemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.fftconvolve&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.preemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.speed&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.AddNoise&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Convolve&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Deemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.FFTConvolve&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Preemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Speed&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.SpeedPerturbation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The operators can be used to synthetically diversify training data to improve the generalizability of downstream models.&lt;/p&gt;

&lt;p&gt;For usage details, please refer to the &lt;a href=&quot;https://pytorch.org/audio/2.0.0/functional.html&quot;&gt;functional&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/transforms.html&quot;&gt;transform&lt;/a&gt; documentation and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/tutorials/audio_data_augmentation_tutorial.html&quot;&gt;Audio Data Augmentation&lt;/a&gt; tutorial.&lt;/p&gt;

&lt;h3 id=&quot;beta-wavlm-and-xls-r-models&quot;&gt;[Beta] WavLM and XLS-R models&lt;/h3&gt;

&lt;p&gt;The release adds two self-supervised learning models for speech and audio.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/9814838&quot;&gt;WavLM&lt;/a&gt; that is robust to noise and reverberation.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.09296&quot;&gt;XLS-R&lt;/a&gt; that is trained on cross-lingual datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides the model architectures, torchaudio also supports corresponding pre-trained pipelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_BASE&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_BASE_PLUS&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_LARGE&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_300M&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_1B&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_2B&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For usage details, please refer to the &lt;a href=&quot;https://pytorch.org/audio/2.0.0/generated/torchaudio.models.Wav2Vec2Model.html#factory-functions&quot;&gt;factory function&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/pipelines.html#id3&quot;&gt;pre-trained pipelines&lt;/a&gt; documentation.&lt;/p&gt;

&lt;h2 id=&quot;torchrl&quot;&gt;TorchRL&lt;/h2&gt;

&lt;p&gt;The initial release of torchrl includes several features that span across the entire RL domain. TorchRL can already be used in online, offline, multi-agent, multi-task and distributed RL settings, among others. See below:&lt;/p&gt;

&lt;h3 id=&quot;beta-environment-wrappers-and-transforms&quot;&gt;[Beta] Environment wrappers and transforms&lt;/h3&gt;

&lt;p&gt;torchrl.envs includes several wrappers around common environment libraries. This allows users to swap one library with another without effort. These wrappers build an interface between these simulators and torchrl:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;dm_control:&lt;/li&gt;
  &lt;li&gt;Gym&lt;/li&gt;
  &lt;li&gt;Brax&lt;/li&gt;
  &lt;li&gt;EnvPool&lt;/li&gt;
  &lt;li&gt;Jumanji&lt;/li&gt;
  &lt;li&gt;Habitat&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It also comes with many commonly used transforms and vectorized environment utilities that allow for a fast execution across simulation libraries. Please refer to the &lt;a href=&quot;https://pytorch.org/rl/reference/envs.html&quot;&gt;documentation&lt;/a&gt; for more detail.&lt;/p&gt;

&lt;h3 id=&quot;beta-datacollectors&quot;&gt;[Beta] Datacollectors&lt;/h3&gt;

&lt;p&gt;Data collection in RL is made easy via the usage of single process or multiprocessed/distributed data collectors that execute the policy in the environment over a desired duration and deliver samples according to the user’s needs. These can be found in torchrl.collectors and are documented &lt;a href=&quot;https://pytorch.org/rl/reference/collectors.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-objective-modules&quot;&gt;[Beta] Objective modules&lt;/h3&gt;

&lt;p&gt;Several objective functions are included in torchrl.objectives, among which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A generic PPOLoss class and derived ClipPPOLoss and KLPPOLoss&lt;/li&gt;
  &lt;li&gt;SACLoss and DiscreteSACLoss&lt;/li&gt;
  &lt;li&gt;DDPGLoss&lt;/li&gt;
  &lt;li&gt;DQNLoss&lt;/li&gt;
  &lt;li&gt;REDQLoss&lt;/li&gt;
  &lt;li&gt;A2CLoss&lt;/li&gt;
  &lt;li&gt;TD3Loss&lt;/li&gt;
  &lt;li&gt;ReinforceLoss&lt;/li&gt;
  &lt;li&gt;Dreamer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vectorized value function operators also appear in the library. Check the documentation &lt;a href=&quot;https://pytorch.org/rl/reference/objectives.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-models-and-exploration-strategies&quot;&gt;[Beta] Models and exploration strategies&lt;/h3&gt;

&lt;p&gt;We provide multiple models, modules and exploration strategies. Get a detailed description in &lt;a href=&quot;https://pytorch.org/rl/reference/modules.html&quot;&gt;the doc&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-composable-replay-buffer&quot;&gt;[Beta] Composable replay buffer&lt;/h3&gt;

&lt;p&gt;A composable replay buffer class is provided that can be used to store data in multiple contexts including single and multi-agent, on and off-policy and many more.. Components include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storages (list, physical or memory-based contiguous storages)&lt;/li&gt;
  &lt;li&gt;Samplers (Prioritized, sampler without repetition)&lt;/li&gt;
  &lt;li&gt;Writers&lt;/li&gt;
  &lt;li&gt;Possibility to add transforms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Replay buffers and other data utilities are documented &lt;a href=&quot;https://pytorch.org/rl/reference/data.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-logging-tools-and-trainer&quot;&gt;[Beta] Logging tools and trainer&lt;/h3&gt;

&lt;p&gt;We support multiple logging tools including tensorboard, wandb and mlflow.&lt;/p&gt;

&lt;p&gt;We provide a generic Trainer class that allows for easy code recycling and checkpointing.&lt;/p&gt;

&lt;p&gt;These features are documented &lt;a href=&quot;https://pytorch.org/rl/reference/trainers.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;tensordict&quot;&gt;TensorDict&lt;/h2&gt;

&lt;p&gt;TensorDict is a new data carrier for PyTorch.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensordict-specialized-dictionary-for-pytorch&quot;&gt;[Beta] TensorDict: specialized dictionary for PyTorch&lt;/h3&gt;

&lt;p&gt;TensorDict allows you to execute many common operations across batches of tensors carried by a single container. TensorDict supports many shape and device or storage operations, and  can readily be used in distributed settings. Check the &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/&quot;&gt;documentation&lt;/a&gt; to know more.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensorclass-a-dataclass-for-pytorch&quot;&gt;[Beta] @tensorclass: a dataclass for PyTorch&lt;/h3&gt;

&lt;p&gt;Like TensorDict, &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/reference/prototype.html&quot;&gt;tensorclass&lt;/a&gt; provides the opportunity to write dataclasses with built-in torch features such as shape or device operations.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensordictnn-specialized-modules-for-tensordict&quot;&gt;[Beta] tensordict.nn: specialized modules for TensorDict&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/reference/nn.html&quot;&gt;tensordict.nn module&lt;/a&gt; provides specialized nn.Module subclasses that make it easy to build arbitrarily complex graphs that can be executed with TensorDict inputs. It is compatible with the latest PyTorch features such as functorch, torch.fx and torch.compile.&lt;/p&gt;

&lt;h2 id=&quot;torchrec&quot;&gt;TorchRec&lt;/h2&gt;

&lt;h3 id=&quot;beta-keyedjaggedtensor-all-to-all-redesign-and-input-dist-fusion&quot;&gt;[Beta] KeyedJaggedTensor All-to-All Redesign and Input Dist Fusion&lt;/h3&gt;

&lt;p&gt;We observed performance regression due to a bottleneck in sparse data distribution for models that have multiple, large KJTs to redistribute.&lt;/p&gt;

&lt;p&gt;To combat this we altered the comms pattern to transport the minimum data required in the initial collective to support the collective calls for the actual KJT tensor data. This data sent in the initial collective, ‘splits’ means more data is transmitted over the comms stream overall, but the CPU is blocked for significantly shorter amounts of time leading to better overall QPS.&lt;/p&gt;

&lt;p&gt;Furthermore, we altered the TorchRec train pipeline to group the initial collective calls for the splits together before launching the more expensive KJT tensor collective calls. This fusion minimizes the CPU blocked time as launching each subsequent input distribution is no longer dependent on the previous input distribution.&lt;/p&gt;

&lt;p&gt;With this feature, variable batch sizes are now natively supported across ranks. These features are documented &lt;a href=&quot;https://github.com/pytorch/torchrec/commit/d0d23bef8aef5a79a1061fbc842c97bb68b91463&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchvision&quot;&gt;TorchVision&lt;/h2&gt;

&lt;h3 id=&quot;beta-extending-torchvisions-transforms-to-object-detection-segmentation--video-tasks&quot;&gt;[Beta] Extending TorchVision’s Transforms to Object Detection, Segmentation &amp;amp; Video tasks&lt;/h3&gt;

&lt;p&gt;TorchVision is extending its Transforms API! Here is what’s new:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You can use them not only for Image Classification but also for Object Detection, Instance &amp;amp; Semantic Segmentation and Video Classification.&lt;/li&gt;
  &lt;li&gt;You can use new functional transforms for transforming Videos, Bounding Boxes and Segmentation Masks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about these new transforms &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/&quot;&gt;from our docs&lt;/a&gt;, and submit any feedback in our &lt;a href=&quot;https://github.com/pytorch/vision/issues/6753&quot;&gt;dedicated issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchtext&quot;&gt;TorchText&lt;/h2&gt;

&lt;h3 id=&quot;beta-adding-scriptable-t5-and-flan-t5-to-the-torchtext-library-with-incremental-decoding-support&quot;&gt;[Beta] Adding scriptable T5 and Flan-T5 to the TorchText library with incremental decoding support!&lt;/h3&gt;

&lt;p&gt;TorchText has added the T5 model architecture with pre-trained weights for both the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;original T5 paper&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;&gt;Flan-T5&lt;/a&gt;. The model is fully torchscriptable and features an optimized &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.ao.nn.quantizable.MultiheadAttention.html?highlight=multihead#torch.ao.nn.quantizable.MultiheadAttention&quot;&gt;multiheaded attention implementation&lt;/a&gt;. We include several examples of how to utilize the model including summarization, classification, and translation.&lt;/p&gt;

&lt;p&gt;For more details, please refer to &lt;a href=&quot;https://pytorch.org/text/stable/models.html&quot;&gt;our docs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchx&quot;&gt;TorchX&lt;/h2&gt;

&lt;p&gt;TorchX is moving to community supported mode. More details will be coming in at a later time.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-release/" rel="alternate" type="text/html" title="PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever" />
      <published>2023-03-15T00:00:00-07:00</published>
      <updated>2023-03-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-release</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-release/">&lt;p&gt;We are excited to announce the release of &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.0.0&quot;&gt;PyTorch® 2.0&lt;/a&gt; which we highlighted during the &lt;a href=&quot;https://www.youtube.com/@PyTorch/playlists?view=50&amp;amp;sort=dd&amp;amp;shelf_id=2&quot;&gt;PyTorch Conference&lt;/a&gt; on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.&lt;/p&gt;

&lt;p&gt;This next-generation release includes a Stable version of Accelerated Transformers (formerly called Better Transformers); Beta includes torch.compile as the main API for PyTorch 2.0,  the scaled_dot_product_attention function as part of torch.nn.functional, the MPS backend, functorch APIs in the torch.func module; and other Beta/Prototype improvements across various inferences, performance and training optimization features on GPUs and CPUs. For a comprehensive introduction and technical overview of torch.compile, please visit the 2.0 &lt;a href=&quot;/get-started/pytorch-2.0&quot;&gt;Get Started page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Along with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. More details can be found in this &lt;a href=&quot;/blog/new-library-updates-in-pytorch-2.0/&quot;&gt;library blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This release is composed of over 4,541 commits and 428 contributors since 1.13.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.0 and the overall 2-series this year.&lt;/p&gt;

&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.&lt;/li&gt;
  &lt;li&gt;As an underpinning technology of torch.compile, TorchInductor with Nvidia and AMD GPUs will rely on OpenAI Triton deep learning compiler to generate performant code and hide low level hardware details. OpenAI Triton-generated kernels achieve performance that’s on par with hand-written kernels and specialized cuda libraries such as cublas.&lt;/li&gt;
  &lt;li&gt;Accelerated Transformers introduce high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA). The API is integrated with torch.compile() and model developers may also use the &lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;scaled dot product attention&lt;/a&gt; kernels directly by calling the new scaled_dot_product_attention() operator.&lt;/li&gt;
  &lt;li&gt;Metal Performance Shaders (MPS) backend provides GPU accelerated PyTorch training on Mac platforms with added support for Top 60 most used ops, bringing coverage to over 300 operators.&lt;/li&gt;
  &lt;li&gt;Amazon AWS optimizes the PyTorch CPU inference on AWS Graviton3 based &lt;a href=&quot;https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7g-instances-powered-by-aws-graviton3-processors/&quot;&gt;C7g instances&lt;/a&gt;. PyTorch 2.0 improves inference performance on Graviton compared to the previous releases, including improvements for Resnet50 and Bert.&lt;/li&gt;
  &lt;li&gt;New prototype features and technologies across TensorParallel, DTensor, 2D parallel, TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.&lt;/li&gt;
&lt;/ul&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;
&lt;strong&gt;Stable&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;a href=&quot;#stable-features&quot;&gt;Accelerated PT 2 Transformers&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-features&quot;&gt;torch.compile&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-features&quot;&gt;DTensor&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;CUDA support for 11.7 &amp;amp; 11.8 (deprecating CUDA 11.6) &lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-pytorch-mps-backend&quot;&gt;PyTorch MPS Backend&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-tensorparallel&quot;&gt;TensorParallel&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt; 

&lt;a href=&quot;#deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;Python 3.8 (deprecating Python 3.7)&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;Scaled dot product attention&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-2d-parallel&quot;&gt;2D Parallel&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#optimized-pytorch-inference-with-aws-graviton-processors&quot;&gt;AWS Graviton3&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-functorch---torchfunc&quot;&gt;functorch&lt;/a&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;

&lt;a href=&quot;#beta-torchcompile&quot;&gt;Torch.compile (dynamic=True)&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#beta-dispatchable-collectives&quot;&gt;Dispatchable Collectives&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#beta-torchset_default_device-and-torchdevice-as-context-manager&quot;&gt;Torch.set_default &amp;amp; torch.device&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-x86-as-the-new-default-quantization-backend-for-x86-cpu&quot;&gt;X86 quantization backend&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-gnn-inference-and-training-optimization-on-cpu&quot;&gt;GNN inference and training performance&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public 2.0, 1.13 and 1.12 feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1H3jazwO8BBCwK8JwLNYspLiHfUrzshEtyqjL-X93I9g/edit#gid=790902532&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stable-features&quot;&gt;Stable Features&lt;/h2&gt;

&lt;h3 id=&quot;stable-accelerated-pytorch-2-transformers&quot;&gt;[Stable] Accelerated PyTorch 2 Transformers&lt;/h3&gt;

&lt;p&gt;The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API. In releasing Accelerated PT2 Transformers, our goal is to make training and deployment of state-of-the-art Transformer models affordable across the industry. This release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA), extending the inference “fastpath” architecture, previously known as “Better Transformer.”&lt;/p&gt;

&lt;p&gt;Similar to the “fastpath” architecture, custom kernels are fully integrated into the PyTorch Transformer API – thus, using the native Transformer and MultiHeadAttention API will enable users to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;transparently see significant speed improvements;&lt;/li&gt;
  &lt;li&gt;support many more use cases including models using Cross-Attention, Transformer Decoders, and for training models; and&lt;/li&gt;
  &lt;li&gt;continue to use fastpath inference for fixed and variable sequence length Transformer Encoder and Self Attention use cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To take full advantage of different hardware models and Transformer use cases, multiple SDPA custom kernels are supported (see below), with custom kernel selection logic that will pick the highest-performance kernel for a given model and hardware type. In addition to the existing Transformer API, model developers may also use the &lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;scaled dot product attention&lt;/a&gt; kernels directly by calling the new scaled_dot_product_attention() operator. Accelerated PyTorch 2 Transformers are integrated with torch.compile() .  To use your model while benefiting from the additional acceleration of PT2-compilation (for inference or training), pre-process the model with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model = torch.compile(model)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We have achieved major speedups for training transformer models and in particular large language models with Accelerated PyTorch 2 Transformers using a combination of custom kernels and torch.compile().&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch20post.png&quot; alt=&quot;alt_text&quot; title=&quot;Accelerated PyTorch 2 speed&quot; width=&quot;100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Figure: Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-torchcompile&quot;&gt;[Beta] torch.compile&lt;/h3&gt;

&lt;p&gt;torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.&lt;/p&gt;

&lt;p&gt;Underpinning torch.compile are new technologies – TorchDynamo, AOTAutograd, PrimTorch and TorchInductor:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TorchDynamo captures PyTorch programs safely using Python Frame Evaluation Hooks and is a significant innovation that was a result of 5 years of our R&amp;amp;D into safe graph capture.&lt;/li&gt;
  &lt;li&gt;AOTAutograd overloads PyTorch’s autograd engine as a tracing autodiff for generating ahead-of-time backward traces.&lt;/li&gt;
  &lt;li&gt;PrimTorch canonicalizes ~2000+ PyTorch operators down to a closed set of ~250 primitive operators that developers can target to build a complete PyTorch backend. This substantially lowers the barrier of writing a PyTorch feature or backend.&lt;/li&gt;
  &lt;li&gt;TorchInductor is a deep learning compiler that generates fast code for multiple accelerators and backends. For NVIDIA and AMD GPUs, it uses OpenAI Triton as a key building block. For intel CPUs, we generate C++ code using multithreading, vectorized instructions and offloading appropriate operations to mkldnn when possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all the new technologies, torch.compile is able to work 93% of time across 165 open-source models and runs 20% faster on average at float32 precision and 36% faster on average at AMP precision.&lt;/p&gt;

&lt;p&gt;For more information, please refer to &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;https://pytorch.org/get-started/pytorch-2.0/&lt;/a&gt; and for TorchInductor CPU with Intel &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-update-5-cpu-backend-backend-performance-update-and-deep-dive-on-key-optimizations/1117&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-pytorch-mps-backend&quot;&gt;[Beta] PyTorch MPS Backend&lt;/h3&gt;

&lt;p&gt;MPS backend provides GPU-accelerated PyTorch training on Mac platforms. This release brings improved correctness, stability, and operator coverage.&lt;/p&gt;

&lt;p&gt;MPS backend now includes support for the Top 60 most used ops, along with the most frequently requested operations by the community, bringing coverage to over 300 operators. The major focus of the release was to enable full OpInfo-based forward and gradient mode testing to address silent correctness issues. These changes have resulted in wider adoption of MPS backend by 3rd party networks such as Stable Diffusion, YoloV5, WhisperAI, along with increased coverage for Torchbench networks and Basic tutorials. We encourage developers to update to the latest macOS release to see the best performance and stability on the MPS backend.&lt;/p&gt;

&lt;p&gt;Links&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/mps.html&quot;&gt;MPS Backend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/MPS-Backend&quot;&gt;Developer information&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.apple.com/metal/pytorch/&quot;&gt;Accelerated PyTorch training on Mac&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.apple.com/documentation/metal?language=objc&quot;&gt;Metal&lt;/a&gt;, &lt;a href=&quot;https://developer.apple.com/documentation/metalperformanceshaders?language=objc&quot;&gt;Metal Performance Shaders&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://developer.apple.com/documentation/metalperformanceshadersgraph?language=objc&quot;&gt;Metal Performance Shaders Graph&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;beta-scaled-dot-product-attention-20&quot;&gt;[Beta] Scaled dot product attention 2.0&lt;/h3&gt;

&lt;p&gt;We are thrilled to announce the release of PyTorch 2.0, which introduces a powerful scaled dot product attention function as part of torch.nn.functional. This function includes multiple implementations that can be seamlessly applied depending on the input and hardware in use.&lt;/p&gt;

&lt;p&gt;In previous versions of PyTorch, you had to rely on third-party implementations and install separate packages to take advantage of memory-optimized algorithms like &lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;FlashAttention&lt;/a&gt;. With PyTorch 2.0, all these implementations are readily available by default.&lt;/p&gt;

&lt;p&gt;These implementations include &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; from HazyResearch, Memory-Efficient Attention from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; project, and a native C++ implementation that is ideal for non-CUDA devices or when high-precision is required.&lt;/p&gt;

&lt;p&gt;PyTorch 2.0 will automatically select the optimal implementation for your use case, but you can also toggle them individually for finer-grained control. Additionally, the scaled dot product attention function can be used to build common transformer architecture components.&lt;/p&gt;

&lt;p&gt;Learn more with the &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention&quot;&gt;documentation&lt;/a&gt; and this &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-functorch---torchfunc&quot;&gt;[Beta] functorch -&amp;gt; torch.func&lt;/h3&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://github.com/google/jax&quot;&gt;Google JAX&lt;/a&gt;, functorch is a library that offers composable vmap (vectorization) and autodiff transforms. It enables advanced autodiff use cases that would otherwise be tricky to express in PyTorch. Examples include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ensembling.html&quot;&gt;model ensembling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/jacobians_hessians.html&quot;&gt;efficiently computing jacobians and hessians&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/per_sample_grads.html&quot;&gt;computing per-sample-gradients (or other per-sample quantities)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re excited to announce that, as the final step of upstreaming and integrating functorch into PyTorch, the functorch APIs are now available in the torch.func module. Our function transform APIs are identical to before, but we have changed how the interaction with NN modules work. Please see the &lt;a href=&quot;https://pytorch.org/docs/master/func.html&quot;&gt;docs&lt;/a&gt; and the &lt;a href=&quot;https://pytorch.org/docs/master/func.migrating.html&quot;&gt;migration guide&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Furthermore, we have &lt;a href=&quot;https://pytorch.org/docs/master/notes/extending.func.html&quot;&gt;added support for torch.autograd.Function&lt;/a&gt;: one is now able to apply function transformations (e.g. vmap, grad, jvp) over torch.autograd.Function.&lt;/p&gt;

&lt;h3 id=&quot;beta-dispatchable-collectives&quot;&gt;[Beta] Dispatchable Collectives&lt;/h3&gt;

&lt;p&gt;Dispatchable collectives is an improvement to the existing init_process_group() API which changes backend to an optional argument. For users, the main advantage of this feature is that it will allow them to write code that can run on both GPU and CPU machines without having to change the backend specification. The dispatchability feature will also make it easier for users to support both GPU and CPU collectives, as they will no longer need to specify the backend manually (e.g. “NCCL” or “GLOO”). Existing backend specifications by users will be honored and will not require change.&lt;/p&gt;

&lt;p&gt;Usage example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch.distributed.dist
…
# old
dist.init_process_group(backend=”nccl”, ...)
dist.all_reduce(...) # with CUDA tensors works
dist.all_reduce(...) # with CPU tensors does not work

# new
dist.init_process_group(...) # backend is optional
dist.all_reduce(...) # with CUDA tensors works
dist.all_reduce(...) # with CPU tensors works
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/docs/master/distributed.html#torch.distributed.init_process_group&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchset_default_device-and-torchdevice-as-context-manager&quot;&gt;[Beta] torch.set_default_device and torch.device as context manager&lt;/h3&gt;

&lt;p&gt;torch.set_default_device allows users to change the default device that factory functions in PyTorch allocate on. For example, if you torch.set_default_device(‘cuda’), a call to torch.empty(2) will allocate on CUDA (rather than on CPU). You can also use torch.device as a context manager to change the default device on a local basis. This resolves a long standing feature request from PyTorch’s initial release for a way to do this.&lt;/p&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-x86-as-the-new-default-quantization-backend-for-x86-cpu&quot;&gt;[Beta] “X86” as the new default quantization backend for x86 CPU&lt;/h3&gt;

&lt;p&gt;The new X86 quantization backend, which utilizes FBGEMM and oneDNN kernel libraries, replaces FBGEMM as the default quantization backend for x86 CPU platforms and offers improved int8 inference performance compared to the original FBGEMM backend, leveraging the strengths of both libraries, with 1.3X – 2X inference performance speedup measured on 40+ deep learning models. The new backend is functionally compatible with the original FBGEMM backend.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table: Geomean Speedup of X86 Quantization Backend vs. FBGEMM Backend&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt; 
   &lt;/td&gt;
   &lt;td&gt;1 core/instance
   &lt;/td&gt;
   &lt;td&gt;2 cores/instance
   &lt;/td&gt;
   &lt;td&gt;4 cores/instance
   &lt;/td&gt;
   &lt;td&gt;1 socket (32 cores)/instance
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz
   &lt;/td&gt;
   &lt;td&gt;1.76X
   &lt;/td&gt;
   &lt;td&gt;1.80X
   &lt;/td&gt;
   &lt;td&gt;2.04X
   &lt;/td&gt;
   &lt;td&gt;1.34X
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;By default, users on x86 platforms will utilize the x86 quantization backend and their PyTorch programs will remain unchanged when using the default backend. Alternatively, users have the option to specify “X86” as the quantization backend explicitly. Example code is shown below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.ao.quantization import get_default_qconfig_mappingfrom torch.quantization.quantize_fx
import prepare_fx, convert_fx
 
# get default configuration
qconfig_mapping = get_default_qconfig_mapping()
 
# or explicitly specify the backend
# qengine = 'x86'
# torch.backends.quantized.engine = qengine
# qconfig_mapping = get_default_qconfig_mapping(qengine)
 
# construct fp32 model
model_fp32 = ...
 
# prepare
prepared_model = prepare_fx(model_fp32, qconfig_mapping, example_inputs=x)
 
# calibrate
...
 
# convert
quantized_model = convert_fx(prepared_model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Find more information: &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/83888&quot;&gt;https://github.com/pytorch/pytorch/issues/83888&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html&quot;&gt;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-gnn-inference-and-training-optimization-on-cpu&quot;&gt;[Beta] GNN inference and training optimization on CPU&lt;/h3&gt;

&lt;p&gt;PyTorch 2.0 includes several critical optimizations to improve GNN inference and training performance on CPU. Before 2.0, GNN models of PyG suffers from low efficiency on CPU due to lack of performance tuning for several critical kernels (scatter/gather, etc) and the lack of GNN-related sparse matrix multiplication ops. To be specific, optimizations include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;scatter_reduce: performance hotspot in Message Passing when the edge index is stored in Coordinate format (COO).&lt;/li&gt;
  &lt;li&gt;gather: backward of scatter_reduce, specially tuned for the GNN compute when the index is an expanded tensor.&lt;/li&gt;
  &lt;li&gt;torch.sparse.mm with reduce flag: performance hotspot in Message Passing when the edge index is stored in Compressed Sparse Row (CSR). Supported reduce flag of: sum, mean, amax, amin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On PyG benchmarks/examples, OGB benchmarks, a 1.12x - 4.07x performance speedup is measured (1.13.1 compared with 2.0) for single node inference and training.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Model-Dataset&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Speedup Ratio&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;5&quot;&gt;GCN-Reddit (inference)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.25x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.31x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.68x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt; 
GraphSage-ogbn-products (inference)
   &lt;/td&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.15x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.33x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;full-batch-sparse
   &lt;/td&gt;
   &lt;td&gt;4.07x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-PROTEINS (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-REDDIT-BINARY (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;GCN-Reddit (training)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.12x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Learn more: &lt;a href=&quot;https://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;PyG CPU Performance Optimization&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-accelerating-inference-on-cpu-with-pytorch-by-leveraging-onednn-graph&quot;&gt;[Beta] Accelerating inference on CPU with PyTorch by leveraging oneDNN Graph&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;https://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on AI hardware.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It automatically identifies the graph partitions to be accelerated via fusion.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul and their neighbor operations for both inference and training use cases.&lt;/li&gt;
  &lt;li&gt;Although work is ongoing to integrate oneDNN Graph with TorchDynamo as well, its integration with the PyTorch JIT Fuser attained beta status in PyTorch 2.0 for &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-float&quot;&gt;Float32&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16&lt;/a&gt; inference (on machines that support AVX512_BF16 ISA).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From a developer’s/researcher’s perspective, the usage is quite simple &amp;amp; intuitive, with the only change in code being an API invocation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Leverage oneDNN Graph, with &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.jit.trace.html&quot;&gt;JIT-tracing&lt;/a&gt;, a model is profiled with an example input.&lt;/li&gt;
  &lt;li&gt;The context manager &lt;em&gt;with torch.jit.fuser(“fuser3”):&lt;/em&gt; can also be used instead of invoking &lt;em&gt;torch.jit.enable_onednn_fusion(True)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;For accelerating &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16 inference&lt;/a&gt;, we rely on eager-mode AMP (Automatic Mixed Precision) support in PyTorch &amp;amp; disable JIT mode’s AMP, as both of them are currently divergent:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Assuming we have a model of the name 'model'
 
example_input = torch.rand(1, 3, 224, 224)
 
# enable oneDNN Graph
torch.jit.enable_onednn_fusion(True)
# Disable AMP for JIT
torch._C._jit_set_autocast_mode(False)
with torch.no_grad(), torch.cpu.amp.autocast():
	model = torch.jit.trace(model, (example_input))
	model = torch.jit.freeze(model)
 	# 2 warm-ups (2 for tracing/scripting with an example, 3 without an example)
	model(example_input)
	model(example_input)
 
	# speedup would be observed in subsequent runs.
	model(example_input)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;distributed-api&quot;&gt;Distributed API&lt;/h3&gt;

&lt;h4 id=&quot;prototype-dtensor&quot;&gt;[Prototype] DTensor&lt;/h4&gt;

&lt;p&gt;PyTorch &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/distributed/_tensor/README.md&quot;&gt;DistributedTensor&lt;/a&gt; (DTensor) is a prototyping effort with distributed tensor primitives to allow easier distributed computation authoring in the SPMD (Single Program Multiple Devices) paradigm. The primitives are simple but powerful when used to express tensor distributions with both sharded and replicated parallelism strategies. PyTorch DTensor empowered PyTorch &lt;a href=&quot;https://pytorch.org/docs/master/distributed.tensor.parallel.html&quot;&gt;Tensor Parallelism&lt;/a&gt; along with other advanced parallelism explorations. In addition, it also offers a uniform way to save/load state_dict for distributed checkpointing purposes, even when there’re complex tensor distribution strategies such as combining tensor parallelism with parameter sharding in FSDP. More details can be found in this &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/88838&quot;&gt;RFC&lt;/a&gt; and the &lt;a href=&quot;https://colab.research.google.com/drive/12Pl5fvh0eLPUrcVO7s6yY4n2_RZo8pLR#scrollTo=stYPKb9Beq4e&quot;&gt;DTensor examples notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-tensorparallel&quot;&gt;[Prototype] TensorParallel&lt;/h4&gt;

&lt;p&gt;We now support DTensor based Tensor Parallel which users can distribute their model parameters across different GPU devices. We also support Pairwise Parallel which shards two concatenated linear layers in a col-wise and row-wise style separately so that only one collective(all-reduce/reduce-scatter) is needed in the end. More details can be found in this &lt;a href=&quot;https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/example.py&quot;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-2d-parallel&quot;&gt;[Prototype] 2D Parallel&lt;/h4&gt;

&lt;p&gt;We implemented the integration of the aforementioned TP with FullyShardedDataParallel(FSDP) as 2D parallel to further scale large model training. More details can be found in this &lt;a href=&quot;https://docs.google.com/presentation/d/17g6WqrO00rP3MsxbRENsPpjrlSkwiA_QB4r93_eB5is/edit?usp=sharing&quot;&gt;slide&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/test/distributed/tensor/parallel/test_2d_parallel.py&quot;&gt;code example&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-torchcompiledynamictrue&quot;&gt;[Prototype] torch.compile(dynamic=True)&lt;/h4&gt;

&lt;p&gt;Experimental support for PT2 compilation with dynamic shapes is available in this release. Inference compilation with inductor for simple models is supported, but there are a lot of limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training available in a future release (This is partially fixed in nightlies!)&lt;/li&gt;
  &lt;li&gt;Minifier available in a future release.&lt;/li&gt;
  &lt;li&gt;It is easy to end up in a situation where the dimension you wanted to be dynamic gets specialized anyway. Some of these issues are fixed in nightlies, others are not.&lt;/li&gt;
  &lt;li&gt;We do not appropriately propagate Inductor guards to the top-level, this is tracked at &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/96296&quot;&gt;#96296&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Data-dependent operations like nonzero still require a graph break.&lt;/li&gt;
  &lt;li&gt;Dynamic does not work with non-standard modes like reduce-overhead or max-autotune.&lt;/li&gt;
  &lt;li&gt;There are many bugs in Inductor compilation. To track known bugs, check the &lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+dynamic+shapes%22&quot;&gt;dynamic shapes&lt;/a&gt; label on the PyTorch issue tracker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the latest and greatest news about dynamic shapes support on master, check out &lt;a href=&quot;https://dev-discuss.pytorch.org/t/state-of-symbolic-shapes-branch/777/43&quot;&gt;our status reports&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;highlightsperformance-improvements&quot;&gt;Highlights/Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;&lt;a href=&quot;https://pytorch.org/blog/deprecation-cuda-python-support/&quot;&gt;Deprecation of Cuda 11.6 and Python 3.7 support&lt;/a&gt; for PyTorch 2.0&lt;/h3&gt;

&lt;p&gt;If you are still using or depending on CUDA 11.6 or Python 3.7 builds, we strongly recommend moving to at least CUDA 11.7 and Python 3.8, as it would be the minimum versions required for PyTorch 2.0. For more detail, please refer to the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/RELEASE.md#release-compatibility-matrix&quot;&gt;Release Compatibility Matrix for PyTorch&lt;/a&gt; releases.&lt;/p&gt;

&lt;h3 id=&quot;python-311-support-on-anaconda-platform&quot;&gt;Python 3.11 support on Anaconda Platform&lt;/h3&gt;

&lt;p&gt;Due to lack of Python 3.11 support for packages that PyTorch depends on, including NumPy, SciPy, SymPy, Pillow and others on the Anaconda platform. We will not be releasing Conda binaries compiled with Python 3.11 for PyTorch Release 2.0. The Pip packages with Python 3.11 support will be released, hence if you intend to use PyTorch 2.0 with Python 3.11 please use our Pip packages. Please note: Conda packages with Python 3.11 support will be made available on our nightly channel. Also we are planning on releasing Conda Python 3.11 binaries as part of future release once Anaconda provides these key dependencies. More information and instructions on how to download the Pip packages can be found &lt;a href=&quot;https://dev-discuss.pytorch.org/t/pytorch-2-0-message-concerning-python-3-11-support-on-anaconda-platform/1087&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;optimized-pytorch-inference-with-aws-graviton-processors&quot;&gt;Optimized PyTorch Inference with AWS Graviton processors&lt;/h3&gt;

&lt;p&gt;The optimizations focused on three key areas: GEMM kernels, bfloat16 support, primitive caching and the memory allocator. For aarch64 platforms, PyTorch supports Arm Compute Library (ACL) GEMM kernels via Mkldnn(OneDNN) backend. The ACL library provides Neon/SVE GEMM kernels for fp32 and bfloat16 formats. The bfloat16 support on c7g allows efficient deployment of bfloat16 trained, AMP (Automatic Mixed Precision) trained, or even the standard fp32 trained models. The standard fp32 models leverage bfloat16 kernels via OneDNN fast math mode, without any model quantization. Next we implemented primitive caching for conv, matmul and inner product operators. More information on the updated PyTorch user guide with the upcoming 2.0 release improvements and TorchBench benchmark details can be found &lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.0 which we highlighted during the PyTorch Conference on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Democratizing AI with PyTorch Foundation and ROCm™ support for PyTorch</title>
      <link href="https://pytorch.org/blog/democratizing-ai-with-pytorch/" rel="alternate" type="text/html" title="Democratizing AI with PyTorch Foundation and ROCm™ support for PyTorch" />
      <published>2023-02-14T00:00:00-08:00</published>
      <updated>2023-02-14T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/democratizing-ai-with-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/democratizing-ai-with-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/2023-02-14-democratizing-ai-with-pytorch-1.png&quot; alt=&quot;AMD Founding Member&quot; width=&quot;50%&quot; style=&quot;display:block; margin-left:auto; margin-right:auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Last year, Meta announced that &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; joined the Linux Foundation as a neutral home for growing the machine learning project and community with AMD representation as a part of the founding membership and governing board.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;PyTorch Foundation’s&lt;/a&gt; mission is to drive AI adoption by democratizing its software ecosystem through open source principles aligning with the AMD core principle of an Open software ecosystem. AMD strives to foster innovation through the support for latest generations of hardware, tools, libraries, and other components to simplify and accelerate adoption of AI across a broad range of scientific discoveries.&lt;/p&gt;

&lt;div class=&quot;d-md-flex&quot;&gt;
&lt;div style=&quot;flex-basis: 60%;&quot;&gt;
&lt;p&gt;
AMD, along with key PyTorch codebase developers (including those at Meta AI), delivered a set of updates to the &lt;a href=&quot;https://www.amd.com/en/graphics/servers-solutions-rocm&quot; target=&quot;_blank&quot;&gt;ROCm™&lt;/a&gt; open software ecosystem that brings stable support for &lt;a href=&quot;https://www.amd.com/en/graphics/instinct-server-accelerators&quot; target=&quot;_blank&quot;&gt;AMD Instinct™&lt;/a&gt; accelerators as well as many Radeon™ GPUs. This now gives PyTorch developers the ability to build their next great AI solutions leveraging AMD GPU accelerators &amp;amp; ROCm. The support from PyTorch community in identifying gaps, prioritizing key updates, providing feedback for performance optimizing and supporting our journey from “Beta” to “Stable” was immensely helpful and we deeply appreciate the strong collaboration between the two teams at AMD and PyTorch. The move for ROCm support from “Beta” to “Stable” came in the PyTorch 1.12 release (June 2022) brings the added support to easily run PyTorch on native environment without having to configure custom dockers. This is a sign of confidence about the quality of support and performance of PyTorch using AMD Instinct and ROCm. The results of these collaborative efforts are evident in the performance measured on key industry benchmarks like Microsoft’s SuperBench shown below in Graph 1.
&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&quot;
  border: 1px solid #d0d0d0;
  border-radius: 10px;
  box-sizing: border-box;
  -webkit-filter: drop-shadow(0 2px 5px rgba(0,0,0,.1));
  filter: drop-shadow(0 2px 5px rgba(0,0,0,.1));
  padding: 30px;
  background-color: #f8f8f8;
  margin: 20px;
  color: black;
  font-size: 1.6rem;
  flex-basis: 40%;
&quot;&gt;
&lt;p&gt;
&lt;em&gt;“We are excited to see the significant impact of developers at AMD to contribute to and extend features within PyTorch to make AI models run in a more performant, efficient, and scalable way. A great example of this is the thought-leadership around unified memory approaches between the framework and future hardware systems, and we look forward to seeing that feature progress.”&lt;/em&gt;&lt;br /&gt; 
- Soumith Chintala, PyTorch lead-maintainer and Director of Engineering, Meta AI
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The progressive improvements on both the AMD CDNA™ architecture as well as ROCm and PyTorch shows single GPU model throughput increase from AMD Instinct MI100 to the latest generation AMD Instinct MI200 family GPUs going from ROCm 4.2 to ROCm 5.3 and from PyTorch 1.7 to PyTorch 1.12.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-02-14-democratizing-ai-with-pytorch-2.png&quot; alt=&quot;Graph 1: ML model performance over generation using Microsoft Superbench Suite&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;Graph 1: ML model performance over generation using Microsoft Superbench Suite &lt;sup&gt;1, 2, 3&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Below are a few of the key updates for ROCm support since the PyTorch 1.12 release&lt;/p&gt;

&lt;h2 id=&quot;full-continuous-integration-ci-for-rocm-on-pytorch&quot;&gt;Full Continuous Integration (CI) for ROCm on PyTorch&lt;/h2&gt;

&lt;p&gt;With the ROCm support for PyTorch move from “Beta” to “Stable,” all the functions and features commits are now verified through a full Continuous Integration (CI) process. The CI process helps ensure the proper build and test process ahead of an expected Docker and PIP wheel release with stable commits forthcoming.&lt;/p&gt;

&lt;h2 id=&quot;support-for-kineto-profiler&quot;&gt;Support for &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto Profiler&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The addition of Kineto profiler support to ROCm now helps developers and users understand performance bottlenecks through effective diagnosis and profiling tools. The tool also provides recommendations to improve known issues and visualization through TensorBoard UI.&lt;/p&gt;

&lt;h2 id=&quot;key-pytorch-libraries-support-added&quot;&gt;Key PyTorch Libraries support added&lt;/h2&gt;

&lt;p&gt;PyTorch ecosystem libraries like &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;TorchText&lt;/a&gt; (Text classification), &lt;a href=&quot;https://pytorch.org/torchrec/&quot;&gt;TorchRec&lt;/a&gt; (libraries for recommender systems - RecSys), &lt;a href=&quot;https://pytorch.org/vision/stable/index.html&quot;&gt;TorchVision&lt;/a&gt; (Computer Vision), &lt;a href=&quot;https://pytorch.org/audio/stable/index.html&quot;&gt;TorchAudio&lt;/a&gt; (audio and signal processing) are fully supported since ROCm 5.1 and upstreamed with PyTorch 1.12.&lt;/p&gt;

&lt;p&gt;Key libraries provided with the ROCm software stack including &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/MIOpen&quot;&gt;MIOpen&lt;/a&gt; (Convolution models), &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/rccl&quot;&gt;RCCL&lt;/a&gt; (ROCm Collective Communications) and &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/rocBLAS&quot;&gt;rocBLAS&lt;/a&gt; (BLAS for transformers) were further optimized to offer new potential efficiencies and higher performance.&lt;/p&gt;

&lt;p&gt;MIOpen innovates on several fronts, such as implementing fusion to optimize for memory bandwidth and GPU launch overheads, providing an auto-tuning infrastructure to overcome the large design space of problem configurations, and implementing different algorithms to optimize convolutions for different filter and input sizes. MIOpen is one of the first libraries to publicly support the bfloat16 data-type for convolutions, allowing efficient training at lower precision maintaining expected accuracy.&lt;/p&gt;

&lt;p&gt;RCCL (pronounced “Rickle”) is a stand-alone library of standard collective communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, reduce-scatter, gather, scatter, and all-to-all. There is support for direct GPU-to-GPU send and receive operations. It has been optimized to achieve high bandwidth on platforms using PCIe®, Infinity Fabric™ (GPU to GPU) as well as networking using InfiniBand Verbs or TCP/IP sockets. RCCL supports an arbitrary number of GPUs installed in single or multiple nodes and can be used in either single- or multi-process (e.g., MPI) applications.&lt;/p&gt;

&lt;p&gt;Along with the above key highlights, over 50 features and functionality improvements were completed jointly between AMD and PyTorch to add stable support for ROCm. These include improvements to tools, compilers, runtime, graph optimizations through TorchScript, INT8 quant path usage, and &lt;a href=&quot;https://onnxruntime.ai/&quot;&gt;ONNX runtime integration&lt;/a&gt; including support for Navi 21 based Radeon™ PRO datacenter graphics card to name a few.&lt;/p&gt;

&lt;h2 id=&quot;aitemplate-inference-engine&quot;&gt;&lt;a href=&quot;https://github.com/facebookincubator/AITemplate&quot;&gt;AITemplate&lt;/a&gt; Inference Engine&lt;/h2&gt;

&lt;p&gt;MetaAI recently published a blog announcing the release of its open source AITemplate (&lt;a href=&quot;https://ai.facebook.com/blog/gpu-inference-engine-nvidia-amd-open-source/&quot;&gt;link&lt;/a&gt;) for a unified inference system supporting AMD Instinct GPU accelerators using the AMD ROCm stack. This Python based framework can help significantly improve performance through increased utilization of AMD matrix cores for transformer blocks. This is achieved through the AMD &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/composable_kernel&quot;&gt;Composable Kernel (CK) library&lt;/a&gt; which provides performance critical Kernels for ML AI workloads across multiple architectures including GPUs and CPUs through HIP &amp;amp; C++.&lt;/p&gt;

&lt;p&gt;Moreover, the AITemplate also provides out-of-the-box support for widely used AI models like BERT, ResNET, Vision Transformer, Stable Diffusion etc. simplifying deployment process through these pretrained models.&lt;/p&gt;

&lt;h2 id=&quot;whats-coming-with-future-rocm-releases&quot;&gt;What’s coming with future ROCm releases?&lt;/h2&gt;

&lt;h3 id=&quot;unified-memory-models-for-cpu--gpu&quot;&gt;Unified memory models for CPU + GPU&lt;/h3&gt;

&lt;p&gt;As system architecture evolves to address the complexity of large problem sizes and data sets, memory management becomes a key performance bottle neck that needs a cohesive strategy to be addressed through innovations at both hardware and software levels. AMD is uniquely positioned to address this problem with its effective data center solutions integrating AMD EPYC™ CPU cores with its AMD Instinct GPU compute units in a truly unified datacenter APU (Accelerated Processing Unit) form factor set to be launched in 2H 2023.&lt;/p&gt;

&lt;p&gt;The software work to leverage the unified CPU + GPU memory has already started in collaboration with the PyTorch team, to enable the usage of a fast, low latency, synchronized memory model that enables not only AMD but also other AI accelerators to address the complex memory management problem of today. We are looking forward to this joint effort and announcement soon.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The content in this blog highlights the joint work between AMD and key PyTorch contributors including Meta, working on many of the core features, as well as Microsoft enabling ONNX Runtime support. We are looking forward to working with the other founding members at the PyTorch Foundation on the next steps and improvements to democratize and grow adoption of PyTorch across the industry.&lt;/p&gt;

&lt;h2 id=&quot;cautionary-statement&quot;&gt;CAUTIONARY STATEMENT&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;
This blog contains forward-looking statements concerning Advanced Micro Devices, Inc. (AMD) such as the availability, timing and expected benefits of an AMD datacenter APU form factor, which are made pursuant to the Safe Harbor provisions of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are commonly identified by words such as “would,” “may,” “expects,” “believes,” “plans,” “intends,” “projects” and other terms with similar meaning. Investors are cautioned that the forward-looking statements in this blog are based on current beliefs, assumptions and expectations, speak only as of the date of this blog and involve risks and uncertainties that could cause actual results to differ materially from current expectations. Such statements are subject to certain known and unknown risks and uncertainties, many of which are difficult to predict and generally beyond AMD’s control, that could cause actual results and other future events to differ materially from those expressed in, or implied or projected by, the forward-looking information and statements. Investors are urged to review in detail the risks and uncertainties in AMD’s Securities and Exchange Commission filings, including but not limited to AMD’s most recent reports on Forms 10-K and 10-Q. AMD does not assume, and hereby disclaims, any obligation to update forward-looking statements made in this blog, except as may be required by law. 
&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;endnotes&quot;&gt;Endnotes&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;small&gt;MI100D-01 SuperBench v0.5 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™ 7763 CPU server tested with 1x AMD Instinct™ MI100 (32GB HBM2e) 300W GPU, SBIOS 2.2, Ubuntu® 20.04.5 LTS, host ROCm™ 5.2.0, guest ROCm 4.2,    PyTorch 1.7.0. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
  &lt;li&gt;&lt;small&gt;MI200D-01 SuperBench v0.6 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™ 7763 CPU server tested with 1x AMD Instinct™ MI210 (64GB HBM2e) 300W GPU, SBIOS 2.2, Ubuntu 20.04.5 LTS, host ROCm 5.3.0, guest ROCm 5.3, PyTorch 1.12. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
  &lt;li&gt;&lt;small&gt;MI200D-02: SuperBench v0.6 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™️ 7763 CPU server tested with 1x AMD Instinct™️ MI250 (128GB HBM2e) 560W GPU, SBIOS M12, Ubuntu 20.04 LTS, host ROCm 5.3.0, guest ROCm 5.3, PyTorch 1.12. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deprecation of CUDA 11.6 and Python 3.7 Support</title>
      <link href="https://pytorch.org/blog/deprecation-cuda-python-support/" rel="alternate" type="text/html" title="Deprecation of CUDA 11.6 and Python 3.7 Support" />
      <published>2023-02-02T00:00:00-08:00</published>
      <updated>2023-02-02T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/deprecation-cuda-python-support</id>
      <content type="html" xml:base="https://pytorch.org/blog/deprecation-cuda-python-support/">&lt;p&gt;For the upcoming PyTorch 2.0 feature release (target March 2023), we will target CUDA 11.7 as the stable version and CUDA 11.8 as the experimental version of CUDA and Python &amp;gt;=3.8, &amp;lt;=3.11.&lt;/p&gt;

&lt;p&gt;If you are still using or depending on CUDA 11.6 or Python 3.7 builds, we strongly recommend moving to at least CUDA 11.7 and Python 3.8, as it would be the minimum versions required for PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note that as of Feb 1, CUDA 11.6 and Python 3.7  are no longer included in the nightlies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please refer to the Release Compatibility Matrix for PyTorch releases:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch Version&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Python&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Stable CUDA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Experimental CUDA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2.0
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.8, &amp;lt;=3.11
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.7, CUDNN 8.5.0.96
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.8, CUDNN 8.7.0.84
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.13
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.7, &amp;lt;=3.10
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.6, CUDNN 8.3.2.44
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.7, CUDNN 8.5.0.96
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.12
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.7, &amp;lt;=3.10
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.3, CUDNN 8.3.2.44
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.6, CUDNN 8.3.2.44
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As of 2/1/2023&lt;/p&gt;

&lt;p&gt;For more information on PyTorch releases, updated compatibility matrix and release policies, please see (and bookmark) &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/RELEASE.md#release-compatibility-matrix&quot;&gt;Readme&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">For the upcoming PyTorch 2.0 feature release (target March 2023), we will target CUDA 11.7 as the stable version and CUDA 11.8 as the experimental version of CUDA and Python &amp;gt;=3.8, &amp;lt;=3.11.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Trace Analysis for the Masses</title>
      <link href="https://pytorch.org/blog/trace-analysis-for-masses/" rel="alternate" type="text/html" title="PyTorch Trace Analysis for the Masses" />
      <published>2023-01-09T00:00:00-08:00</published>
      <updated>2023-01-09T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/trace-analysis-for-masses</id>
      <content type="html" xml:base="https://pytorch.org/blog/trace-analysis-for-masses/">&lt;p&gt;We are excited to announce the public release of Holistic Trace Analysis (HTA), an open source performance analysis and visualization Python library for PyTorch users. HTA takes as input &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto traces&lt;/a&gt; collected by the &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/&quot;&gt;PyTorch profiler&lt;/a&gt;, which are complex and challenging to interpret, and up-levels the performance information contained in these traces. It was initially developed internally at Meta to understand and debug performance problems for large-scale distributed training jobs on GPUs. The multidisciplinary team has made a number of enhancements to HTA’s features and scaled them to support state-of-the-art ML workloads.&lt;/p&gt;

&lt;p&gt;ML researchers and systems engineers often struggle to computationally scale up their models because they are not aware of the performance bottlenecks in their workloads. The resources requested for a job (e.g. GPUs, memory) are often misaligned with the resources actually required due to lack of visibility “under the hood”. To achieve the best performance from the hardware stack, it is imperative to understand the resource utilization and bottlenecks for distributed training workloads.&lt;/p&gt;

&lt;p&gt;The initial HTA implementation was specifically targeted at Deep Learning Based Recommendation Models (DLRM). To make the features in HTA generic and applicable to use cases such as analyzing Vision and NLP models, we decided to refactor the HTA codebase and make the library available to the larger community. This new codebase has implemented several important ideas which lead to significant efficiency and performance improvements.&lt;/p&gt;

&lt;p&gt;In this blog, we present several features implemented in the open source version of HTA, which can be used as a Python script as well as interactively in a Jupyter notebook. HTA provides the following features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Breakdown by Dimensions&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Temporal&lt;/strong&gt;: Breakdown of GPU time in terms of time spent in computation, communication, memory events, and idle time on a single node and across all ranks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Idle Time&lt;/strong&gt;: Breakdown of GPU idle time into waiting for the host, waiting for another kernel or attributed to an unknown cause.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kernel&lt;/strong&gt;: Find kernels with the longest duration on each rank.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Communication Computation Overlap&lt;/strong&gt;: Calculate the percentage of time when communication overlaps computation.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical Analysis&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Kernel Duration Distribution&lt;/strong&gt;: Distribution of average time taken by longest kernels across different ranks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;CUDA Kernel Launch&lt;/strong&gt;: Distributions of GPU kernels with very small duration, large duration, and excessive launch time.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Augmented Counters (Memory bandwidth, Queue length)&lt;/strong&gt;: Augmented trace files which provide insights into memory copy bandwidth and number of outstanding operations on each CUDA stream.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Patterns&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Frequent CUDA Kernels&lt;/strong&gt;: Find the CUDA kernels most frequently launched by any given PyTorch or user defined operator.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Trace Comparison&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Trace Diff&lt;/strong&gt;: A trace comparison tool to identify and visualize the differences between traces.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;HTA source code is available to users via &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis&quot;&gt;Github&lt;/a&gt;. Users can request new features or build their own analysis using the core libraries and data structures provided in the codebase in addition to the features mentioned above.&lt;/p&gt;

&lt;h2 id=&quot;gpu-training-performance-debugging-101&quot;&gt;GPU Training Performance Debugging 101&lt;/h2&gt;

&lt;p&gt;To understand the GPU performance in distributed training jobs, we consider how the model operators interact with the GPU devices and how such interactions are reflected in certain measurable metrics.&lt;/p&gt;

&lt;p&gt;At a high level, we can break down the GPU operations in a model execution into three broad categories, henceforth referred to as kernel types:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Computation (COMP)&lt;/strong&gt; - Compute kernels execute compiled routines for matrix multiplication and similar numeric calculations. They are responsible for all of the number-crunching necessary for model execution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication (COMM)&lt;/strong&gt; - Communication kernels are routines which are responsible for exchanging and synchronizing data between different GPU devices in a distributed training job. The NVIDIA Collective Communication Library (NCCL) is a widely used communication library and all its kernels have the prefix “nccl”. Example NCCL kernels include NCCL_AllGather, NCCL_ReduceScatter, NCCL_AllReduce, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory (MEM)&lt;/strong&gt; - Memory kernels manage the memory allocations/deallocations on the GPU devices and data movement between the memory space on the host and the GPUs. The memory kernels include Memcpy_H2D, Memcpy_D2H, Memcpy_D2D, Memset, etc. Here, H represents the Host and D represents the GPU Device. Thus, H2D, D2H, D2D stands for Host to Device, Device to Host and Device to Device respectively.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Because a modern GPU device like the NVIDIA A100 GPU is a massively parallel device which is capable of running multiple kernels simultaneously, it is possible to overlap the computation, communication, and memory kernels to reduce the model execution time. One common technique to achieve the overlap is to utilize multiple CUDA streams. A CUDA stream is a sequence of operations that execute on a GPU device in the order in which they are issued by the host code. Different CUDA streams can be interleaved and even run concurrently, thus achieving the effect of kernel overlap.&lt;/p&gt;

&lt;p&gt;To help understand the above concepts, Figure 1 provides a timeline of the GPU kernels in a sample distributed training job on 8 GPUs for one iteration. In the figure below, each rank represents one GPU and the kernels on each GPU run on 6 CUDA streams. In the right column of the figure, you can see names of the GPU kernels used. In the middle of the figure, you see the overlap between compute and communicate kernels. This figure is created using the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/plot_timeline.ipynb&quot;&gt;plot_timeline example notebook&lt;/a&gt; available in HTA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image6.png&quot; alt=&quot;Figure 1. An example of the execution timeline of GPU Kernels across multiple ranks&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 1. An example of the execution timeline of GPU Kernels across multiple ranks&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The performance of multiple GPU training jobs is affected by multiple factors. Among these factors, how does a model execution create and orchestrate the GPU kernels plays a critical role. HTA provides insights on how the model execution interacts with the GPU devices and highlights the opportunities for performance improvement.&lt;/p&gt;

&lt;p&gt;With the features we built in HTA, we aim to provide users insights into “what is happening under the hood in a distributed GPU training?” We briefly describe these features in the next few paragraphs.&lt;/p&gt;

&lt;h2 id=&quot;features-in-holistic-trace-analysis&quot;&gt;Features in Holistic Trace Analysis&lt;/h2&gt;

&lt;p&gt;For most users, understanding the performance of GPU training jobs is nontrivial. Thus, we built this library to simplify the task of trace analysis and provide the user useful insights by examining the model execution traces. As the first step, we developed features which are important and generic enough so that most users can benefit from this library.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal Breakdown&lt;/strong&gt;: We begin by asking whether the GPU is spending time on computation, communication, memory events, or is it idle? To answer this question, the temporal breakdown feature presents a breakdown in terms of these categories. To achieve high training efficiency the code should maximize time used by computation kernels and minimize idle time and non-compute time (time used by communication or memory kernels). This is accomplished by implementing concurrent execution of computation kernels with communication or memory kernels. &lt;em&gt;Note that, during concurrent execution of computation kernels with communication/memory kernels the time spent by communication/memory kernels is accounted for under compute time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image3.png&quot; alt=&quot;Figure 2: Temporal Breakdown across 8 GPUs&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 2: Temporal Breakdown across 8 GPUs&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel Breakdown&lt;/strong&gt;: It is natural to ask which kernels are taking the most amount of time. The next feature breaks down the time spent within each kernel type (COMM, COMP, MEM) and sorts them by duration. We present this information for each kernel type and for each rank as a pie chart. See figure 3 below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image1.png&quot; alt=&quot;Figure 3: Pie chart of top computation and communication kernels&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 3: Pie chart of top computation and communication kernels&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel Duration Distribution&lt;/strong&gt;: Subsequently, one can also ask - for any given kernel, what is the distribution of the time spent across the ranks? To answer this, HTA generates bar graphs for the average duration of a given kernel across all ranks. Additionally, the error bars in the bar graphs show the minimum and maximum amount of time taken by a given kernel on a given rank. Figure 4 below shows a discrepancy between average duration on rank 0 as compared to other ranks. This anomalous behavior on rank 0 guides the user on where to look for possible bugs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image4.png&quot; alt=&quot;Figure 4: Average duration of NCCL AllReduce Kernel across 8 ranks&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 4: Average duration of NCCL AllReduce Kernel across 8 ranks&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Communication Computation Overlap&lt;/strong&gt;: In distributed training, a significant amount of time is spent in communication and synchronization events among multiple GPU devices. To achieve high GPU efficiency (i.e. TFLOPS/GPU) it is vital to keep the GPU doing actual computation work. In other words, a GPU should not be blocked because of waiting for data from other GPUs. One way to measure the extent to which computation is blocked by data dependencies is to calculate the computation-communication overlap. Higher GPU efficiency is observed if communication events overlap computation events. Lack of communication and computation overlap will lead to the GPU being idle, thus the efficiency would be low. Thus, the communication computation overlap feature calculates the percentage of time communication and computation overlap in a job for each rank and generates a bar graph representation. See figure below. More precisely, we measure the following ratio&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;(time spent in computation while communicating) / (time spent in communication)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image5.png&quot; alt=&quot;Figure 5: Communication computation overlap&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 5: Communication computation overlap&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Augmented Counters (Queue length, Memory bandwidth)&lt;/strong&gt;: To aid in debugging, HTA calculates the memory bandwidth statistics for D2H, H2D and D2D memory copy (memcpy) and memory set (memset) events. Additionally, HTA also computes the number of outstanding CUDA operations on each CUDA stream. We refer to this as queue length. When the queue length on a stream is 1024 or larger new events cannot be scheduled on that stream and the CPU will stall until the GPU events have processed. Additionally, HTA generates a new trace file containing tracks with the memory bandwidth and queue length time series. See Figure 6 below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image2.png&quot; alt=&quot;Figure 6: Memory Bandwidth and Queue Length&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 6: Memory Bandwidth and Queue Length&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These primary features give us a peek into the system performance and help answer “what is happening in the system?”. As HTA evolves, we hope to address “why is X happening?” and also suggest possible solutions to overcome the bottlenecks.&lt;/p&gt;

&lt;h2 id=&quot;installation-and-usage&quot;&gt;Installation and Usage&lt;/h2&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;p&gt;For installing the HTA please refer to the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/README.md&quot;&gt;README&lt;/a&gt;. In brief, the user is required to clone the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis&quot;&gt;repo&lt;/a&gt; and install the necessary Python packages via pip.&lt;/p&gt;

&lt;h3 id=&quot;usage&quot;&gt;Usage&lt;/h3&gt;

&lt;p&gt;This version of Holistic Trace Analysis is currently in beta and we recommend using HTA in a Jupyter notebook. A &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_analysis_demo.ipynb&quot;&gt;demo notebook&lt;/a&gt; is provided for your convenience. To get started, import the hta package in a Jupyter notebook, create a TraceAnalysis object and off we go in exactly two lines of code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hta.trace_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TraceAnalysis&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TraceAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;All trace files for a training or inference job must be stored in a unique folder.&lt;/li&gt;
  &lt;li&gt;Trace files are in json or gzipped json format.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faq&quot;&gt;FAQ&lt;/h2&gt;

&lt;h4 id=&quot;q-how-can-i-install-hta&quot;&gt;Q. How can I install HTA?&lt;/h4&gt;

&lt;p&gt;Please see the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/README.md&quot;&gt;README&lt;/a&gt; in the root directory of the repository.&lt;/p&gt;

&lt;h4 id=&quot;q-is-there-any-documentation-on-the-features-and-api-in-hta&quot;&gt;Q. Is there any documentation on the features and API in HTA?&lt;/h4&gt;

&lt;p&gt;The documentation and detailed API is available &lt;a href=&quot;https://hta.readthedocs.io/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;q-can-you-implement-feature-x&quot;&gt;Q. Can you implement feature X?&lt;/h4&gt;

&lt;p&gt;Depending on how widely the feature is needed and the level of effort required to implement it we would consider developing the feature. Please open a &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/issues&quot;&gt;Github Issue&lt;/a&gt; and tag it with the feature-request label.&lt;/p&gt;

&lt;h4 id=&quot;q-can-i-modify-the-code&quot;&gt;Q. Can I modify the code?&lt;/h4&gt;

&lt;p&gt;Please do and &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/pulls&quot;&gt;send a PR&lt;/a&gt; along the way, if you think it would be useful for others.&lt;/p&gt;

&lt;h4 id=&quot;q-how-can-i-collect-traces-in-pytorch&quot;&gt;Q. How can I collect traces in PyTorch?&lt;/h4&gt;

&lt;p&gt;Please refer to this tutorial &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#use-profiler-to-record-execution-events&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;q-can-hta-be-used-at-production-scale&quot;&gt;Q. Can HTA be used at production scale?&lt;/h4&gt;

&lt;p&gt;Yes, please see a use case study &lt;a href=&quot;https://pytorch.org/blog/performance-debugging-of-production-pytorch-models-at-meta/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Anupam Bhatnagar, Xizhou Feng,  Brian Coutinho, Yifan Liu, Sung-Han Lin, Louis Feng, and Yuzhen Huang</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the public release of Holistic Trace Analysis (HTA), an open source performance analysis and visualization Python library for PyTorch users. HTA takes as input Kineto traces collected by the PyTorch profiler, which are complex and challenging to interpret, and up-levels the performance information contained in these traces. It was initially developed internally at Meta to understand and debug performance problems for large-scale distributed training jobs on GPUs. The multidisciplinary team has made a number of enhancements to HTA’s features and scaled them to support state-of-the-art ML workloads.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Compromised PyTorch-nightly dependency chain between December 25th and December 30th, 2022.</title>
      <link href="https://pytorch.org/blog/compromised-nightly-dependency/" rel="alternate" type="text/html" title="Compromised PyTorch-nightly dependency chain between December 25th and December 30th, 2022." />
      <published>2022-12-31T00:00:00-08:00</published>
      <updated>2022-12-31T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/compromised-nightly-dependency</id>
      <content type="html" xml:base="https://pytorch.org/blog/compromised-nightly-dependency/">&lt;p&gt;If you installed PyTorch-nightly on Linux via pip between December 25, 2022 and December 30, 2022, please uninstall it and torchtriton immediately, and use the latest nightly binaries (newer than Dec 30th 2022).&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;pip3 uninstall &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; torch torchvision torchaudio torchtriton
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;pip3 cache purge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch-nightly Linux packages installed via pip during that time installed a dependency, torchtriton, which was compromised on the Python Package Index (PyPI) code repository and ran a malicious binary. This is what is known as a supply chain attack and directly affects dependencies for packages that are hosted on public package indices.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Users of the PyTorch &lt;strong&gt;stable&lt;/strong&gt; packages &lt;strong&gt;are not&lt;/strong&gt; affected by this issue.&lt;/p&gt;

&lt;h2 id=&quot;how-to-check-if-your-python-environment-is-affected&quot;&gt;How to check if your Python environment is affected&lt;/h2&gt;

&lt;p&gt;The following command searches for the malicious binary in the torchtriton package (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PYTHON_SITE_PACKAGES/triton/runtime/triton&lt;/code&gt;) and prints out whether your current Python environment is affected or not.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;import pathlib;import importlib.util;s=importlib.util.find_spec('triton'); affected=any(x.name == 'triton' for x in (pathlib.Path(s.submodule_search_locations[0] if s is not None else '/' ) / 'runtime').glob('*'));print('You are {}affected'.format('' if affected else 'not '))&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The malicious binary is executed when the triton package is imported, which requires explicit code to do and is not PyTorch’s default behavior.&lt;/p&gt;

&lt;h2 id=&quot;the-background&quot;&gt;The Background&lt;/h2&gt;

&lt;p&gt;At around 4:40pm GMT on December 30 (Friday), we learned about a malicious dependency package (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchtriton&lt;/code&gt;) that was uploaded to the Python Package Index (PyPI) code repository with the same package name as the one we ship on the &lt;a href=&quot;https://download.pytorch.org/whl/nightly&quot;&gt;PyTorch nightly package index&lt;/a&gt;. Since the &lt;a href=&quot;https://github.com/pypa/pip/issues/8606&quot;&gt;PyPI index takes precedence&lt;/a&gt;, this malicious package was being installed instead of the version from our official repository. This design enables somebody to register a package by the same name as one that exists in a third party index, and pip will install their version by default.&lt;/p&gt;

&lt;p&gt;This malicious package has the same name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchtriton&lt;/code&gt; but added in code that uploads sensitive data from the machine.&lt;/p&gt;

&lt;h2 id=&quot;what-we-know&quot;&gt;What we know&lt;/h2&gt;

&lt;p&gt;torchtriton on PyPI contains a malicious triton binary which is installed at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PYTHON_SITE_PACKAGES/triton/runtime/triton&lt;/code&gt;. Its SHA256 hash is listed below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHA256(triton)= 2385b29489cd9e35f92c072780f903ae2e517ed422eae67246ae50a5cc738a0e&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The binary’s main function does the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get system information
    &lt;ul&gt;
      &lt;li&gt;nameservers from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/resolv.conf&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;hostname from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gethostname()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;current username from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getlogin()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;current working directory name from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getcwd()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;environment variables&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Read the following files
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/hosts&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/passwd&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;The first 1,000 files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/*&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/.gitconfig&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/.ssh/*&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upload all of this information, including file contents, via encrypted DNS queries to the domain *.h4ck[.]cfd, using the DNS server wheezy[.]io&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The binary’s file upload functionality is limited to files less than 99,999 bytes in size. It also uploads only the first 1,000 files in $HOME (but all files &amp;lt; 99,999 bytes in the .ssh directory).&lt;/p&gt;

&lt;h2 id=&quot;steps-taken-towards-mitigation&quot;&gt;Steps taken towards mitigation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;torchtriton has been removed as a dependency for our nightly packages and replaced with pytorch-triton (&lt;a href=&quot;https://github.com/pytorch/pytorch/pull/91539&quot;&gt;pytorch/pytorch#91539&lt;/a&gt;) and a dummy package registered on PyPI (so that this issue doesn’t repeat)&lt;/li&gt;
  &lt;li&gt;All nightly packages that depend on torchtriton have been removed from our package indices at https://download.pytorch.org until further notice&lt;/li&gt;
  &lt;li&gt;We have reached out to the PyPI security team to get proper ownership of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchtriton&lt;/code&gt; package on PyPI and to delete the malicious version&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>The PyTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html">If you installed PyTorch-nightly on Linux via pip between December 25, 2022 and December 30, 2022, please uninstall it and torchtriton immediately, and use the latest nightly binaries (newer than Dec 30th 2022).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Torchserve Performance Tuning, Animated Drawings Case-Study</title>
      <link href="https://pytorch.org/blog/torchserve-performance-tuning/" rel="alternate" type="text/html" title="Torchserve Performance Tuning, Animated Drawings Case-Study" />
      <published>2022-12-28T00:00:00-08:00</published>
      <updated>2022-12-28T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/torchserve-performance-tuning</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchserve-performance-tuning/">&lt;p&gt;In this post we discuss performance tuning of Torchserve for serving your models in production. One of the biggest challenges in the life cycle of a ML project is deploying models in production.  This requires a reliable serving solution along with solutions that address the MLOps needs. A robust serving solution needs to provide support for multi model serving, model versioning, metric logging, monitoring and scaling to serve the peak traffic. In this post, we will have an overview of Torchserve and how to tune its performance for production use-cases. We discuss the &lt;a href=&quot;https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/&quot;&gt;Animated Drawings app&lt;/a&gt; from Meta that can turn your human figure sketches to animations and how it could serve the peak traffic with Torchserve. The Animated Drawing’s workflow is below.&lt;/p&gt;

&lt;p&gt;
&lt;img src=&quot;/assets/images/sketch_animator.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/&quot;&gt;https://ai.facebook.com/blog/using-ai-to-bring-childrens-drawings-to-life/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Many AI systems and tools are designed to handle realistic images of humans, children’s drawings add a level of complexity and unpredictability as they are often constructed in abstract, fanciful ways. These types of morphological and stylistic variations can confuse even state-of-the-art AI systems that excel at spotting objects in photorealistic images and drawings.
Meta AI researchers are working to overcome this challenge so that AI systems will be better able to recognize drawings of human figures in the wildly varied ways that children create them. This great blog post provides more details about the Animated Drawings and the approach taken.&lt;/p&gt;

&lt;h2 id=&quot;torchserve&quot;&gt;Torchserve&lt;/h2&gt;

&lt;p&gt;
&lt;img src=&quot;/assets/images/Tuning-flow-chart.png&quot; width=&quot;90%&quot; /&gt;
&lt;center&gt;&lt;i&gt;Fig1. Overall flow of Torchserve performance tuning&lt;/i&gt; &lt;/center&gt;
&lt;/p&gt;

&lt;p&gt;Once you have trained your model, it needs to be integrated into a larger system to have a full-fledged application, we use the term “model serving” to refer to this integration. Basically model serving is making your trained model available to run inferences and subsequent use of the model.&lt;/p&gt;

&lt;p&gt;Torchserve is the Pytorch preferred solution for serving models in production. It is a performant and scalable tool that wraps your model in a HTTP or HTTPS API. It has a frontend implemented in Java that handles multiple tasks from assigning workers for serving models to handling the connection between client and server. Torchserve has a Python backend that is responsible for handling the inference service.&lt;/p&gt;

&lt;p&gt;Torchserve supports multi model serving and versioning for AB test, dynamic batching, logging and metrics. It exposes four APIs for &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/inference_api.md&quot;&gt;inference&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/inference_api.md#explanations-api&quot;&gt;explanations&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/management_api.md&quot;&gt;management&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics_api.md&quot;&gt;metrics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/inference_api.md&quot;&gt;Inference&lt;/a&gt; API is listening on port 8080 and accessible through localhost by default, this can be configured in &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/configuration.md&quot;&gt;Torchserve configuration&lt;/a&gt; and enable getting predictions from the model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/inference_api.md#explanations-api&quot;&gt;Explanation&lt;/a&gt; API uses  Captum under the hood to provide explanations of the model that is being served and listens to the port 8080 as well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/management_api.md#management-api&quot;&gt;Management&lt;/a&gt; API allows to register or unregister and describe a model. It also enables users to  scale up or down the number of workers that serve the model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics_api.md&quot;&gt;Metric&lt;/a&gt; API by default listens to port 8082 and enables us to monitor the model that is being served.&lt;/p&gt;

&lt;p&gt;Torchserve let you scale your model serving and handle the peak traffic by supporting &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/batch_inference_with_ts.md&quot;&gt;batch inference&lt;/a&gt; and multiple  workers that serve your model. Scaling can be done through &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/management_api.md&quot;&gt;management&lt;/a&gt;  API and settings through a &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/configuration.md&quot;&gt;configuration&lt;/a&gt; file. Also, metric API helps you to monitor your model serving through default and customizable metrics.&lt;/p&gt;

&lt;p&gt;Other advanced settings such as the length of the queue for the received requests, maximum wait time for a batch of inputs and many other properties are configurable through a&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/configuration.md&quot;&gt; config file&lt;/a&gt; that can be passed to Torchserve when it is started.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Steps to serve your model with Torchserve&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/getting_started.md#install-torchserve-and-torch-model-archiver&quot;&gt;Install Torchserve, model archiver&lt;/a&gt; and its requirements.&lt;/li&gt;
  &lt;li&gt;Choose a default handler that fits your task (e.g image classification, etc) or author a &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/custom_service.md#custom-handlers&quot;&gt;custom handler&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers#create-model-archive-eager-mode&quot;&gt;Package your model&lt;/a&gt; artifacts (trained model checkpoint and all other necessary files for loading and running your model) and the handler into a “.mar” file using &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/model-archiver/README.md&quot;&gt;Torcharchive&lt;/a&gt; and place it in the model store.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/getting_started.md&quot;&gt;Start serving your model&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/getting_started.md#get-predictions-from-a-model&quot;&gt;Run inference&lt;/a&gt;.
We will discuss model handlers and metrics in more detail here.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;model-handlers&quot;&gt;Model handlers&lt;/h2&gt;

&lt;p&gt;Torchserve uses a handler in the backend to load the models, preprocess the received data, run inference and post-process the response. Handler in torchserve is a &lt;strong&gt;python script&lt;/strong&gt; that all the model initialization, preprocessing, inference and post processing logic goes into.&lt;/p&gt;

&lt;p&gt;Torchserve provides an out of the box handler for a number of applications like image classification, segmentation, object detection and text classification. It also supports custom handlers, in case your use case is not supported in default handlers.&lt;/p&gt;

&lt;p&gt;It provides a great flexibility in custom handlers, this potentially make Torchserve as &lt;strong&gt;multi-framework&lt;/strong&gt; serving tool. Custom handlers let you define your custom logic to initialize a model that can be used also to load models from other frameworks such as ONNX.&lt;/p&gt;

&lt;p&gt;Torchserve &lt;strong&gt;handler&lt;/strong&gt; is made of four main &lt;strong&gt;functions&lt;/strong&gt;, &lt;strong&gt;initialize&lt;/strong&gt;, &lt;strong&gt;preprocess&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;postprocess&lt;/strong&gt; that each return a list. The code snippet below shows an example of a custom handler.&lt;strong&gt;Custom handlers inherit&lt;/strong&gt; from &lt;strong&gt;BaseHandler&lt;/strong&gt; in Torchserve and can &lt;strong&gt;overwrite&lt;/strong&gt; any of the &lt;strong&gt;main&lt;/strong&gt; &lt;strong&gt;functions&lt;/strong&gt;.  Here is an example of the handler used for loading the &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;Detectron2&lt;/a&gt; model for figure detection, this model has been exported to Torchscript and uses model.half() to run the inference with FP16, details are explained in another &lt;a href=&quot;&quot;&gt;section&lt;/a&gt; in this post.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyModelHandler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BaseHandler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manifest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manifest&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;system_properties&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model_dir&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;serialized_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;manifest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;serializedFile&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model_pt_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;serialized_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;cuda:&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gpu_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gpu_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_pt_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map_location&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;request_body&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;body&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;input_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BytesIO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request_body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imdecode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fromstring&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;inference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;postprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_output&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;responses_json&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'classes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pred_classes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'scores'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'scores'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;boxes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'pred_boxes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dumps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;responses_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;responses&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;metrics&quot;&gt;Metrics&lt;/h2&gt;

&lt;p&gt;An essential component in serving models in production is the ability to monitor them. &lt;strong&gt;Torchserve&lt;/strong&gt; &lt;strong&gt;collects&lt;/strong&gt; &lt;strong&gt;system level&lt;/strong&gt; &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics.md&quot;&gt;metrics&lt;/a&gt; regularly and &lt;strong&gt;allows&lt;/strong&gt; adding &lt;strong&gt;custom metrics&lt;/strong&gt; as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics.md#system-metrics&quot;&gt;System level metrics&lt;/a&gt;&lt;/strong&gt; consist of CPU utilization, available and used disk space and memory on the host machine along with number of requests with different response codes (e.g 200-300, 400-500 and above 500). &lt;strong&gt;Custom metrics&lt;/strong&gt; can be &lt;strong&gt;added&lt;/strong&gt; to the metrics as explained &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics.md#custom-metrics-api&quot;&gt;here&lt;/a&gt;. TorchServe logs these two sets of metrics to different log files. Metrics are collected by default at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;System metrics - log_directory/ts_metrics.log&lt;/li&gt;
  &lt;li&gt;Custom metrics - log directory/model_metrics.log&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As mentioned before, Torchserve also exposes &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics_api.md&quot;&gt;metric API&lt;/a&gt;, that by default listens to port 8082 and enables users to query and monitor the collected metrics.  The default metrics endpoint returns Prometheus formatted metrics. You can query metrics using curl requests or point a &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics_api.md#prometheus-server&quot;&gt;Prometheus Server&lt;/a&gt; to the endpoint and use &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/metrics_api.md#grafana&quot;&gt;Grafana&lt;/a&gt; for dashboards.&lt;/p&gt;

&lt;p&gt;While serving a model you can query metrics using curl request as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl http://127.0.0.1:8082/metrics
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In case you are looking into exporting the logged metrics, please refer to this &lt;a href=&quot;https://github.com/google/mtail&quot;&gt;example&lt;/a&gt; that uses mtail to export metrics to Prometheus. Tracking these metrics in a dashboard allows you to monitor performance regressions that may have been sporadic or hard to spot during an offline benchmark run.&lt;/p&gt;

&lt;h2 id=&quot;what-to-consider-for-tuning-performance-of-a-model-in-production&quot;&gt;What to consider for tuning performance of a model in production&lt;/h2&gt;

&lt;p&gt;The workflow suggested in Fig 1, is the general idea on how to approach model deployment in production with Torchserve.&lt;/p&gt;

&lt;p&gt;In many cases serving models in production is &lt;strong&gt;optimized&lt;/strong&gt; &lt;strong&gt;based&lt;/strong&gt; on &lt;strong&gt;throughput&lt;/strong&gt; or &lt;strong&gt;latency&lt;/strong&gt; service level agreement (&lt;strong&gt;SLA)s&lt;/strong&gt;. Usually &lt;strong&gt;real-time&lt;/strong&gt; &lt;strong&gt;applications&lt;/strong&gt; are more concerned about &lt;strong&gt;latency&lt;/strong&gt; whereas &lt;strong&gt;off-line applications&lt;/strong&gt; may care more about higher &lt;strong&gt;throughput&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There are a number of main factors contributing to the performance of a serving model in production. In particular, we are focusing on serving Pytorch models with Torchserve here, however most of these factors generalize to all models from other frameworks as well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Model optimizations&lt;/strong&gt;: this is a pre-step for deploying models into production. This is a very broad discussion that we will get into in a series of future blogs. This includes techniques like quantization, pruning to decrease the size of the model, using Intermediate representations (IR graphs) such as Torchscript in Pytorch, fusing kernels and many others. Currently &lt;a href=&quot;https://github.com/msaroufim/torchprep&quot;&gt;torchprep&lt;/a&gt; provides many of these techniques as a CLI tool.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Batch inference:&lt;/strong&gt; it refers to feeding multiple inputs into a model, while it is essential during training, it can be very helpful to manage the cost at inference time as well. Hardware accelerators are optimized for parallelism and batching helps to saturate the compute capacity and often leads to higher throughput. The main difference in inference is you can’t wait too long to get a batch filled from clients, something we call dynamic batching&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Number of Workers :&lt;/strong&gt; Torchserve uses workers to serve models. Torchserve workers are Python processes that hold a copy of the model weights for running inference. Too few workers means you’re not benefitting from enough parallelism but too many can cause worker contention and degrade end to end performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hardware :&lt;/strong&gt; choosing the appropriate hardware based on the model, application and latency, throughput budget. This could be one of the &lt;strong&gt;supported&lt;/strong&gt; hardwares in Torchserve, &lt;strong&gt;CPU, GPU, AWS Inferentia&lt;/strong&gt;. Some hardware configurations are intended for best in class performance and others are better suited for cost effective inference. From our experiments we’ve found that GPUs shine best at larger batch sizes whereas the right CPUs and AWS Inferentia can be far more cost effective for lower batch sizes and low latency.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;best-practices-for-performance-tuning-on-torchserve&quot;&gt;Best Practices for Performance tuning on Torchserve&lt;/h2&gt;

&lt;p&gt;To get the best performance out of your model while serving it with Torchserve, we are sharing some of the best practices here. Torchserve provides a  &lt;a href=&quot;https://github.com/pytorch/serve/tree/c87bfec8916d340de5de5810b14a016049b0e395/benchmarks#benchmarking-with-apache-bench&quot;&gt;benchmark&lt;/a&gt; suite that provides helpful insight to make informed decisions on different choices as detailed below.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Optimize your model&lt;/strong&gt; as the first step, Pytorch model optimization &lt;a href=&quot;https://pytorch.org/tutorials/&quot;&gt;tutorials&lt;/a&gt;. &lt;strong&gt;Model optimization&lt;/strong&gt; choices are also closely &lt;strong&gt;tied&lt;/strong&gt; to the &lt;strong&gt;hardware&lt;/strong&gt; of choice. We will discuss it in more detail in another blog post.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Deciding&lt;/strong&gt; the &lt;strong&gt;hardware&lt;/strong&gt; for model deployment can be closely related to the latency and throughput budget and cost per inference. Depending on the size of model and application it can vary, for some models like computer vision models it has been historically not affordable to run in production on CPU.  However, by having optimizations such &lt;a href=&quot;https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/examples/intel_extension_for_pytorch/README.md&quot;&gt;IPEX&lt;/a&gt; as recently added to Torchserve this has been much more affordable and cost beneficial and you can learn more in this investigative &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html&quot;&gt;case study&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Workers&lt;/strong&gt; in Torchserve are Python processes that provide parallelism, setting the number of workers should be done carefully. By default Torchserve launch number of workers equal to VCPUs or available GPUs on the host, this can add a considerable amount of time to the Torchserve start.&lt;/p&gt;

    &lt;p&gt;Torchserve exposes a &lt;a href=&quot;https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/configuration.md#config-model&quot;&gt;config property&lt;/a&gt; to set the number of workers. To provide an &lt;strong&gt;efficient parallelism&lt;/strong&gt; through &lt;strong&gt;multiple workers&lt;/strong&gt; and avoiding them to compete over resources, as a baseline we &lt;strong&gt;recommend&lt;/strong&gt; following setting on CPU and GPU:&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt; : In the handler,  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.set_num_threads(1) &lt;/code&gt;then set the number of workers to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num physical cores / 2. &lt;/code&gt;But the the best threading configurations can be achieved by leveraging the Intel CPU launcher script.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: number of available GPUs can be set through&lt;a href=&quot;https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/configuration.md#limit-gpu-usage&quot;&gt; number_gpus&lt;/a&gt; in config.properties. Torchserve uses round robin to assign workers to GPUs. We recommend setting the number of workers as follows.  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Number of worker = (Number of available GPUs) / (Number of Unique Models). &lt;/code&gt;Note that GPUs that are pre-Ampere do not provide any resource isolation with Multi Instance GPUs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Batch size&lt;/strong&gt; can directly affect the latency and the throughput. To better utilize the compute resources batch size needs to be increased. However, there is a tradeoff between latency and throughput. &lt;strong&gt;Larger batch sizes&lt;/strong&gt; can &lt;strong&gt;increase&lt;/strong&gt; the &lt;strong&gt;throughput but results in a higher latency&lt;/strong&gt; as well.  Batch size can be set in Torchserve in two ways, either through&lt;a href=&quot;https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/configuration.md#config-model&quot;&gt; model config&lt;/a&gt; in config.properties or while registering the model using &lt;a href=&quot;https://github.com/pytorch/serve/blob/c87bfec8916d340de5de5810b14a016049b0e395/docs/management_api.md#scale-workers&quot;&gt;Management API&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the next section, we are going to use Torchserve benchmark suite to decide the best combination of model optimization,  hardware, workers, and batch size.&lt;/p&gt;

&lt;h2 id=&quot;animated-drawings-performance-tuning&quot;&gt;Animated Drawings Performance Tuning&lt;/h2&gt;

&lt;p&gt;To use the Torchserve benchmark suite, first we need to have an archived file, “.mar” file as discussed above, that contains the model, handler and all other artifacts to load and run inference. Animated Drawings uses Detectron2’s implementation of Mask-RCNN for an object detection model.&lt;/p&gt;

&lt;h3 id=&quot;how-to-run-benchmark-suite&quot;&gt;How to run benchmark suite&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/serve/tree/master/benchmarks#auto-benchmarking-with-apache-bench&quot;&gt;Automated benchmark suite&lt;/a&gt; in Torchserve let you benchmark multiple models with different setting including batch size and number of worker and finally generate a report for you. To get started:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/pytorch/serve.git

cd serve/benchmarks

pip install -r requirements-ab.txt

apt-get install apache2-utils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Model level settings can be configured in a yaml file similar to&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;na&quot;&gt;Model_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;eager_mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;benchmark_engine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;ab&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;.mar&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;file&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;workers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;batch_delay&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10000&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;concurrency&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;input&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;backend_profiling&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;False&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;exec_env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;local&quot;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;processors&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;cpu&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;gpus&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;all&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This yaml file will be referenced in the &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/benchmarks/benchmark_config_template.yaml#L12&quot;&gt;benchmark_config_template&lt;/a&gt;.yaml file that includes other settings for generating reports, this can optionally work with AWS cloud watch for logs as well.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python benchmarks/auto_benchmark.py --input benchmark_config_template.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running the &lt;strong&gt;benchmarks&lt;/strong&gt;, results will be written in “csv” file that can be found in “_ /tmp/benchmark/ab_report.csv_” and full report “/tmp/ts_benchmark/report.md”. It will include items such as Torchserve average latency, model P99 latency, throughput, number of concurrency, number of requests, handler time, and some other metrics. Here we focus on some of the important ones that we track to tune the performance which are, &lt;strong&gt;concurrency&lt;/strong&gt;, &lt;strong&gt;model P99&lt;/strong&gt; latency, &lt;strong&gt;throughput&lt;/strong&gt;. We look at these numbers specifically in &lt;strong&gt;combination&lt;/strong&gt; with &lt;strong&gt;batch size&lt;/strong&gt;, the used &lt;strong&gt;device, number of workers&lt;/strong&gt; and if any &lt;strong&gt;model optimization&lt;/strong&gt; has been done.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;latency SLA&lt;/strong&gt; for this model has been set to &lt;strong&gt;100 ms,&lt;/strong&gt; this is real-time application and as we discussed earlier, latency is more of a concern and &lt;strong&gt;throughput&lt;/strong&gt; ideally should be as high as possible while it does &lt;strong&gt;not violate&lt;/strong&gt; the &lt;strong&gt;latency SLA.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Through searching the space, over different batch sizes (1-32), number of workers (1-16) and devices (CPU,GPU), we have run a set of experiments that summarized the best ones in the table below.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;Device 
   &lt;/td&gt;
   &lt;td&gt;Concurrency 
   &lt;/td&gt;
   &lt;td&gt;# Requests
   &lt;/td&gt;
   &lt;td&gt;#workers
   &lt;/td&gt;
   &lt;td&gt;Batch size
   &lt;/td&gt;
   &lt;td&gt;Payload/image
   &lt;/td&gt;
   &lt;td&gt;Optimization 
   &lt;/td&gt;
   &lt;td&gt;Throughput 
   &lt;/td&gt;
   &lt;td&gt;Latency P99
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU
   &lt;/td&gt;
   &lt;td&gt;10
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;small
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;3.45
   &lt;/td&gt;
   &lt;td&gt;305.3 ms
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;small
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;3.45
   &lt;/td&gt;
   &lt;td&gt;291.8 ms
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;10
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;small
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;41.05
   &lt;/td&gt;
   &lt;td&gt;25.48  ms
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;small
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;42.21
   &lt;/td&gt;
   &lt;td&gt;23.6  ms
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;10
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;small
   &lt;/td&gt;
   &lt;td&gt;N/A
   &lt;/td&gt;
   &lt;td&gt;54.78
   &lt;/td&gt;
   &lt;td&gt;73.62 ms
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;10
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;small
   &lt;/td&gt;
   &lt;td&gt;model.half()
   &lt;/td&gt;
   &lt;td&gt;78.62
   &lt;/td&gt;
   &lt;td&gt;50.69 ms
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;10
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;8
   &lt;/td&gt;
   &lt;td&gt;small
   &lt;/td&gt;
   &lt;td&gt;model.half()
   &lt;/td&gt;
   &lt;td&gt;85.29
   &lt;/td&gt;
   &lt;td&gt;94.4 ms
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The latency of this model on CPU with all of the tried settings in terms of batch size, concurrency and number of workers did not meet the SLA, in fact ~13x higher.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Moving&lt;/strong&gt; the model serving &lt;strong&gt;to GPU&lt;/strong&gt;, immediately could &lt;strong&gt;improve&lt;/strong&gt; the &lt;strong&gt;latency&lt;/strong&gt; ~**13x **from 305 ms down to 23.6 ms.&lt;/p&gt;

&lt;p&gt;One of the &lt;strong&gt;simplest&lt;/strong&gt; &lt;strong&gt;optimizations&lt;/strong&gt; that we could do for the model was lowering its precision to &lt;strong&gt;fp16&lt;/strong&gt;, it is one liner (&lt;strong&gt;model.half()&lt;/strong&gt;)  and could reduce the &lt;strong&gt;model P99 latency **by  **32%&lt;/strong&gt; and increase the throughput by almost the same amount.&lt;/p&gt;

&lt;p&gt;There could be other optimization done by Torchscripting the model and using  &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/jit/_freeze.py#L168&quot;&gt;optimize_for_inference&lt;/a&gt; or other tricks including onnx or tensorrt runtime optimizations which leverage aggressive fusions are out of the scope of this post. We will discuss model optimizations in a separate post.&lt;/p&gt;

&lt;p&gt;We found both on CPU and GPU , setting **number of workers=1 **worked the best in this case.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Moving the model to GPU, using &lt;strong&gt;number of workers = 1&lt;/strong&gt;, and &lt;strong&gt;batch size = 1&lt;/strong&gt; increased the &lt;strong&gt;Throughput ~12x compared&lt;/strong&gt; to &lt;strong&gt;CPU and latency ~13x.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Moving the model to GPU, using &lt;strong&gt;model.half()&lt;/strong&gt;, &lt;strong&gt;number of workers = 1&lt;/strong&gt;, and &lt;strong&gt;batch size = 8&lt;/strong&gt; yielded &lt;strong&gt;best&lt;/strong&gt; results in terms of &lt;strong&gt;Throughput&lt;/strong&gt; and tolerable latency. &lt;strong&gt;Throughput&lt;/strong&gt; increased &lt;strong&gt;~25x compared&lt;/strong&gt; to &lt;strong&gt;CPU with latency still meeting the SLA (94.4ms).&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Note: if you are running the benchmark suite, make sure you are setting a proper &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_delay&lt;/code&gt; and set the concurrency of the request to a number proportional to your batch size. Concurrency here means the number of concurrent requests being sent to the server.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post, we have discussed the considerations and knobs that Torchserve expose to tune the performance in production. We have discussed the Torchserve benchmark suite as a means to tune the performance and get insights on possible choices for model optimizations, hardware choice and cost in general. We used Animated Drawings app which uses Detectron2’s Mask-RCNN model as a case-study to showcase the performance tuning with benchmark suite.&lt;/p&gt;

&lt;p&gt;For more details on Performance tuning in Torchserve please refer to our documentation &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docs/performance_guide.md&quot;&gt;here&lt;/a&gt;.
Also feel free to open a ticket on &lt;a href=&quot;https://github.com/pytorch/serve/issues&quot;&gt;Torchserve repo&lt;/a&gt; for any further questions and feedback.&lt;/p&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h3&gt;

&lt;p&gt;We would like to thank Somya Jain (Meta), Christopher Gustave (Meta) for their great support and guidance throughout many steps of this blog and providing insights to Sketch Animator workflow. Also, special thanks to&lt;a href=&quot;https://www.linkedin.com/in/li-ning-7274604/&quot;&gt; Li Ning&lt;/a&gt; from AWS for the great efforts to make performance tuning much easier on Torchserve with automated benchmark suite.&lt;/p&gt;

&lt;style&gt;

    td{
        border: 1px solid black;
    }
    
    /* article.pytorch-article table tr td:first-of-type{
        padding: 0.3125rem;
    }

    article.pytorch-article table td {
    padding: 0.3125rem;
    } */

   li a:focus, li a:hover, li a:active{
    cursor: pointer;
    text-decoration: underline;
    }

    ol a:hover, ol a:active{
    cursor: pointer;
    text-decoration: underline;
    }

}

&lt;/style&gt;</content>

      
      
      
      
      

      <author>
          <name>Hamid Shojanazeri, Geeta Chauhan, Mark Saroufim, Jesse Smith</name>
        
        
      </author>

      

      

      
        <summary type="html">In this post we discuss performance tuning of Torchserve for serving your models in production. One of the biggest challenges in the life cycle of a ML project is deploying models in production. This requires a reliable serving solution along with solutions that address the MLOps needs. A robust serving solution needs to provide support for multi model serving, model versioning, metric logging, monitoring and scaling to serve the peak traffic. In this post, we will have an overview of Torchserve and how to tune its performance for production use-cases. We discuss the Animated Drawings app from Meta that can turn your human figure sketches to animations and how it could serve the peak traffic with Torchserve. The Animated Drawing’s workflow is below.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling Vision Model Training Platforms with PyTorch</title>
      <link href="https://pytorch.org/blog/scaling-vision-model-training-platforms-with-pytorch/" rel="alternate" type="text/html" title="Scaling Vision Model Training Platforms with PyTorch" />
      <published>2022-12-22T00:00:00-08:00</published>
      <updated>2022-12-22T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/scaling-vision-model-training-platforms-with-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/scaling-vision-model-training-platforms-with-pytorch/">&lt;p&gt;&lt;em&gt;TL;DR: We demonstrate the use of PyTorch with FairScale’s FullyShardedDataParallel (FSDP) API in writing large vision transformer models. We discuss our techniques for scaling and optimizing these models on a GPU cluster. The goal of this platform scaling effort is to enable research at scale. This blog does not discuss model accuracy, new model architectures, or new training recipes.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Latest vision research [1, 2] demonstrates model scaling as a promising research direction. In this project, we aim to enable our platforms to train massive vision transformer (ViT) [3] models. We present our work on scaling the largest trainable ViT from 1B to 120B parameters in FAIR vision platforms. We wrote ViT in PyTorch and leveraged its support for large-scale, distributed training on a GPU cluster.&lt;/p&gt;

&lt;p&gt;In the rest of this blog, we will first discuss the main challenges, namely &lt;em&gt;scalability&lt;/em&gt;, &lt;em&gt;optimization&lt;/em&gt;, and &lt;em&gt;numerical stability&lt;/em&gt;. Then we will discuss how we tackle them with techniques including &lt;em&gt;data and model parallelism&lt;/em&gt;, &lt;em&gt;automatic mixed precision&lt;/em&gt;, &lt;em&gt;kernel fusion&lt;/em&gt;, and &lt;em&gt;bfloat16&lt;/em&gt;. Finally, we present our results and conclude.&lt;/p&gt;

&lt;h2 id=&quot;2-main-challenges&quot;&gt;2. Main Challenges&lt;/h2&gt;

&lt;h3 id=&quot;21-scalability&quot;&gt;2.1 Scalability&lt;/h3&gt;

&lt;p&gt;The key scalability challenge is to efficiently shard a model’s operations and state across multiple GPUs. A 100B parameter model requires ~200GB of RAM just for parameters, assuming fp16 representation. So, it is impossible to fit the model on a single GPU (A100 has at most 80GB RAM). Therefore, we need some way to efficiently shard a model’s data (input, parameters, activations, and optimizer state) across multiple GPUs.&lt;/p&gt;

&lt;p&gt;Another aspect of this problem is to scale without significantly changing the training recipe. E.g. Certain representation learning recipes use a global batch size of up to 4096 beyond which we start to see accuracy degradation. We cannot scale to more than 4096 GPUs without using some form of tensor or pipeline parallelism.&lt;/p&gt;

&lt;h3 id=&quot;22-optimization&quot;&gt;2.2 Optimization&lt;/h3&gt;

&lt;p&gt;The key optimization challenge is to maintain high GPU utilization even as we scale the number of model parameters and flops. When we scale models to teraflops and beyond, we start to hit major bottlenecks in our software stack that super-linearly increase training time and reduce accelerator utilization. We require hundreds or thousands of GPUs to run just a single experiment. Improvements in accelerator utilization can lead to significant reductions in cost and improve fleet utilization. It enables us to fund more projects and run more experiments in parallel.&lt;/p&gt;

&lt;h3 id=&quot;23-numerical-stability&quot;&gt;2.3 Numerical Stability&lt;/h3&gt;

&lt;p&gt;The key stability challenge is to avoid numerical instability and divergence at large scale. We empirically observed in our experiments that the training instability gets severe and hard to deal with when we scale up model sizes, data, batch sizes, learning rate, etc. Vision Transformers particularly face training instability even at a lower parameter threshold. E.g., we find it challenging to train even ViT-H (with just 630M parameters) in mixed-precision mode without using strong data augmentation. We need to study the model properties and training recipes to make sure that the models train stably and converge.&lt;/p&gt;

&lt;h2 id=&quot;3-our-solutions&quot;&gt;3. Our Solutions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt; depicts our solutions to each of the challenges.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_1-solutions-to-the-challenges.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;31-addressing-scaling-challenges-with-data-parallelism-and-model-parallelism&quot;&gt;3.1 Addressing scaling challenges with data parallelism and model parallelism&lt;/h3&gt;

&lt;p&gt;We apply various forms of data and model parallelism to enable fitting very large models in GPU memory.&lt;/p&gt;

&lt;p&gt;We use FairScale’s &lt;em&gt;FullyShardedDataParallel (FSDP)&lt;/em&gt; API [4], based on PyTorch, to shard parameters, gradients, and optimizer state across multiple GPUs, thereby reducing the memory footprint per GPU. This process consists of the following three steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Step 1: We wrapped the entire model in a single FSDP instance. This shards the model parameters at the end of a forward pass and gathers parameters at the beginning of a forward pass. This enabled us to scale ~3x from 1.5B to 4.5B parameters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 2: We experimented with wrapping individual model layers in separate FSDP instances. This nested wrapping further reduced the memory footprint by sharding and gathering parameters of individual model layers instead of an entire model. The peak memory is then determined by an individually wrapped transformer block in GPU memory in this mode instead of the entire model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Step 3: We used &lt;em&gt;activation-checkpoint&lt;/em&gt; to reduce the memory consumption by activations. It saves the input tensors and discards the intermediate activation tensors during the forward pass. These are recomputed during the backward pass.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, we experimented with model-parallelism techniques such as pipeline parallelism [5], which allow us to scale to more GPUs without increasing the batch size.&lt;/p&gt;

&lt;h3 id=&quot;32-addressing-optimization-challenges-with-advanced-amp-and-kernel-fusion&quot;&gt;3.2 Addressing optimization challenges with advanced AMP and kernel fusion&lt;/h3&gt;

&lt;h4 id=&quot;advanced-amp&quot;&gt;Advanced AMP&lt;/h4&gt;

&lt;p&gt;Automatic Mixed Precision (AMP) [6] training refers to training models using a lower precision of bits than FP32 or the default but still maintaining accuracy. We experimented with three levels of AMP as described below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AMP O1: This refers to training in mixed precision where weights are in FP32 and some operations are in FP16. With AMP O1, the ops that might impact accuracy remain in FP32 and are not autocasted to FP16.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AMP O2: This refers to training in mixed precision but with more weights and ops in FP16 than in O1. Weights do not implicitly remain in FP32 and are cast to FP16. A copy of the master weights is maintained in the FP32 precision that is used by the optimizer. If we want the normalization layer weights in FP32 then we need to explicitly use layer wrapping to ensure that.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Full FP16: This refers to training in full FP16 where weights and operations are in FP16. FP16  is challenging to enable for training due to convergence issues.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We found that AMP O2 with LayerNorm wrapping in FP32 leads to the best performance without sacrificing accuracy.&lt;/p&gt;

&lt;h4 id=&quot;kernel-fusion&quot;&gt;Kernel Fusion&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;To reduce GPU kernel launch overhead and increase GPU work granularity, we experimented with kernel fusions, including fused dropout and fused layer-norm, using the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xformers library&lt;/a&gt; [7].&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;33-addressing-stability-challenges-by-studying-ops-numerical-stability-and-training-recipes&quot;&gt;3.3 Addressing stability challenges by studying ops numerical stability and training recipes&lt;/h3&gt;

&lt;h4 id=&quot;bfloat16-in-general-but-with-layernorm-in-fp32&quot;&gt;BFloat16 in general but with LayerNorm in FP32&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;https://cloud.google.com/tpu/docs/bfloat16&quot;&gt;bfloat16&lt;/a&gt; (BF16) [8] floating-point format provides the same dynamic range as FP32 with a memory footprint identical to FP16. We found that we could train models in the BF16 format using the same set of hyperparameters as in FP32, without special parameter tuning. Nevertheless, we found that we need to keep LayerNorm in FP32 mode in order for the training to converge.&lt;/p&gt;

&lt;h3 id=&quot;34-final-training-recipe&quot;&gt;3.4 Final training recipe&lt;/h3&gt;

&lt;p&gt;A summary of the final training recipe.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Wrap the outer model in an FSDP instance. Enable parameter sharding after the forward pass.&lt;/li&gt;
  &lt;li&gt;Wrap individual ViT blocks with activation checkpointing, nested FSDP wrapping, and parameter flattening.&lt;/li&gt;
  &lt;li&gt;Enable mixed precision mode (AMP O2) with bfloat16 representation. Maintain the optimizer state in FP32 precision to enhance numerical stability.&lt;/li&gt;
  &lt;li&gt;Wrap normalization layers like LayerNorm in FP32 for better numerical stability.&lt;/li&gt;
  &lt;li&gt;Maximize the Nvidia TensorCore utilization by keeping matrix dimensions to be multiple of 8. For More details check &lt;a href=&quot;https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf&quot;&gt;Nvidia Tensor Core Performance Guide&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;4-results&quot;&gt;4. Results&lt;/h2&gt;

&lt;p&gt;In this section, we show the scaling results of ViT on three types of tasks: (1) image classification, (2) object detection (3) video understanding. &lt;strong&gt;Our key result is that we are able to train massive ViT backbones across these vision tasks after applying the discussed scaling and optimization techniques. This enables vision research at a much larger scale.&lt;/strong&gt; We trained the models to convergence to verify that we maintain the current baselines even with all the optimizations. A common trend in Figures 2, 3, 4 is that we are able to train up to 25B-param models with an epoch time of less than 4 hours on 128 A100 GPUs. The 60B and 120B models are relatively slower to train.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt; shows the &lt;em&gt;image-classification&lt;/em&gt; scaling result. It plots the epoch time for training ViTs on ImageNet using 128 A100-80GB GPUs with different model sizes.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_2-image-classification-scaling-result.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 2: Image-classification scaling result.&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt; shows the &lt;em&gt;object-detection&lt;/em&gt; scaling result. It plots the epoch time for training &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;&gt;ViTDet&lt;/a&gt; [9] with different ViT backbones on COCO using 128 A100-80GB GPUs.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_3-object-detection-scaling-result.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 3: Object-detection scaling result.&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt; shows the &lt;em&gt;video-understanding&lt;/em&gt; scaling result. It plots the epoch time for training &lt;a href=&quot;https://arxiv.org/abs/2112.01526&quot;&gt;MViTv2&lt;/a&gt; [10] models on &lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;&gt;Kinetics 400&lt;/a&gt; [11] using 128 V100 (32 GB) GPUs in FP32.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_4-video-understanding-scaling-result.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 4: Video-understanding scaling result.&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt; shows the optimization result with the ViT-H model in Figure 2 on 8 A100-40GB GPUs.
Three versions are used: (1) the baseline uses PyTorch’s DDP [12] with AMP O1, (2) FSDP + AMP-O2 + other optimizations, and (3) FSDP + FP16 + other optimizations. These optimizations altogether speed up the training by up to 2.2x.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/scaling-vision-figure_5-training-speedups-from-various-optimizations.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 5: Training speedups from various optimizations.&lt;/b&gt;
&lt;/p&gt;

&lt;h2 id=&quot;5-concluding-remarks&quot;&gt;5. Concluding Remarks&lt;/h2&gt;

&lt;p&gt;We have demonstrated the use of PyTorch with FairScale’s FullyShardedDataParallel (FSDP) API in writing large vision transformer models. We discuss our techniques for scaling and optimizing these models on a GPU cluster.  We hope that this article can motivate others to develop large-scale ML models with PyTorch and its ecosystem.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/abs/2111.06377&quot;&gt;Masked Autoencoders Are Scalable Vision Learners&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://arxiv.org/abs/2201.08371&quot;&gt;Revisiting Weakly Supervised Pre-Training of Visual Perception Models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://arxiv.org/abs/2010.11929v2&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;https://fairscale.readthedocs.io/en/stable/api/nn/fsdp.html&quot;&gt;fairscale.nn.FullyShardedDataParallel&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://pytorch.org/docs/stable/pipeline.html&quot;&gt;Pipeline parallelism in PyTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html#module-torch.amp&quot;&gt;Automatic Mixed Precision (AMP) in PyTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xformers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&quot;https://cloud.google.com/tpu/docs/bfloat16&quot;&gt;The bfloat16 numerical format&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&quot;https://arxiv.org/abs/2203.16527&quot;&gt;Exploring Plain Vision Transformer Backbones for Object Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] &lt;a href=&quot;https://arxiv.org/abs/2112.01526&quot;&gt;MViTv2: Improved Multiscale Vision Transformers for Classification and Detection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[11] &lt;a href=&quot;https://www.deepmind.com/open-source/kinetics&quot;&gt;https://www.deepmind.com/open-source/kinetics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[12] &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&quot;&gt;Getting Started with Distributed Data Parallel (DDP)&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Vaibhav Aggarwal, Mannat Singh, Anjali Sridhar, Yanghao Li, Shoubhik Debnath, Ronghang Hu, Will Feng, Xinlei Chen, Tingting Markstrum, Diana Liskovich, Anupam Bhatnagar, Chay Ryali, Haoqi Fan, Tete Xiao, Min Xu, Rahul Iyer, Christoph Feichtenhofer, Ross Girshick, Piotr Dollar, Aaron Adcock, Wan-Yen Lo, CK Luk</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR: We demonstrate the use of PyTorch with FairScale’s FullyShardedDataParallel (FSDP) API in writing large vision transformer models. We discuss our techniques for scaling and optimizing these models on a GPU cluster. The goal of this platform scaling effort is to enable research at scale. This blog does not discuss model accuracy, new model architectures, or new training recipes.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Efficient Large-Scale Training with Pytorch FSDP and AWS</title>
      <link href="https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/" rel="alternate" type="text/html" title="Efficient Large-Scale Training with Pytorch FSDP and AWS" />
      <published>2022-12-16T00:00:00-08:00</published>
      <updated>2022-12-16T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/efficient-large-scale-training-with-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/">&lt;p&gt;Cutting-edge AI models are becoming extremely large. The cost and overhead of training these models is increasing rapidly, and involves large amounts of engineering and guesswork to find the right training regime. FSDP reduces these costs significantly by enabling you to train much larger models with the same amount of resources. FSDP lowers the memory footprint on your GPUs, and is usable via a lightweight configuration that requires substantially less effort, typically with just a few lines of code.&lt;/p&gt;

&lt;p&gt;The main performance gains in FSDP come from maximizing the overlap between network communication and model computation, and eliminating the memory redundancy inherent in traditional data parallel training (DDP).  PyTorch FSDP can train models approximately 4x larger on the same server resources as DDP and 20x larger if we combine activation checkpointing and activation offloading.&lt;/p&gt;

&lt;p&gt;Since PyTorch 1.12, FSDP is now in beta status, and has added a number of new features that can be tuned to further accelerate your model training.&lt;/p&gt;

&lt;p&gt;In this series of blog posts, we will explain multiple performance optimizations you can run with FSDP to boost your distributed training speed and model sizes within the context of your available server resources.  We use the HuggingFace T5 3B, 11B and DeepVit, in fine-tuning mode, as the running examples throughout the series.&lt;/p&gt;

&lt;p&gt;As a preview of some of the optimizations discussed in this series, we show the before and after performance scaled in Flops below (Note that these results can vary based on your server resources and model architecture).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_1.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;i&gt; *T5 3B Performance measured on AWS A100 and A10 servers. Original with no optimizations and Tuned with the applied optimization &lt;/i&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_2.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;i&gt; *T5 11B Performance measured on A100 servers. Original with no optimizations and Tuned with the applied optimization &lt;/i&gt;&lt;/p&gt;

&lt;p&gt;In this first post, we will provide a quick overview of FSDP and how it can make training large- scale AI models more efficient.  We will highlight briefly the multiple performance options available, and dive deeper into the details on these in upcoming posts.  We will then conclude with an overview on how to leverage AWS parallel cluster for large- scale training with FSDP.&lt;/p&gt;

&lt;table style=&quot;border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Optimization &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;T5 Model &lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Throughput Improvement &lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Mixed Precision
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;5x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;10x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Activation Checkpointing (AC)
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;10x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;100x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Transformer Wrapping Policy
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;2x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;Unable to run the experiment without the Transformer wrapping policy.&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;Full Shard Strategy
   &lt;/td&gt;
   &lt;td&gt;3 B
   &lt;/td&gt;
   &lt;td&gt;1.5x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;11 B
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;Not able to run with Zero2&lt;/em&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Performance optimization gains on T5 models over non-optimized.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In our experiments with the T5 3B model, using the  &lt;a href=&quot;https://www.youtube.com/watch?v=HQeKwCsnH4k&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=2&quot;&gt;transformer wrapping policy&lt;/a&gt; resulted in &amp;gt;2x higher throughput measured in TFLOPS versus the default wrapping policy. &lt;a href=&quot;https://www.youtube.com/watch?v=5B4d0FuxSQc&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=3&quot;&gt;Activation checkpointing&lt;/a&gt; resulted in 10x improvement by reinvesting the freed memory from the checkpoints into larger batch size. &lt;a href=&quot;https://www.youtube.com/watch?v=-caN92JtKqA&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=4&quot;&gt;Mixed precision&lt;/a&gt; with BFloat16 resulted in ~5x improvement versus FP32 and finally the &lt;a href=&quot;https://www.youtube.com/watch?v=a3iW6Cggccw&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=5&quot;&gt;full sharding strategy&lt;/a&gt; versus zero2 (DDP)  resulted in 1.5x improvement.&lt;/p&gt;

&lt;p&gt;We ran similar experiments for a larger model, T5 11B, but the larger model size resulted in some changes to the experiment space.  Specifically, we found that two optimizations,  transformer wrapping policy and activation checkpointing, were needed to enable us to run these experiments on 3 nodes (each node had 8 A100 gpus with 80 GB of memory). With these optimizations, we could fit a batch size of 50 and get higher throughput compared to removing each one of them. Thus rather than running on/off solely for a single optimization test as with the 3B model, the larger model experiments were done with 1 of 3 optimizations turned on/off while always running the other two in order to allow a usable batch size for both test states for each item.&lt;/p&gt;

&lt;p&gt;Based on TFLOP comparisons, with the 11B model, we saw even more payoff from the optimizations.  Mixed precision(~10x improvement) and activation checkpointing (~100x improvement) had a much larger impact with the 11B model compared to the 3B parameter model. With mixed precision we could fit ~2x larger batch sizes and with activation checkpointing &amp;gt;15x batch sizes (from 3 with no activation checkpointing to 50 with activation checkpointing) which translated into large throughput improvements.&lt;/p&gt;

&lt;p&gt;We also have observed that for these larger models &amp;gt; 3B, using Zero2 sharding strategy would result in minimal room left in memory for the batch data, and had to go with very small batch sizes (e.g 1-2) that essentially makes full sharding strategy a necessity to enable fitting larger batches sizes.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note - this tutorial assumes a basic understanding of FSDP. To learn more about basics of FSDP please refer to the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;getting started&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html&quot;&gt;advanced FSDP &lt;/a&gt;tutorials.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is FSDP? How does it make Large-Scale Training More Efficient&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FSDP&lt;/strong&gt; expands upon distributed data parallel, by parallelizing not just data, but the model parameters, the optimizer states and gradients associated with the model. Specifically - &lt;strong&gt;each&lt;/strong&gt; &lt;strong&gt;GPU only stores a subset of the entire model&lt;/strong&gt; &lt;strong&gt;and the associated subset of optimizer states and gradients.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;To show the evolution of distributed training, we can start from the beginning, where AI models were simply trained on a single GPU.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;DDP (Distributed Data Parallel) was the initial step up from training with only a single GPU, and was an effort to address the data and model size growth, where multiple GPUs each housed their own copy of the same model. The gain here is that the data for each batch could be split and processed independently on each GPU, all at the same time,thus parallelizing the processing of the data set and increasing training speed by the increasing number of GPUs. The tradeoff is the need to communicate the gradients between each GPU to synchronize the models after the backward pass.&lt;/p&gt;

&lt;p&gt;FSDP expands on scaling models by removing the redundancy of optimizer calculations and state storage, as well as gradient and memory storage of model parameters that are present in DDP (DDP = Distributed Data Parallel). This redundancy reduction, along with increased communication overlap where model parameter communication takes place at the same time as model computation, is what allows FSDP to train much larger models with the same resources as DDP.&lt;/p&gt;

&lt;p&gt;A key point is that this efficiency also allows for AI models that are larger than a single GPU to be trained. The model size available for training is now increased to the aggregate memory of all GPUs, rather than the size of a single GPU. (And as a point of note, FSDP can go beyond aggregated GPU memory by leveraging CPU memory as well, though we will not directly cover this aspect here).&lt;/p&gt;

&lt;p&gt;As discussed in a previous &lt;a href=&quot;https://medium.com/pytorch/pytorch-data-parallel-best-practices-on-google-cloud-6c8da2be180d&quot;&gt;blog post&lt;/a&gt;, with DDP the largest model that we could train on 32, A100 gpus with 40 GB memory (4 nodes) was up to 3B parameters, and batch size of 128, with the help of activation checkpointing. By contrast, using FSDP we were able to train up to 81B model size, combining activation checkpointing, along with activation and parameter offloading. In another &lt;a href=&quot;https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff&quot;&gt;experiment&lt;/a&gt;, we benchmarked a 1T parameter model with FSDP using 512 gpus.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_3.png&quot; width=&quot;90%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;For intuition on the parameter level workings of FSDP, below we show an animation detailing how the model parameters are sharded and communicated assuming a two GPU scenario and a simple 8 parameter model:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_5.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Above - the animations walk through the steps involved with the initial sharding of the model amongst ranks, and we start the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gathers&lt;/code&gt; and forward pass&lt;/em&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_6.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We continue through the model with the forward pass. After each FSDP unit completes, non-locally owned params are dropped to free memory, and optionally activations can be checkpointed. This continues until we finish the forward pass and compute the loss.&lt;/em&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_6.5.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;During the backward pass, another &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; is used to load the parameters and the gradients are computed. These gradients are then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce_scattered&lt;/code&gt; so that the local owners of each param can aggregate and prepare to update the weights.&lt;/em&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_7.gif&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Finally, each rank passes the summed gradients through the optimizer states and updates the weights to complete the mini-batch.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With the model now distributed across the entire set of available GPUs, the logical question is how data moves through the model given this sharding of model parameters.&lt;/p&gt;

&lt;p&gt;This is accomplished by FSDP coordinating with all GPUs to effectively share (communicate) the respective parts of the model.  The model is decomposed into FSDP units and parameters within each unit are flattened and then sharded across all GPUs.  Within each FSDP unit, GPU’s are assigned interleaving ownership of individual model parameters.&lt;/p&gt;

&lt;p&gt;By interleaving, we mean the following - assuming 2 gpus with an id of 1 and 2, the FSDP unit ownership pattern would be [12121212],  rather than a contiguous chunk of [111222].&lt;/p&gt;

&lt;p&gt;During training, an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; is initiated and the locally owned model parameters within a FSDP unit are shared by the owner GPU with the other non-owners, when they need it, on a ‘just in time’ type basis. FSDP prefetches parameters to overlap &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_gather&lt;/code&gt; communication with computation.&lt;/p&gt;

&lt;p&gt;When those requested parameters arrive, the GPU uses the delivered parameters, in combination with the parameters it already owns, to create a fully populated FSDP unit. Thus there is a moment where each GPU hits peak memory usage while holding a fully populated FSDP unit.&lt;/p&gt;

&lt;p&gt;It then processes the data through the FSDP unit, and drops the parameters it received from other GPU’s to free up memory for the next unit…the process continues over and over proceeding through the entire model to complete the forward pass.The process is then repeated (in general) for the backward pass.(note - this is a simplified version for understanding..there is additional complexity but this should help construct a basic mental model of the FSDP process).&lt;/p&gt;

&lt;p&gt;This eliminates much of the memory redundancy present in DDP, but imposes the cost of higher amounts of network communication to shuttle these requested parameters back and forth amongst all the GPUs.&lt;strong&gt;Overlapping the communication timing with the computation taking place is the basis of many of the performance improvements we’ll discuss in this series.&lt;/strong&gt; The key gains are frequently based on the fact that communication can often take place at the same time as computation.As you can surmise, &lt;strong&gt;having high communication speed is vital for FSDP performance.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-do-i-optimize-my-training-with-fsdp&quot;&gt;&lt;strong&gt;How do I optimize my training with FSDP?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;There are four main performance improvements we will cover - the transformer wrapper, activation checkpointing, mixed precision, and selecting the proper sharding strategy. The flowchart below will help as a checklist for tuning options that we will discuss in this post.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_8.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Wrapping policy - &lt;em&gt;for transformers, use Transformer wrapping policy&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The first performance optimization is leveraging the FSDP transformer wrapper for transformer models.&lt;/p&gt;

&lt;p&gt;One of the pre-defined wrapping policy is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_autowrap_policy&lt;/code&gt;. With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_autowrap_policy&lt;/code&gt;, FSDP will traverse the module structure from bottom to top, a new FSDP unit will be created once the current unit has at least the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_num_params&lt;/code&gt; specified within the size policy (this defaults to 1e8, or 100M). If the module can not be created as an FSDP unit, FSDP will continue to check its parent module. This size based wrapping policy may not be ideal for some model structures, PyTorch distributed team is actively working on a new default wrapping policy in the next release which is based on size and also module execution order, users can simply tune the size and achieve the optimized performance.&lt;/p&gt;

&lt;p&gt;In the current release, you can greatly improve your performance when running Transformer models by using the ‘transformer wrapper’. You will need to provide the appropriate layer class for your model. Here, layer class is the class that houses the Multi-Head Attention and Feed Forward Network.&lt;/p&gt;

&lt;p&gt;FSDP will then form the FSDP units around the layer class rather than arbitrary breaks based on parameter size. By sharding the model around layer classes that are uniformly repeated within the transformer, FSDP can create uniform FSDP units that better balance the overlap of computation and communication. By contrast, size based wrapping can produce very uneven or skewed shards for models, which then have uneven matching of compute vs communication overlap. As discussed earlier, the main driver of FSDP high performance is the overlap of communication and computation, and hence why the Transformer wrapper provides improved performance. Note that the Transformer wrapper can also be used for non-transformer models if these models have a list of uniform layers.&lt;/p&gt;

&lt;p&gt;Let’s compare the performance difference on a T5, 3B parameter model when running under the default wrapper and the transformer wrapper.&lt;/p&gt;

&lt;p&gt;For default wrapping, we don’t need to take any action - we simply pass the model to FSDP as shown:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;device_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case FSDP will simply wrap the whole model in a single FSDP unit.&lt;/p&gt;

&lt;p&gt;Running on an &lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf&quot;&gt;NVIDIA A100-SXM4–40GB&lt;/a&gt; with 8 GPUs, we are able to reach 2.3 TFlops and 95% GPU memory utilization with a batch size of 14.&lt;/p&gt;

&lt;p&gt;However, since T5 is a transformer model, we are better served to leverage the transformer wrapper for this model.&lt;/p&gt;

&lt;p&gt;To use that, we need to isolate the layer class for the transformer, and then pass it in to create our transformer wrapper.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers.models.t5.modeling_t5&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T5Block&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And now we can create our Transformer wrapper:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;transformer_auto_wrapper_policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;transformer_auto_wrap_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;transformer_layer_cls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;T5Block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# &amp;lt; ---- Your Transformer layer class
&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With our model aware wrapper ready, we can initialize FSDP:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# invoke FSDP with your transformer wrapper policy:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;auto_wrap_policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_auto_wrapper_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;device_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# streaming init
&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running this wrapped model, we can see some substantial performance gains.We can fit nearly double the batch size, going to 28, and with better memory and communication efficiency, we see a TFlops increase to 5.07 from 2.3.&lt;/p&gt;

&lt;p&gt;Thus, we’ve increased our training throughput by over 200% (2.19x) due to providing greater model info to FSDP! The transformer wrapping policy results in more fine-grained and balanced FSDP units each holding a layer class, which leads to a more effective communication-computation overlap.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_9.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Above: Graphical comparison of TFlops based on wrapper type&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you are training a Transformer model, it pays to configure your training with FSDP using the transformer wrapper. For more information on how to isolate your layer class, please see our in depth video on Transformer wrapping &lt;a href=&quot;https://www.youtube.com/watch?v=HQeKwCsnH4k&quot;&gt;here&lt;/a&gt;, where we walk through a number of transformers showing where the layer class can be found.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mixed precision - &lt;em&gt;use BF16 if you have an Ampere architecture GPU&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;FSDP supports a flexible mixed precision policy that gives you granular control over parameters, gradients and buffer data types. This lets you easily leverage BFloat16 or FP16 to increase your training speed by up to 70%.&lt;/p&gt;

&lt;p&gt;*Note that BFloat 16 is only available on Ampere type GPUs. On AWS this is available with p4dn and g5 instances.&lt;/p&gt;

&lt;p&gt;By way of comparison, we can show a 77% speed improvement when comparing fully tuned BFloat16 vs FP32 on an 8B DeepVit model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_10.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We have obtained even greater acceleration using BFloat16 in fine-tuning a 3B HuggingFace T5 model as shown in the figures below. We observed that because of the lower precision the validation loss of BFloat16 is slightly behind in the first few epochs, but it is able to catch up and results in the same final accuracy as FP32.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_10a.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;To use mixed precision, we create a policy with our desired data types, and pass it in during the FSDP initialization.&lt;/p&gt;

&lt;p&gt;To create our policy, we need to import the MixedPrecision class, and then define our custom policy using our customized class:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed.fsdp&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixedPrecision&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bfSixteen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixedPrecision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;param_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;# Gradient communication precision.
&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;reduce_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;# Buffer precision.
&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;buffer_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;auto_wrap_policy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer_auto_wrapper_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;mixed_precision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloatPolicy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can mix and match the precision for parameters, gradients and buffers as you prefer:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;comboPolicy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MixedPrecision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Param precision
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;param_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Gradient communication precision.
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;reduce_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Buffer precision.
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;buffer_dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For training with FP16, you will need to also use the ShardedGradScaler, which we will cover in subsequent posts. For BFloat16, it is a drop-in replacement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AnyPrecision Optimizer - &lt;em&gt;going beyond mixed precision with full BF16 training&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Mixed precision training, both in FSDP and elsewhere, maintains the working weights in the reduced datatype (BF16 or FP16) while keeping the master weights in full FP32. The reason for the master weights in FP32 is that running in pure BF16 will result in ‘weight stagnation’, where very small weight updates are lost due to the lower precision, and the accuracy flatlines over time while FP32 weights can continue to improve from these small updates.&lt;/p&gt;

&lt;p&gt;In order to resolve this dilemma, we can use the new AnyPrecision optimizer available in &lt;a href=&quot;https://github.com/pytorch/torchdistx&quot;&gt;TorchDistX&lt;/a&gt; (Torch Distributed Experimental) that allows you to successfully train and keep the master weights in pure BF16 instead of FP32. In addition, unlike the typical storage of optimizer states in FP32, AnyPrecision is able to maintain states in pure BF16 as well.&lt;/p&gt;

&lt;p&gt;AnyPrecision enables pure BF16 training by maintaining an extra buffer that tracks the precision lost during the weight updates and re-applies that during the next update…effectively resolving the weight stagnation issue without requiring FP32.&lt;/p&gt;

&lt;p&gt;As a comparison of the throughput gains available with pure BF16 training using AnyPrecision, we ran experiments using FSDP with the T5 11B model with regular FP32 training, Mixed Precision training with BF16, and pure BF16 training using the AnyPrecision optimizer on 3 nodes with A100 gpus as mentioned previously.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_11.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;As shown above, training with AnyPrecision and pure BF16 resulted in 2x the throughput vs Mixed Precision, and over 20x improvement vs FP32.&lt;/p&gt;

&lt;p&gt;The potential tradeoff is the impact on final accuracy - in the cases we tested, the accuracy was equal or better than FP32 due to a regularization effect from the slightly reduced precision, but your results may vary.&lt;/p&gt;

&lt;p&gt;AnyPrecision optimizer is available for you to test with &lt;a href=&quot;https://github.com/pytorch/torchdistx&quot;&gt;here&lt;/a&gt;, and is a drop in replacement for AdamW optimizer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Activation checkpointing - &lt;em&gt;increasing throughput by trading compute for memory&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_12.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FSDP supports activation checkpointing once the model has been sharded&lt;/strong&gt;, and makes it easy to implement. The graph above shows ~4x throughput improvement using activation checkpointing.&lt;/p&gt;

&lt;p&gt;Activation checkpointing is where the intermediate activations are freed during the forward pass, and a checkpoint is left as a placeholder. This generally increases available GPU memory by over 30%.&lt;/p&gt;

&lt;p&gt;The tradeoff is that during the backward pass, these previously removed intermediate activations must be re-calculated again using information in the checkpoint (duplicate compute), but by leveraging the increased GPU memory, one can increase the batch size such that the net throughput can increase substantially.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# verify we have FSDP activation support ready by importing:
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.distributed.algorithms._checkpoint.checkpoint_wrapper&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;checkpoint_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;CheckpointImpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;apply_activation_checkpointing_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The steps required to implement activation checkpointing is to first import the FSDP checkpointing functions. We need declare our checkpointer wrapper type which is non-reentrant and create a check function to identify which layer to wrap as follows&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;non_reentrant_wrapper&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checkpoint_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;offload_to_cpu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checkpoint_impl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CheckpointImpl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NO_REENTRANT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;check_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;submodule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submodule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T5Block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;apply_activation_checkpointing_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;checkpoint_wrapper_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;non_reentrant_wrapper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;check_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;check_fn&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Important note - this must be run after the model has been initialized with FSDP.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, hopefully you’ve seen how some initial tuning with FSDP options can have a large impact on your training performance.&lt;/p&gt;

&lt;p&gt;With that, we turn our attention from how to scale within FSDP, to how to scale your server hardware for FSDP using AWS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Large Scale Training with FSDP on AWS - &lt;em&gt;For multi-node prioritize high speed network&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AWS provides several services that can be used to run distributed training with FSDP: &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing&quot;&gt;Amazon EC2 Accelerated Computing instances&lt;/a&gt;, AWS &lt;a href=&quot;https://aws.amazon.com/hpc/parallelcluster/&quot;&gt;ParallelCluster&lt;/a&gt;, and Amazon &lt;a href=&quot;https://aws.amazon.com/sagemaker/features/?nc=sn&amp;amp;loc=2&quot;&gt;Sagemaker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this series of blog posts, we used &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/p4/&quot;&gt;Amazon EC2 p4d&lt;/a&gt; instances in a single-instance multi-GPU configuration and in a multi-instance configuration using AWS &lt;a href=&quot;https://aws.amazon.com/hpc/parallelcluster/&quot;&gt;ParallelCluster&lt;/a&gt; and SageMaker in order to run our training jobs.&lt;/p&gt;

&lt;p&gt;Here, we’ll focus specifically on AWS parallel cluster and provide an overview of how to utilize it for training purposes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AWS ParallelCluster Setup&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AWS ParallelCluster is an open source, cluster management tool that makes it easy for you to deploy and manage High Performance Computing (HPC) clusters on AWS.  AWS ParallelCluster uses yaml configuration files to provision all the necessary resources. It also supports multiple instance types, job submission queues, shared file systems like &lt;a href=&quot;https://aws.amazon.com/efs/?trk=3c5ce89c-8865-47a3-bec3-f6820351aa6d&amp;amp;sc_channel=ps&amp;amp;sc_campaign=acquisition&amp;amp;sc_medium=ACQ-P|PS-GO|Non-Brand|Desktop|SU|Storage|Solution|US|EN|DSA&amp;amp;ef_id=Cj0KCQjwuaiXBhCCARIsAKZLt3l6dtldpE152xuxTMa3mbUbaqtTXwsBdfDRIzCL8cw3NO5DO_y1vOgaAj1pEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!579408162404!!!g!!&quot;&gt;Amazon EFS&lt;/a&gt; (NFS) or &lt;a href=&quot;https://aws.amazon.com/fsx/lustre/?refid=3c5ce89c-8865-47a3-bec3-f6820351aa6d&quot; target=&quot;_blank&quot;&gt;Amazon FSx for Lustre&lt;/a&gt;, and job schedulers like AWS Batch and Slurm.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;/assets/images/largeblog_index_13.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Workflow on Clusters&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The high level idea is to have a cluster that has a head node which controls the compute nodes. The actual training job runs on the compute nodes. Overall steps to run a training job on a cluster are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set up an AWS ParallelCuster (we discuss below)&lt;/li&gt;
  &lt;li&gt;Connect to the head node, and import the training code/ setup the environment.&lt;/li&gt;
  &lt;li&gt;Pull the data and place it in a shared folder that compute nodes can access (FSx Lustre drive).&lt;/li&gt;
  &lt;li&gt;Run the training job using a job scheduler (in this case Slurm).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Setup AWS ParallelCuster&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To setup AWS ParallelCluster,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Deploy a network stack.&lt;/strong&gt; This step is optional since you could use your account default VPC and let AWS ParallelCluster create your subnets and security groups. However, we prefer to compartmentalize our desired network infrastructure and do this deployment via a CloudFormation stack.&lt;/p&gt;

    &lt;p&gt;Since we deploy a public and a private subnet, we want to create them into an Availability Zone that contains our target instances, in this case p4d. We consult their availability in the region we use (us-east-1) through the following AWS CLI command:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws ec2 describe-instance-type-offerings --location-type availability-zone \ --filters Name=instance-type,Values=p4d.24xlarge --region us-east-1 --output table&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;We see three availability zones containing p4d instances, we pick one of them (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-east-1c&lt;/code&gt;, yours may be different) when deploying our network stack. This can be done with the AWS Console or the AWS CLI. In our case we use the latter as follows&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws cloudformation create-stack --stack-name VPC-Large-Scale --capabilities CAPABILITY_IAM --template-body file://VPC-Large-Scale.yaml --parameters ParameterKey=SubnetsAZ,ParameterValue=us-east-1c&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;CloudFormation will deploy our new VPC, subnets, security groups and endpoints on our behalf. Once done, you can retrieve the IDs of the public and private subnets by querying the stack outputs and the values &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PublicSubnet&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PrivateSubnet&lt;/code&gt;.&lt;/p&gt;

    &lt;p&gt;For example, using the AWS CLI for the private subnet:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws cloudformation describe-stacks --stack-name VPC-Large-Scale --query &quot;Stacks[0].Outputs[?OutputKey=='PrivateSubnet'].OutputValue&quot; --output text&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Create ParallelCluster,&lt;/strong&gt; The cluster configuration file specifies the resources for our cluster. These resources include instance type for Head node, compute nodes, access to S3 buckets, shared storage where our data will be located. We will use Amazon FSx for Lustre that offers a fully managed shared storage service with &lt;a href=&quot;https://en.wikipedia.org/wiki/Lustre_(file_system)&quot;&gt;Lustre&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://github.com/lessw2020/t5_11/blob/main/hpc-cluster/cluster.yaml&quot;&gt;Here&lt;/a&gt; is an example of a cluster configuration file. We can use AWs ParallelCluster CLI to create the cluster. Please note that the private and public subnet IDs will need to be replaced by the ones you retrieved earlier. You will be able to control the cluster using the AWS ParallelCluster CLI to start, stop, pause, etc.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pcluster create-cluster --cluster-name my-hpc-cluster --cluster-configuration cluster.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;SSH to Head node -&lt;/strong&gt; once the cluster is ready, we can connect to the Head node using the SSH protocol, pull our training code with and place the data in the shared storage specified in the cluster configuration file.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pcluster ssh --cluster-name cluster -i your-key_pair
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Launch the training job -&lt;/strong&gt; now that we have the data and training code, we can launch the slurm job for training. Here is an &lt;a href=&quot;https://github.com/lessw2020/t5_11/blob/main/hpc-cluster/modified-bert.slurm&quot;&gt;example&lt;/a&gt; of a slurm script to launch the job using torchrun.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More details on how to set up the cluster is out of the scope of this post, however we will have a separate post on it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s next?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With this post we provided a high level overview of FSDP and how it efficiently scales distributed AI training. The flowchart included will help provide a checklist for you to review tuning options discussed such as the transformer wrapper and activation checkpointing.&lt;/p&gt;

&lt;p&gt;In the next posts, we will continue with the T5 model and go deeper into each of the topics above, specifically with sharding strategy and other optimizations to provide more insight and details. For now, a good reference for the sharding strategy is in our video tutorial &lt;a href=&quot;https://www.youtube.com/watch?v=a3iW6Cggccw&amp;amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&amp;amp;index=5&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;If you have questions or find an issue, please find the authors &lt;a href=&quot;https://www.linkedin.com/in/less-wright-22b59017/&quot;&gt;Less&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/hamid-nazeri/&quot;&gt;Hamid&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/geetachauhan/&quot;&gt;Geeta&lt;/a&gt; or open an issue on&lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt; PyTorch github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Special thanks to:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pytorch Distributed team, Shen Li, Rohan Varma, Yanli Zhao, Andrew Gu, Anjali Sridhar, Ana Simoes, Pierre-Yves Aquilanti, Sundar Ranganathan, and the broader AWS team for supporting us with providing infrastructure and technical support for running the large scale experiments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT&quot;&gt;FSDP video series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;Getting started with FSDP&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html&quot;&gt;Advanced tutorial on FSDP&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html?highlight=fsdp#module-torch.distributed.fsdp&quot;&gt;API documentation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;style&gt;

    td{
        border: 1px solid black;
    }
    
    article.pytorch-article table tr td:first-of-type{
        padding: 0.3125rem;
    }

    article.pytorch-article table td {
    padding: 0.3125rem;
    }
}

&lt;/style&gt;</content>

      
      
      
      
      

      <author>
          <name>Less Wright, Hamid Shojanazeri, Geeta Chauhan</name>
        
        
      </author>

      

      

      
        <summary type="html">Cutting-edge AI models are becoming extremely large. The cost and overhead of training these models is increasing rapidly, and involves large amounts of engineering and guesswork to find the right training regime. FSDP reduces these costs significantly by enabling you to train much larger models with the same amount of resources. FSDP lowers the memory footprint on your GPUs, and is usable via a lightweight configuration that requires substantially less effort, typically with just a few lines of code.</summary>
      

      
      
    </entry>
  
</feed>


