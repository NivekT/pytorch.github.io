<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-03-30T17:48:22-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Experience the power of PyTorch 2.0 on AMD Solutions</title>
      <link href="https://pytorch.org/blog/experience-power/" rel="alternate" type="text/html" title="Experience the power of PyTorch 2.0 on AMD Solutions" />
      <published>2023-03-29T00:00:00-07:00</published>
      <updated>2023-03-29T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/experience-power</id>
      <content type="html" xml:base="https://pytorch.org/blog/experience-power/">&lt;p&gt;PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework.  The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that PyTorch 2.0 stable release includes support for AMD Instinct™ and Radeon™ GPUs that are supported by the ROCm™ software platform.&lt;/p&gt;

&lt;p&gt;Along with the stable PyTorch 2.0 release, the Beta includes torch.compile underpinned by TorchInductor with support for AMD Instinct and Radeon GPUs through OpenAI Triton deep learning compiler. Through TorchInductor, developers can now generate low level code using Triton that are portable and performant to hand-written kernels on native hardware centric kernel programming models.&lt;/p&gt;

&lt;p&gt;Compilers like Triton can optimize the code generated by machine learning frameworks such as PyTorch for multiple AI accelerators including AMD Instinct GPU accelerator by leveraging hardware-specific features of the AMD CDNA™ GPU architecture. This makes it easy for developers and users to switch seamlessly from any HW to AMD Instinct GPU accelerators and get great out of the box performance.&lt;/p&gt;

&lt;p&gt;In addition, compilers like Triton can also enable developers to use high-level programming languages, such as Python, to write machine learning code that can be efficiently compiled and executed on specialized hardware. This can help greatly improve the productivity of machine learning developers, as they can focus on the algorithmic aspects of their models and rely on the compiler to generate efficient code.&lt;/p&gt;

&lt;p&gt;OpenAI Triton is a just-in-time (JIT) compiler that optimizes and accelerates the execution of deep learning models on various hardware architectures such as CPUs, GPUs, and ASICs. Here is a high-level overview&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Model Loading: The Triton server loads a trained deep learning model from a storage location, typically a file in a model format such as torchfx graphs.&lt;/li&gt;
  &lt;li&gt;Graph Optimization: Triton optimizes the graph representation of the loaded model. This includes transformations such as common subexpression elimination, dead code elimination, and operator fusion, which can help reduce memory usage and computational overhead.&lt;/li&gt;
  &lt;li&gt;Tensor Memory Allocation: Triton allocates memory for the tensors used by the model. This includes input and output tensors, as well as intermediate tensors created during computation.&lt;/li&gt;
  &lt;li&gt;Hardware-Specific Optimization: Triton applies hardware-specific optimizations to the optimized graph representation of the model. These optimizations can include using low-level hardware instructions, optimizing data movement between different types of memory, and leveraging hardware-specific data structures that leverages domain specific architectures like CDNA on AMD Instinct GPUs&lt;/li&gt;
  &lt;li&gt;Code Generation: Triton generates efficient machine code for the optimized graph representation of the model. This code can then be executed on the hardware platform for which it was optimized.&lt;/li&gt;
  &lt;li&gt;Execution: Triton executes the generated code on the hardware platform, typically in a just-in-time fashion. Triton can also dynamically adjust the batch size and other parameters of the model during execution to maximize performance.&lt;/li&gt;
  &lt;li&gt;Result Return: Triton returns the results of the computation to the client that requested the inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By design, PyTorch 2.0 is backward compatible to earlier PyTorch releases. This holds true for the ROCm build of PyTorch 2.0 as well. Developers using PyTorch with AMD GPUs can migrate to PyTorch 2.0 with the confidence that their existing code will continue to work without any required changes, so there is no penalty to access the improvements that come with this release. On the other hand, using PyTorch 2.0 and TorchInductor can result in significant performance improvement over the default eager-mode as shown below.&lt;/p&gt;

&lt;p&gt;The initial results using AMD Instinct MI250 GPUs already shows strong performance improvement with minimal optimization on TorchInductor compared to the default eager-mode. We see an average performance increase of up to 1.54X on 44 out of the 45 models on HuggingFace benchmarks suite with CamemBert, DistillGPT2 and T5Small being a few of the standout models with up to 1.5X or more performance improvement over eager-mode. We are looking forward to continued engagement with members of the PyTorch team at Meta to enable further optimization on ROCm software stack and the additional performance improvement for future PyTorch releases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/t-vs-eager-mode.svg&quot; alt=&quot;GPU performance improvement for TorchInductor vs eager-mode&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Image 1: AMD MI250 GPU performance improvement for TorchInductor vs eager-mode using HuggingFace &lt;sup&gt;MI200-89.&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.0 follows the same set of install options as before to build and install for supporting AMD GPUs. These include an installable Python package hosted at&lt;a href=&quot;https://pytorch.org/&quot;&gt; pytorch.org&lt;/a&gt;, AMD’s public PyTorch docker image, and of course the option to build from source using the upstream PyTorch repository. As with PyTorch builds for other platforms, the specific command line to be run for pip-based install is provided by the configurator at&lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt; https://pytorch.org/get-started/locally/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The GPUs supported by the ROCm software platform which forms the basis for PyTorch support on AMD GPUs are documented at&lt;a href=&quot;https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&quot;&gt; https://docs.amd.com/bundle/Hardware_and_Software_Reference_Guide/page/Hardware_and_Software_Support.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 represents a major step in continuing to broaden support for ML developers by increasing performance while maintaining a simple, Pythonic interface. This performance uplift is made possible in large part by the new TorchInductor infrastructure, which in turn harnesses the Triton ML programming language and just-in-time compiler. AMD’s support for these technologies allows users to realize the full promise of the new PyTorch architecture.  Our GPU support in PyTorch 2.0 is just one manifestation of a larger vision around AI and machine learning. AI/ML plays an important role in multiple AMD product lines, including Instinct and Radeon GPUs, Alveo™ data center accelerators, and both Ryzen™ and EPYC processors. These hardware and software initiatives are all part of AMD’s Pervasive AI vision, and we look forward to addressing the many new challenges and opportunities of this dynamic space.&lt;/p&gt;

&lt;p&gt;MI200-89 – PyTorch Inductor mode HuggingFace Transformers training speedup, running the standard PyTorch 2.0 test suite, over PyTorch eager-mode comparison based on AMD internal testing on a single GCD as of 3/10/2023 using a 2P AMD EPYC™ 7763 production server with 4x AMD Instinct™ MI250 (128GB HBM2e) 560W GPUs with Infinity Fabric™ technology; host ROCm™ 5.3, guest ROCm™ 5.4.4, PyTorch 2.0.0, Triton 2.0. Server manufacturers may vary configurations, yielding different results. Performance may vary based on factors including use of latest drivers and optimizations.&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;© 2023 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, AMD CDNA, AMD Instinct, EPYC, Radeon, ROCm, Ryzen, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective owners.&lt;/small&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 represents a significant step forward for the PyTorch machine learning framework. The stable release of PyTorch 2.0 brings new features that unlock even higher performance, while remaining backward compatible with prior releases and retaining the Pythonic focus which has helped to make PyTorch so enthusiastically adopted by the AI/ML community. AMD has long been a strong proponent of PyTorch, and we are delighted that PyTorch 2.0 stable release includes support for AMD Instinct™ and Radeon™ GPUs that are supported by the ROCm™ software platform.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated PyTorch 2 Transformers</title>
      <link href="https://pytorch.org/blog/accelerated-pytorch-2/" rel="alternate" type="text/html" title="Accelerated PyTorch 2 Transformers" />
      <published>2023-03-28T00:00:00-07:00</published>
      <updated>2023-03-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-pytorch-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-pytorch-2/">&lt;p&gt;The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API with the goal of making training and deployment of state-of-the-art Transformer models affordable.  Following the successful release of “fastpath” inference execution (“Better Transformer”), this release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA).&lt;/p&gt;

&lt;p&gt;You can take advantage of the new fused SDPA kernels either by calling the new SDPA operator directly (as described in the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#beta-implementing-high-performance-transformers-with-scaled-dot-product-attention-sdpa&quot;&gt;SDPA tutorial&lt;/a&gt;), or transparently via integration into the pre-existing PyTorch Transformer API.  All features of the PyTorch Transformer API will continue to work compatibly, with many features mapped to high-performance SDPA kernels, while other features are impossible to support with higher performance (e.g., need_weights, as per below) while expanded high-performance support for other features may still be under active development.  &lt;br /&gt;
 &lt;br /&gt;
Similar to the “fastpath” architecture, custom kernels are fully integrated into the PyTorch Transformer API – thus, using the native Transformer and MultiHeadAttention API will enable users to transparently see significant speed improvements.  Unlike the “fastpath” architecture, the newly introduced “custom kernels” support many more use cases including models using Cross-Attention, Transformer Decoders, and for training models, in addition to the existing fastpath inference for fixed and variable sequence length Transformer Encoder and Self Attention use cases.&lt;/p&gt;

&lt;p&gt;To take full advantage of different hardware models and Transformer use cases, multiple SDPA custom kernels are supported, with custom kernel selection logic that will pick the highest-performance kernel for a given model and hardware type.  In particular, the first custom kernels included with the PyTorch 2.0 release are the &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash Attention&lt;/a&gt; kernel (sdpa_flash, for 16-bit floating point training and inference on Nvidia GPUs with SM80+ architecture level) and the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers memory-efficient attention&lt;/a&gt; kernel (sdpa_mem_eff, for 16-bit and 32-bit floating point training and inference on a broad range of Nvidia GPUs).  A general-purpose kernel sdpa_math provides an implementation when the custom kernels are not applicable.&lt;/p&gt;

&lt;p&gt;As mentioned, custom kernels provide a wider range of support for execution scenarios To ensure efficient execution (e,g., to use GPU tensor cores), model configurations need to meet a small number of requirements.  This list of requirements will evolve over time, prospectively relaxing constraints limiting the usage of currently supported custom kernels, or providing additional kernels in the future.&lt;/p&gt;

&lt;p&gt;For the most up to date list of custom kernels and dispatch constraints, you can refer to &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/transformers/cuda/sdp_utils.h&quot;&gt;sdp_utils.h&lt;/a&gt;.  As of PyTorch 2.0, the existing fused SDPA kernels have the following constraints:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flash Attention only supports 16 bit floating point data types (float16 and bfloat16).&lt;/li&gt;
  &lt;li&gt;The head dimension must be a multiple of 8 for 16-bit floating point numbers and a multiple of 4 for 32-bit floating point numbers. At present, the maximum head_dim support for the Flash Attention custom kernel is 128.&lt;/li&gt;
  &lt;li&gt;The CUDA architecture level must be sm5x or better for the mem_efficient kernel, and sm80 for Flash Attention.&lt;/li&gt;
  &lt;li&gt;Flash Attention supports arbitrary dropout, in PyTorch 2.0 the mem_efficient kernel does not support dropout (i.e., dropout must be set to zero for this kernel to be selected in PyTorch 2.0).&lt;/li&gt;
  &lt;li&gt;To support variable-sequence length batches, all SDPA kernels support Nested Tensor inputs that combine input data and padding information using variable sequence length tensors for forward. (You can find more information about Nested Tensors in the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/nestedtensor.html&quot;&gt;Nested Tensor tutorial&lt;/a&gt;.)&lt;/li&gt;
  &lt;li&gt;You can specify both a &lt;em&gt;key_padding_mask&lt;/em&gt; and an &lt;em&gt;attn_mask&lt;/em&gt; by combining them before passing them to the SDPA operator. In particular, you can use the per-batch-element key padding mask of the nn.Transformer API to implement training for variable-sequence length inputs in a batch.&lt;/li&gt;
  &lt;li&gt;At present, the only attention mask supported by fused kernel implementation is the causal mask commonly used for training. To specify the causal mask in custom kernels, it must be specified with the &lt;em&gt;is_causal&lt;/em&gt; boolean and &lt;em&gt;attn_mask&lt;/em&gt; must be None.&lt;/li&gt;
  &lt;li&gt;Support for Nested Tensors is still under development.  Specifically, in PyTorch 2.0, only the sdpa_math kernel supports training with Nested Tensors. Also, PyTorch 2.0 does not support Nested Tensors as part of code being compiled with torch.compile().&lt;/li&gt;
  &lt;li&gt;The SDPA operator does not support returning averaged attention weights because computing them defeats the optimizations that enabled fused kernels to execute more efficiently.  The argument &lt;em&gt;need_weights&lt;/em&gt; for torch.nn.MultiheadAttention’s forward function defaults to True. In order to use the fused kernels, &lt;em&gt;need_weights&lt;/em&gt; needs to be set to &lt;em&gt;need_weights=False&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We find that an attention mask is rarely used in real-world applications, except for the causal mask during training.  Consequently, we reduce kernel complexity and compute cost by building in the option to use a causal mask as attention mask, and select this new capability with the &lt;em&gt;is_causal&lt;/em&gt; parameter introduced in conjunction with the new SDPA operator.&lt;/p&gt;

&lt;p&gt;Providing the &lt;em&gt;is_causal&lt;/em&gt; Boolean flag for the frequently used causal mask also obviates the expensive and memory-intensive allocation of a causal mask, increasing training memory efficiency by allowing more memory to be used for large batch sizes, and reduce memory bandwidth and cache contention – which are both at a premium in GPU accelerators – by not needing to load an attention mask tensor.&lt;/p&gt;

&lt;p&gt;If the constraints of none of the available custom kernels are met, then training falls back to using the default sdpa_math kernel, implementing the mathematical equations for scaled dot product attention using a sequence of PyTorch operator to implement SDPA.  This is the most general “catch-all” fallback kernel to ensure successful training for all models.&lt;/p&gt;

&lt;p&gt;In addition to the existing Transformer API, model developers may also use the scaled dot product attention kernels directly by calling the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention()&lt;/code&gt; operator.  This operator may be used to efficiently implement multi-head attention by combining it with in-projection and outprojection, as described in the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;SDPA tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition to adding custom kernels, Accelerated PyTorch 2 Transformers are integrated with PyTorch 2.0 compilation.  To use your model while benefiting from the additional acceleration of PT2-compilation (for inference or training), pre-process the model with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.compile(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have achieved major speedups for training transformer models and in particular large language models with Accelerated PyTorch 2 Transformers using a combination of custom kernels and torch.compile().&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch_better_transformer_chart1.png&quot; alt=&quot;Better Transformer chart&quot; width=&quot;100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Figure: Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Finally, because the custom kernels are much more memory efficient, try to increase the size of training batches to achieve faster training with increased batch size.&lt;/p&gt;

&lt;p&gt;In addition to automatic kernel selection, a context manager enables developers to override the kernel selection algorithm – this is not required for day to day operation, but enables developers to debug their code as well as enable performance engineers to override kernel selection. The SDPA tutorial provides additional information on using the SDPA context manager.&lt;/p&gt;

&lt;p&gt;In addition to availability as part of the nn.Transformer API, Accelerated PyTorch 2 Transformer custom kernels are also available in conjunction with the torchtext, torchvision, and fairseq domain libraries with the launch of PyTorch 2.0.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Michael Gschwind, Driss Guessous, Christian Puhrsch</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API with the goal of making training and deployment of state-of-the-art Transformer models affordable. Following the successful release of “fastpath” inference execution (“Better Transformer”), this release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.0 &amp;amp; XLA—The Latest Cutting Edge Features</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-xla/" rel="alternate" type="text/html" title="PyTorch 2.0 &amp; XLA—The Latest Cutting Edge Features" />
      <published>2023-03-22T00:00:00-07:00</published>
      <updated>2023-03-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-xla</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-xla/">&lt;p&gt;Today, we are excited to share our latest work for &lt;a href=&quot;https://github.com/pytorch/xla/releases/tag/v2.0.0&quot;&gt;PyTorch/XLA 2.0&lt;/a&gt;. The release of &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch 2.0&lt;/a&gt; is yet another major milestone for this storied community and we are excited to continue to be part of it. When the &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;PyTorch/XLA&lt;/a&gt; project started in 2018 between Google and Meta, the focus was on bringing cutting edge Cloud TPUs to help support the PyTorch community. Along the way, others in the community such as Amazon joined the project and very quickly the community expanded. We are excited about XLA’s &lt;a href=&quot;https://opensource.googleblog.com/2023/03/openxla-is-ready-to-accelerate-and-simplify-ml-development.html&quot;&gt;direction&lt;/a&gt; and the benefits this project continues to bring to the PyTorch community. In this blog we’d like to showcase some key features that have been in development, show code snippets, and illustrate the benefit through some benchmarks.&lt;/p&gt;

&lt;h2 id=&quot;torchdynamo--torchcompile-experimental&quot;&gt;TorchDynamo / torch.compile (Experimental)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/torchdynamo&quot;&gt;TorchDynamo&lt;/a&gt; (Dynamo) is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. It provides a clean API for compiler backends to hook in; its biggest feature is to dynamically modify Python bytecode just before execution. In the PyTorch/XLA 2.0 release, an experimental backend for Dynamo is provided for both inference and training.&lt;/p&gt;

&lt;p&gt;Dynamo provides a &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;Torch FX&lt;/a&gt; (FX) graph when it recognizes a model pattern and PyTorch/XLA uses a Lazy Tensor approach to compile the FX graph and return the compiled function. To get more insight regarding the technical details about PyTorch/XLA’s dynamo implementation, check out &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchdynamo-update-10-integrating-with-pytorch-xla-for-inference-and-training/935&quot;&gt;this&lt;/a&gt; dev-discuss post and &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md&quot;&gt;dynamo doc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a small code example of running ResNet18 with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torch_xla.core.xla_model as xm

def eval_model(loader):
  device = xm.xla_device()
  xla_resnet18 = torchvision.models.resnet18().to(device)
  xla_resnet18.eval()
  dynamo_resnet18 = torch.compile(
      xla_resnet18, backend='torchxla_trace_once')
  for data, _ in loader:
    output = dynamo_resnet18(data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; PyTorch/XLA only traces the ResNet18 model once during the init time and executes the compiled binary everytime &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamo_resnet18&lt;/code&gt; is invoked, instead of tracing the model every step. To illustrate the benefits of Dynamo+XLA, below is an inference speedup analysis to compare Dynamo and LazyTensor (without Dynamo) using TorchBench on a Cloud TPU v4-8 where the y-axis is the speedup multiplier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-inferencespeedup.svg&quot; alt=&quot;Inference Speedup - PyTorch/XLA Dynamo on TPU&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dynamo for training is in the development stage with its implementation being at an earlier stage than inference. Developers are welcome to test this early feature, however, in the 2.0 release, PyTorch/XLA supports the forward and backward pass graphs and not the optimizer graph; the optimizer graph is available in the nightly builds and will land in the PyTorch/XLA 2.1 release. Below is an example of what training looks like using the ResNet18 example with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torch_xla.core.xla_model as xm

def train_model(model, data, target):
  loss_fn = torch.nn.CrossEntropyLoss()
  pred = model(data)
  loss = loss_fn(pred, target)
  loss.backward()
  return pred

def train_model_main(loader):
  device = xm.xla_device()
  xla_resnet18 = torchvision.models.resnet18().to(device)
  xla_resnet18.train()
  dynamo_train_model = torch.compile(
        train_model, backend='aot_torchxla_trace_once')
  for data, target in loader:
    output = dynamo_train_model(xla_resnet18, data, target)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the backend for training is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aot_torchxla_trace_once&lt;/code&gt; (API will be updated for stable release) whereas the inference backend is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchxla_trace_once&lt;/code&gt; (name subject to change). We expect to extract and execute 3 graphs per training step instead of 1 training step if you use the Lazy tensor. Below is a training speedup analysis to compare Dynamo and Lazy using the TorchBench on Cloud TPU v4-8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-trainingspeedup.svg&quot; alt=&quot;Training Speedup - PyTorch/XLA Dynamo on TPU&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pjrt-runtime-beta&quot;&gt;PJRT Runtime (Beta)&lt;/h2&gt;

&lt;p&gt;PyTorch/XLA is migrating from XRT to the new PJRT runtime. PJRT is a better-maintained stack, with demonstrated performance advantages, including, on average, a 35% performance for training on TorchBench 2.0 models. It also supports a richer set of features enabling technologies like SPMD. In the PyTorch/XLA 2.0 release, PJRT is the default runtime for TPU and CPU; GPU support is in experimental state. The PJRT features included in the PyTorch/XLA 2.0 release are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TPU runtime implementation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libtpu&lt;/code&gt; using the &lt;a href=&quot;https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.md#rfc-openxla-pjrt-plugin&quot;&gt;PJRT Plugin API&lt;/a&gt; improves performance by up to 30%&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed&lt;/code&gt; support for TPU v2 and v3, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pjrt://&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;init_method&lt;/code&gt; (Experimental)&lt;/li&gt;
  &lt;li&gt;Single-host GPU support. Multi-host support coming soon. (Experimental)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Switching to PJRT requires no change (or minimal change for GPUs) to user code (see &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/pjrt.md&quot;&gt;pjrt.md&lt;/a&gt; for more details). Runtime configuration is as simple as setting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PJRT_DEVICE&lt;/code&gt; environment variable to the local device type (i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TPU&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPU&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CPU&lt;/code&gt;). Below are examples of using PJRT runtimes on different devices.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TPU Device
PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TPU Pod Device
gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;git clone --depth=1 --branch r2.0 https://github.com/pytorch/xla.git&quot;

gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# GPU Device (Experimental)
PJRT_DEVICE=GPU GPU_NUM_DEVICES=4 python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=128 --num_epochs=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is a performance comparison between XRT and PJRT by task on TorchBench 2.0 on v4-8 TPU. To learn more about PJRT vs. XRT please review the &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.0/docs/pjrt.md#tpu&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-torchbenchtraining.svg&quot; alt=&quot;TorchBench Training Time&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;parallelization&quot;&gt;Parallelization&lt;/h2&gt;

&lt;h3 id=&quot;gspmd-experimental&quot;&gt;GSPMD (Experimental)&lt;/h3&gt;

&lt;p&gt;We are delighted to introduce General and Scalable Parallelization for ML Computation Graphs (&lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt;) in PyTorch as a new experimental data &amp;amp; model sharding solution. &lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; provides automatic parallelization for common ML workloads, allowing developers to write PyTorch programs as if on a single large device and without custom sharded computation ops and/or collective communication ops. The XLA compiler transforms the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. The API (&lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt;) will be available in the PyTorch/XLA 2.0 release as an experimental feature on a single TPU VM host.&lt;/p&gt;

&lt;h4 id=&quot;next-steps-for-gspmd&quot;&gt;Next Steps for GSPMD&lt;/h4&gt;

&lt;p&gt;GSPMD is experimental in 2.0 release. To bring it to Stable status, we plan to address a number of feature gaps and known issues in the following releases, including multi-host support, DTensor integration, partial replication sharding, asynchronous data loading, and checkpointing.&lt;/p&gt;

&lt;h3 id=&quot;fsdp-beta&quot;&gt;FSDP (Beta)&lt;/h3&gt;

&lt;p&gt;PyTorch/XLA &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;introduced&lt;/a&gt; fully sharded data parallel (FSDP) experimental support in version 1.12. This feature is a parallel representation of PyTorch FSDP and there are subtle differences in how XLA and upstream CUDA kernels are set up. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt; is a new argument that enables developers to automatically specify conditions for propagating partitioning specifications to neural network submodules. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt;s may be simply passed in as an argument when wrapping a model with FSDP. Two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt; callables worth noting are: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_auto_wrap_policy&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transformer_auto_wrap_policy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_auto_wrap_policy&lt;/code&gt; enables users to wrap submodules with a minimum number of parameters. The example below wraps model submodules having at least 10M parameters.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto_wrap_policy = partial(size_based_auto_wrap_policy, min_num_params=1e7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transformer_auto_wrap_policy&lt;/code&gt; enables users to wrap all submodules that match a specific layer type. The example below wraps model submodules named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Conv2d&lt;/code&gt;. To learn more, review &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_fsdp.py#L237-L255&quot;&gt;this ResNet example&lt;/a&gt; by Ronghang Hu.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto_wrap_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={torch.nn.Conv2d})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch/XLA FSDP is now integrated in HuggingFace trainer class (&lt;a href=&quot;https://github.com/huggingface/transformers/pull/21406&quot;&gt;PR&lt;/a&gt;) enabling users to train much larger models on PyTorch/XLA (&lt;a href=&quot;https://huggingface.co/docs/transformers/main/en/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;official Hugging Face documentation&lt;/a&gt;). A 16B parameters GPT2 model trained on Cloud TPU v4-64 with this FSDP configuration achieved 39% hardware utilization.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot; style=&quot;max-width: 450px;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TPU Accelerator - Num Devices&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;v4-64
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GPT2 Parameter Count&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;16B
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Layers Wrapped with FSDP&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;GPT2Block
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TFLOPs / Chip&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;275
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PFLOPs / Step&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;50
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Hardware Utilization&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;39%
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;differences-between-fsdp--gspmd&quot;&gt;Differences Between FSDP &amp;amp; GSPMD&lt;/h3&gt;

&lt;p&gt;FSDP is a data parallelism technique that reduces device memory footprint by storing model parameters, optimizer states, and gradients all sharded. Note that the actual computation is still local to the device and requires all-gathering the sharded model parameters for both forward and backward passes, hence the name “data parallel”. FSDP is one of the newest additions to PyTorch/XLA to scale large model training.&lt;/p&gt;

&lt;p&gt;GSPMD on the other hand, is a general parallelization system that enables various types of parallelisms, including both data and model parallelisms. PyTorch/XLA provides a sharding annotation API and XLAShardedTensor abstraction, so a user can annotate any tensor with sharding specs in the PyTorch program. Developers don’t need to manually implement sharded computations or inject collective communications ops to get it right. The XLA compiler does the work so that each computation can run in a distributed manner on multiple devices.&lt;/p&gt;

&lt;h3 id=&quot;examples--preliminary-results&quot;&gt;Examples &amp;amp; Preliminary Results&lt;/h3&gt;

&lt;p&gt;To learn about PyTorch/XLA parallelism sharding API, visit our &lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt; and see the &lt;a href=&quot;https://github.com/pytorch/xla/tree/r2.0/test/spmd&quot;&gt;Sample Code&lt;/a&gt; references. Below is a simple example to enable data and model parallelism.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = SimpleLinear().to(xm.xla_device())
# Sharding annotate the linear layer weights.
xs.mark_sharding(model.fc1.weight, mesh, partition_spec)
# Training loop
model.train()
for step, (data, target) in enumerate(loader):
  optimizer.zero_grad()
  data = data.to(xm.xla_device())
  target = target.to(xm.xla_device())
  # Sharding annotate input data, we can shard any input
  # dimensions. Sharidng the batch dimension enables 
  # data parallelism, sharding the feature dimension enables
  # spatial partitioning.
  xs.mark_sharding(data, mesh, partition_spec)
  ouput = model(data)
  loss = loss_fn(output, target)
  optimizer.step()
  xm.mark_step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following graph highlights the memory efficiency benefits of PyTorch/XLA FSDP and SPMD on Cloud TPU v4-8 running ResNet50.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-batchsizescaling.svg&quot; alt=&quot;Batch Size Scaling with Spatial Partitioning&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing Thoughts…&lt;/h2&gt;

&lt;p&gt;We are excited to bring these features to the PyTorch community, and this is really just the beginning. Areas like dynamic shapes, deeper support for OpenXLA and many others are in development and we plan to put out more blogs to dive into the details. PyTorch/XLA is developed fully open source and we invite you to join the community of developers by filing issues, submitting pull requests, and sending RFCs on &lt;a href=&quot;github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt;. You can try PyTorch/XLA on a variety of XLA devices including TPUs and GPUs. &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;Here&lt;/a&gt; is how to get started.&lt;/p&gt;

&lt;p&gt;Congratulations again to the PyTorch community on this milestone!&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;The PyTorch Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jack Cao, Milad Mohammadi, Alex Wertheim, Yeounoh Chung, Joe Spisak, Will Cromar, Shauheen Zahirazami</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we are excited to share our latest work for PyTorch/XLA 2.0. The release of PyTorch 2.0 is yet another major milestone for this storied community and we are excited to continue to be part of it. When the PyTorch/XLA project started in 2018 between Google and Meta, the focus was on bringing cutting edge Cloud TPUs to help support the PyTorch community. Along the way, others in the community such as Amazon joined the project and very quickly the community expanded. We are excited about XLA’s direction and the benefits this project continues to bring to the PyTorch community. In this blog we’d like to showcase some key features that have been in development, show code snippets, and illustrate the benefit through some benchmarks.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Diffusers with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/accelerated-diffusers-pt-20/" rel="alternate" type="text/html" title="Accelerated Diffusers with PyTorch 2.0" />
      <published>2023-03-16T00:00:00-07:00</published>
      <updated>2023-03-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-diffusers-pt-20</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-diffusers-pt-20/">&lt;p&gt;PyTorch 2.0 has just been released. Its flagship new feature is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;, a one-line code change that promises to automatically improve performance across codebases. We have previously &lt;a href=&quot;https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/&quot;&gt;checked on that promise in Hugging Face Transformers and TIMM models&lt;/a&gt;, and delved deep into its &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;motivation, architecture and the road ahead&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As important as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is, there’s much more to PyTorch 2.0. Notably, PyTorch 2.0 incorporates several strategies to accelerate transformer blocks, and these improvements are very relevant for diffusion models too. Techniques such as &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt;, for example, have become very popular in the diffusion community thanks to their ability to significantly speed up Stable Diffusion and achieve larger batch sizes, and they are now part of PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;In this post we discuss how attention layers are optimized in PyTorch 2.0 and how these optimization are applied to the popular &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;🧨 Diffusers library&lt;/a&gt;. We finish with a benchmark that shows how the use of PyTorch 2.0 and Diffusers immediately translates to significant performance improvements across different hardware.&lt;/p&gt;

&lt;h2 id=&quot;accelerating-transformer-blocks&quot;&gt;Accelerating transformer blocks&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 includes a &lt;em&gt;scaled dot-product attention&lt;/em&gt; function as part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional&lt;/code&gt;. This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. Before PyTorch 2.0, you had to search for third-party implementations and install separate packages in order to take advantage of memory optimized algorithms, such as FlashAttention. The available implementations are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;FlashAttention, from the official &lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;FlashAttention project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Memory-Efficient Attention, from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A native C++ implementation suitable for non-CUDA devices or when high-precision is required.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these methods are available by default, and PyTorch will try to select the optimal one automatically through the use of the new scaled dot-product attention (SDPA) API. You can also individually toggle them for finer-grained control, see &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention&quot;&gt;the documentation&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;using-scaled-dot-product-attention-in-diffusers&quot;&gt;Using scaled dot-product attention in diffusers&lt;/h2&gt;

&lt;p&gt;The incorporation of Accelerated PyTorch 2.0 Transformer attention to the Diffusers library was achieved through the use of the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/api/models#diffusers.UNet2DConditionModel.set_attn_processor&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_attn_processor&lt;/code&gt; method&lt;/a&gt;, which allows for pluggable attention modules to be configured. In this case, a &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57/src/diffusers/models/cross_attention.py#L469&quot;&gt;new attention processor was created&lt;/a&gt;, which is &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57bb7a9ee13af4a08492e524b0a145a2c5/src/diffusers/models/cross_attention.py#L105&quot;&gt;enabled by default when PyTorch 2.0 is available&lt;/a&gt;. For clarity, this is how you could enable it manually (but it’s usually not necessary since diffusers will automatically take care of it):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionPipeline
from diffusers.models.cross_attention import AttnProcessor2_0

pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;)
pipe.to(&quot;cuda&quot;)
pipe.unet.set_attn_processor(AttnProcessor2_0())

prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
image = pipe(prompt).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;stable-diffusion-benchmark&quot;&gt;Stable Diffusion Benchmark&lt;/h2&gt;

&lt;p&gt;We ran a number of tests using accelerated dot-product attention from PyTorch 2.0 in Diffusers. We installed diffusers from pip and used nightly versions of PyTorch 2.0, since our tests were performed before the official release. We also used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.set_float32_matmul_precision('high')&lt;/code&gt; to enable additional fast matrix multiplication algorithms.&lt;/p&gt;

&lt;p&gt;We compared results with the traditional attention implementation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diffusers&lt;/code&gt; (referred to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vanilla&lt;/code&gt; below) as well as with the best-performing solution in pre-2.0 PyTorch: PyTorch 1.13.1 with the xFormers package (v0.0.16) installed.&lt;/p&gt;

&lt;p&gt;Results were measured without compilation (i.e., no code changes at all), and also with a single call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to wrap the UNet module. We did not compile the image decoder because most of the time is spent in the 50 denoising iterations that run UNet evaluations.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float32&quot;&gt;Results in float32&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig1-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float32&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following figures explore performance improvement vs batch size for various representative GPUs belonging to different generations. We collected data for each combination until we reached maximum memory utilization. Vanilla attention runs out of memory earlier than xFormers or PyTorch 2.0, which explains the missing bars for larger batch sizes. Similarly, A100 (we used the 40 GB version) is capable of running batch sizes of 64, but the other GPUs could only reach 32 in our tests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig2-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig3-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig4-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig5-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (V100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We found very significant performance improvements over vanilla attention across the board, without even using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;. An out of the box installation of PyTorch 2.0 and diffusers yields about 50% speedup on A100 and between 35% and 50% on 4090 GPUs, depending on batch size. Performance improvements are more pronounced for modern CUDA architectures such as Ada (4090) or Ampere (A100), but they are still very significant for older architectures still heavily in use in cloud services.&lt;/p&gt;

&lt;p&gt;In addition to faster speeds, the accelerated transformers implementation in PyTorch 2.0 allows much larger batch sizes to be used. A single 40GB A100 GPU runs out of memory with a batch size of 10, and 24 GB high-end consumer cards such as 3090 and 4090 cannot generate 8 images at once. Using PyTorch 2.0 and diffusers we could achieve batch sizes of &lt;strong&gt;48&lt;/strong&gt; for 3090 and 4090, and &lt;strong&gt;64&lt;/strong&gt; for A100. This is of great significance for cloud services and applications, as they can efficiently process more images at a time.&lt;/p&gt;

&lt;p&gt;When compared with PyTorch 1.13.1 + xFormers, the new accelerated transformers implementation is still faster and requires no additional packages or dependencies. In this case we found moderate speedups of up to 2% on datacenter cards such as A100 or T4, but performance was great on the two last generations of consumer cards: up to 20% speed improvement on 3090 and between 10% and 45% on 4090, depending on batch size.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is used, we get an additional performance boost of (typically) 2% and 3% over the previous improvements. As compilation takes some time, this is better geared towards user-facing inference services or training.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float16&quot;&gt;Results in float16&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig6-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float16&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig7-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig8-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig9-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float16&lt;/code&gt; inference, the performance improvements of the accelerated transformers implementation in PyTorch 2.0 are between 20% and 28% over standard attention, across all the GPUs we tested, except for the 4090, which belongs to the more modern Ada architecture. This GPU benefits from a dramatic performance improvement when using PyTorch 2.0 nightlies. With respect to optimized SDPA vs xFormers, results are usually on par for most GPUs, except again for the 4090. Adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to the mix boosts performance a few more percentage points across the board.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 comes with multiple features to optimize the crucial components of the foundational transformer block, and they can be further improved with the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. These optimizations lead to significant memory and time improvements for diffusion models, and remove the need for third-party library installations.&lt;/p&gt;

&lt;p&gt;To take advantage of these speed and memory improvements all you have to do is upgrade to PyTorch 2.0 and use diffusers &amp;gt;= 0.13.0.&lt;/p&gt;

&lt;p&gt;For more examples and in-detail benchmark numbers, please also have a look at the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/torch2.0&quot;&gt;Diffusers with PyTorch 2.0&lt;/a&gt; docs.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The authors are grateful to the PyTorch team for creating such excellent software.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pedro Cuenca, Patrick von Platen, Suraj Patil</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 has just been released. Its flagship new feature is torch.compile(), a one-line code change that promises to automatically improve performance across codebases. We have previously checked on that promise in Hugging Face Transformers and TIMM models, and delved deep into its motivation, architecture and the road ahead.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">New Library Updates in PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/" rel="alternate" type="text/html" title="New Library Updates in PyTorch 2.0" />
      <published>2023-03-15T00:00:00-07:00</published>
      <updated>2023-03-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/new-library-updates-in-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We are bringing a number of improvements to the current PyTorch libraries, alongside the &lt;a href=&quot;/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0 release&lt;/a&gt;. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.&lt;/p&gt;

&lt;p&gt;Along with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. Please find the list of the latest stable versions and updates below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Latest Stable Library Versions (&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;Full List&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchArrow 0.1.0
   &lt;/td&gt;
   &lt;td&gt;TorchRec 0.4.0
   &lt;/td&gt;
   &lt;td&gt;TorchVision 0.15
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchAudio 2.0
   &lt;/td&gt;
   &lt;td&gt;TorchServe 0.7.1
   &lt;/td&gt;
   &lt;td&gt;TorchX 0.4.0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchData 0.6.0
   &lt;/td&gt;
   &lt;td&gt;TorchText 0.15.0
   &lt;/td&gt;
   &lt;td&gt;PyTorch on XLA Devices 1.14
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;prior versions&lt;/a&gt; or (unstable) nightlies, click on versions in the top left menu above ‘Search Docs’.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio&quot;&gt;TorchAudio&lt;/h2&gt;

&lt;h3 id=&quot;beta-data-augmentation-operators&quot;&gt;[Beta] Data augmentation operators&lt;/h3&gt;

&lt;p&gt;The release adds several data augmentation operators under torchaudio.functional and torchaudio.transforms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;torchaudio.functional.add_noise&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.convolve&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.deemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.fftconvolve&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.preemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.speed&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.AddNoise&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Convolve&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Deemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.FFTConvolve&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Preemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Speed&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.SpeedPerturbation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The operators can be used to synthetically diversify training data to improve the generalizability of downstream models.&lt;/p&gt;

&lt;p&gt;For usage details, please refer to the &lt;a href=&quot;https://pytorch.org/audio/2.0.0/functional.html&quot;&gt;functional&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/transforms.html&quot;&gt;transform&lt;/a&gt; documentation and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/tutorials/audio_data_augmentation_tutorial.html&quot;&gt;Audio Data Augmentation&lt;/a&gt; tutorial.&lt;/p&gt;

&lt;h3 id=&quot;beta-wavlm-and-xls-r-models&quot;&gt;[Beta] WavLM and XLS-R models&lt;/h3&gt;

&lt;p&gt;The release adds two self-supervised learning models for speech and audio.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/9814838&quot;&gt;WavLM&lt;/a&gt; that is robust to noise and reverberation.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.09296&quot;&gt;XLS-R&lt;/a&gt; that is trained on cross-lingual datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides the model architectures, torchaudio also supports corresponding pre-trained pipelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_BASE&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_BASE_PLUS&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_LARGE&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_300M&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_1B&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_2B&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For usage details, please refer to the &lt;a href=&quot;https://pytorch.org/audio/2.0.0/generated/torchaudio.models.Wav2Vec2Model.html#factory-functions&quot;&gt;factory function&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/pipelines.html#id3&quot;&gt;pre-trained pipelines&lt;/a&gt; documentation.&lt;/p&gt;

&lt;h2 id=&quot;torchrl&quot;&gt;TorchRL&lt;/h2&gt;

&lt;p&gt;The initial release of torchrl includes several features that span across the entire RL domain. TorchRL can already be used in online, offline, multi-agent, multi-task and distributed RL settings, among others. See below:&lt;/p&gt;

&lt;h3 id=&quot;beta-environment-wrappers-and-transforms&quot;&gt;[Beta] Environment wrappers and transforms&lt;/h3&gt;

&lt;p&gt;torchrl.envs includes several wrappers around common environment libraries. This allows users to swap one library with another without effort. These wrappers build an interface between these simulators and torchrl:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;dm_control:&lt;/li&gt;
  &lt;li&gt;Gym&lt;/li&gt;
  &lt;li&gt;Brax&lt;/li&gt;
  &lt;li&gt;EnvPool&lt;/li&gt;
  &lt;li&gt;Jumanji&lt;/li&gt;
  &lt;li&gt;Habitat&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It also comes with many commonly used transforms and vectorized environment utilities that allow for a fast execution across simulation libraries. Please refer to the &lt;a href=&quot;https://pytorch.org/rl/reference/envs.html&quot;&gt;documentation&lt;/a&gt; for more detail.&lt;/p&gt;

&lt;h3 id=&quot;beta-datacollectors&quot;&gt;[Beta] Datacollectors&lt;/h3&gt;

&lt;p&gt;Data collection in RL is made easy via the usage of single process or multiprocessed/distributed data collectors that execute the policy in the environment over a desired duration and deliver samples according to the user’s needs. These can be found in torchrl.collectors and are documented &lt;a href=&quot;https://pytorch.org/rl/reference/collectors.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-objective-modules&quot;&gt;[Beta] Objective modules&lt;/h3&gt;

&lt;p&gt;Several objective functions are included in torchrl.objectives, among which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A generic PPOLoss class and derived ClipPPOLoss and KLPPOLoss&lt;/li&gt;
  &lt;li&gt;SACLoss and DiscreteSACLoss&lt;/li&gt;
  &lt;li&gt;DDPGLoss&lt;/li&gt;
  &lt;li&gt;DQNLoss&lt;/li&gt;
  &lt;li&gt;REDQLoss&lt;/li&gt;
  &lt;li&gt;A2CLoss&lt;/li&gt;
  &lt;li&gt;TD3Loss&lt;/li&gt;
  &lt;li&gt;ReinforceLoss&lt;/li&gt;
  &lt;li&gt;Dreamer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vectorized value function operators also appear in the library. Check the documentation &lt;a href=&quot;https://pytorch.org/rl/reference/objectives.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-models-and-exploration-strategies&quot;&gt;[Beta] Models and exploration strategies&lt;/h3&gt;

&lt;p&gt;We provide multiple models, modules and exploration strategies. Get a detailed description in &lt;a href=&quot;https://pytorch.org/rl/reference/modules.html&quot;&gt;the doc&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-composable-replay-buffer&quot;&gt;[Beta] Composable replay buffer&lt;/h3&gt;

&lt;p&gt;A composable replay buffer class is provided that can be used to store data in multiple contexts including single and multi-agent, on and off-policy and many more.. Components include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storages (list, physical or memory-based contiguous storages)&lt;/li&gt;
  &lt;li&gt;Samplers (Prioritized, sampler without repetition)&lt;/li&gt;
  &lt;li&gt;Writers&lt;/li&gt;
  &lt;li&gt;Possibility to add transforms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Replay buffers and other data utilities are documented &lt;a href=&quot;https://pytorch.org/rl/reference/data.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-logging-tools-and-trainer&quot;&gt;[Beta] Logging tools and trainer&lt;/h3&gt;

&lt;p&gt;We support multiple logging tools including tensorboard, wandb and mlflow.&lt;/p&gt;

&lt;p&gt;We provide a generic Trainer class that allows for easy code recycling and checkpointing.&lt;/p&gt;

&lt;p&gt;These features are documented &lt;a href=&quot;https://pytorch.org/rl/reference/trainers.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;tensordict&quot;&gt;TensorDict&lt;/h2&gt;

&lt;p&gt;TensorDict is a new data carrier for PyTorch.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensordict-specialized-dictionary-for-pytorch&quot;&gt;[Beta] TensorDict: specialized dictionary for PyTorch&lt;/h3&gt;

&lt;p&gt;TensorDict allows you to execute many common operations across batches of tensors carried by a single container. TensorDict supports many shape and device or storage operations, and  can readily be used in distributed settings. Check the &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/&quot;&gt;documentation&lt;/a&gt; to know more.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensorclass-a-dataclass-for-pytorch&quot;&gt;[Beta] @tensorclass: a dataclass for PyTorch&lt;/h3&gt;

&lt;p&gt;Like TensorDict, &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/reference/prototype.html&quot;&gt;tensorclass&lt;/a&gt; provides the opportunity to write dataclasses with built-in torch features such as shape or device operations.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensordictnn-specialized-modules-for-tensordict&quot;&gt;[Beta] tensordict.nn: specialized modules for TensorDict&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/reference/nn.html&quot;&gt;tensordict.nn module&lt;/a&gt; provides specialized nn.Module subclasses that make it easy to build arbitrarily complex graphs that can be executed with TensorDict inputs. It is compatible with the latest PyTorch features such as functorch, torch.fx and torch.compile.&lt;/p&gt;

&lt;h2 id=&quot;torchrec&quot;&gt;TorchRec&lt;/h2&gt;

&lt;h3 id=&quot;beta-keyedjaggedtensor-all-to-all-redesign-and-input-dist-fusion&quot;&gt;[Beta] KeyedJaggedTensor All-to-All Redesign and Input Dist Fusion&lt;/h3&gt;

&lt;p&gt;We observed performance regression due to a bottleneck in sparse data distribution for models that have multiple, large KJTs to redistribute.&lt;/p&gt;

&lt;p&gt;To combat this we altered the comms pattern to transport the minimum data required in the initial collective to support the collective calls for the actual KJT tensor data. This data sent in the initial collective, ‘splits’ means more data is transmitted over the comms stream overall, but the CPU is blocked for significantly shorter amounts of time leading to better overall QPS.&lt;/p&gt;

&lt;p&gt;Furthermore, we altered the TorchRec train pipeline to group the initial collective calls for the splits together before launching the more expensive KJT tensor collective calls. This fusion minimizes the CPU blocked time as launching each subsequent input distribution is no longer dependent on the previous input distribution.&lt;/p&gt;

&lt;p&gt;With this feature, variable batch sizes are now natively supported across ranks. These features are documented &lt;a href=&quot;https://github.com/pytorch/torchrec/commit/d0d23bef8aef5a79a1061fbc842c97bb68b91463&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchvision&quot;&gt;TorchVision&lt;/h2&gt;

&lt;h3 id=&quot;beta-extending-torchvisions-transforms-to-object-detection-segmentation--video-tasks&quot;&gt;[Beta] Extending TorchVision’s Transforms to Object Detection, Segmentation &amp;amp; Video tasks&lt;/h3&gt;

&lt;p&gt;TorchVision is extending its Transforms API! Here is what’s new:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You can use them not only for Image Classification but also for Object Detection, Instance &amp;amp; Semantic Segmentation and Video Classification.&lt;/li&gt;
  &lt;li&gt;You can use new functional transforms for transforming Videos, Bounding Boxes and Segmentation Masks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about these new transforms &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/&quot;&gt;from our docs&lt;/a&gt;, and submit any feedback in our &lt;a href=&quot;https://github.com/pytorch/vision/issues/6753&quot;&gt;dedicated issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchtext&quot;&gt;TorchText&lt;/h2&gt;

&lt;h3 id=&quot;beta-adding-scriptable-t5-and-flan-t5-to-the-torchtext-library-with-incremental-decoding-support&quot;&gt;[Beta] Adding scriptable T5 and Flan-T5 to the TorchText library with incremental decoding support!&lt;/h3&gt;

&lt;p&gt;TorchText has added the T5 model architecture with pre-trained weights for both the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;original T5 paper&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;&gt;Flan-T5&lt;/a&gt;. The model is fully torchscriptable and features an optimized &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.ao.nn.quantizable.MultiheadAttention.html?highlight=multihead#torch.ao.nn.quantizable.MultiheadAttention&quot;&gt;multiheaded attention implementation&lt;/a&gt;. We include several examples of how to utilize the model including summarization, classification, and translation.&lt;/p&gt;

&lt;p&gt;For more details, please refer to &lt;a href=&quot;https://pytorch.org/text/stable/models.html&quot;&gt;our docs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchx&quot;&gt;TorchX&lt;/h2&gt;

&lt;p&gt;TorchX is moving to community supported mode. More details will be coming in at a later time.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-release/" rel="alternate" type="text/html" title="PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever" />
      <published>2023-03-15T00:00:00-07:00</published>
      <updated>2023-03-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-release</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-release/">&lt;p&gt;We are excited to announce the release of &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.0.0&quot;&gt;PyTorch® 2.0&lt;/a&gt; which we highlighted during the &lt;a href=&quot;https://www.youtube.com/@PyTorch/playlists?view=50&amp;amp;sort=dd&amp;amp;shelf_id=2&quot;&gt;PyTorch Conference&lt;/a&gt; on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.&lt;/p&gt;

&lt;p&gt;This next-generation release includes a Stable version of Accelerated Transformers (formerly called Better Transformers); Beta includes torch.compile as the main API for PyTorch 2.0,  the scaled_dot_product_attention function as part of torch.nn.functional, the MPS backend, functorch APIs in the torch.func module; and other Beta/Prototype improvements across various inferences, performance and training optimization features on GPUs and CPUs. For a comprehensive introduction and technical overview of torch.compile, please visit the 2.0 &lt;a href=&quot;/get-started/pytorch-2.0&quot;&gt;Get Started page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Along with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. More details can be found in this &lt;a href=&quot;/blog/new-library-updates-in-pytorch-2.0/&quot;&gt;library blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This release is composed of over 4,541 commits and 428 contributors since 1.13.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.0 and the overall 2-series this year.&lt;/p&gt;

&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.&lt;/li&gt;
  &lt;li&gt;As an underpinning technology of torch.compile, TorchInductor with Nvidia and AMD GPUs will rely on OpenAI Triton deep learning compiler to generate performant code and hide low level hardware details. OpenAI Triton-generated kernels achieve performance that’s on par with hand-written kernels and specialized cuda libraries such as cublas.&lt;/li&gt;
  &lt;li&gt;Accelerated Transformers introduce high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA). The API is integrated with torch.compile() and model developers may also use the &lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;scaled dot product attention&lt;/a&gt; kernels directly by calling the new scaled_dot_product_attention() operator.&lt;/li&gt;
  &lt;li&gt;Metal Performance Shaders (MPS) backend provides GPU accelerated PyTorch training on Mac platforms with added support for Top 60 most used ops, bringing coverage to over 300 operators.&lt;/li&gt;
  &lt;li&gt;Amazon AWS optimizes the PyTorch CPU inference on AWS Graviton3 based &lt;a href=&quot;https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7g-instances-powered-by-aws-graviton3-processors/&quot;&gt;C7g instances&lt;/a&gt;. PyTorch 2.0 improves inference performance on Graviton compared to the previous releases, including improvements for Resnet50 and Bert.&lt;/li&gt;
  &lt;li&gt;New prototype features and technologies across TensorParallel, DTensor, 2D parallel, TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.&lt;/li&gt;
&lt;/ul&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;
&lt;strong&gt;Stable&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;a href=&quot;#stable-features&quot;&gt;Accelerated PT 2 Transformers&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-features&quot;&gt;torch.compile&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-features&quot;&gt;DTensor&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;CUDA support for 11.7 &amp;amp; 11.8 (deprecating CUDA 11.6) &lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-pytorch-mps-backend&quot;&gt;PyTorch MPS Backend&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-tensorparallel&quot;&gt;TensorParallel&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt; 

&lt;a href=&quot;#deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;Python 3.8 (deprecating Python 3.7)&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;Scaled dot product attention&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-2d-parallel&quot;&gt;2D Parallel&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#optimized-pytorch-inference-with-aws-graviton-processors&quot;&gt;AWS Graviton3&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-functorch---torchfunc&quot;&gt;functorch&lt;/a&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;

&lt;a href=&quot;#beta-torchcompile&quot;&gt;Torch.compile (dynamic=True)&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#beta-dispatchable-collectives&quot;&gt;Dispatchable Collectives&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#beta-torchset_default_device-and-torchdevice-as-context-manager&quot;&gt;Torch.set_default &amp;amp; torch.device&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-x86-as-the-new-default-quantization-backend-for-x86-cpu&quot;&gt;X86 quantization backend&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-gnn-inference-and-training-optimization-on-cpu&quot;&gt;GNN inference and training performance&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public 2.0, 1.13 and 1.12 feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1H3jazwO8BBCwK8JwLNYspLiHfUrzshEtyqjL-X93I9g/edit#gid=790902532&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stable-features&quot;&gt;Stable Features&lt;/h2&gt;

&lt;h3 id=&quot;stable-accelerated-pytorch-2-transformers&quot;&gt;[Stable] Accelerated PyTorch 2 Transformers&lt;/h3&gt;

&lt;p&gt;The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API. In releasing Accelerated PT2 Transformers, our goal is to make training and deployment of state-of-the-art Transformer models affordable across the industry. This release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA), extending the inference “fastpath” architecture, previously known as “Better Transformer.”&lt;/p&gt;

&lt;p&gt;Similar to the “fastpath” architecture, custom kernels are fully integrated into the PyTorch Transformer API – thus, using the native Transformer and MultiHeadAttention API will enable users to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;transparently see significant speed improvements;&lt;/li&gt;
  &lt;li&gt;support many more use cases including models using Cross-Attention, Transformer Decoders, and for training models; and&lt;/li&gt;
  &lt;li&gt;continue to use fastpath inference for fixed and variable sequence length Transformer Encoder and Self Attention use cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To take full advantage of different hardware models and Transformer use cases, multiple SDPA custom kernels are supported (see below), with custom kernel selection logic that will pick the highest-performance kernel for a given model and hardware type. In addition to the existing Transformer API, model developers may also use the &lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;scaled dot product attention&lt;/a&gt; kernels directly by calling the new scaled_dot_product_attention() operator. Accelerated PyTorch 2 Transformers are integrated with torch.compile() .  To use your model while benefiting from the additional acceleration of PT2-compilation (for inference or training), pre-process the model with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model = torch.compile(model)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We have achieved major speedups for training transformer models and in particular large language models with Accelerated PyTorch 2 Transformers using a combination of custom kernels and torch.compile().&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch20post.png&quot; alt=&quot;alt_text&quot; title=&quot;Accelerated PyTorch 2 speed&quot; width=&quot;100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Figure: Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-torchcompile&quot;&gt;[Beta] torch.compile&lt;/h3&gt;

&lt;p&gt;torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.&lt;/p&gt;

&lt;p&gt;Underpinning torch.compile are new technologies – TorchDynamo, AOTAutograd, PrimTorch and TorchInductor:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TorchDynamo captures PyTorch programs safely using Python Frame Evaluation Hooks and is a significant innovation that was a result of 5 years of our R&amp;amp;D into safe graph capture.&lt;/li&gt;
  &lt;li&gt;AOTAutograd overloads PyTorch’s autograd engine as a tracing autodiff for generating ahead-of-time backward traces.&lt;/li&gt;
  &lt;li&gt;PrimTorch canonicalizes ~2000+ PyTorch operators down to a closed set of ~250 primitive operators that developers can target to build a complete PyTorch backend. This substantially lowers the barrier of writing a PyTorch feature or backend.&lt;/li&gt;
  &lt;li&gt;TorchInductor is a deep learning compiler that generates fast code for multiple accelerators and backends. For NVIDIA and AMD GPUs, it uses OpenAI Triton as a key building block. For intel CPUs, we generate C++ code using multithreading, vectorized instructions and offloading appropriate operations to mkldnn when possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all the new technologies, torch.compile is able to work 93% of time across 165 open-source models and runs 20% faster on average at float32 precision and 36% faster on average at AMP precision.&lt;/p&gt;

&lt;p&gt;For more information, please refer to &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;https://pytorch.org/get-started/pytorch-2.0/&lt;/a&gt; and for TorchInductor CPU with Intel &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-update-5-cpu-backend-backend-performance-update-and-deep-dive-on-key-optimizations/1117&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-pytorch-mps-backend&quot;&gt;[Beta] PyTorch MPS Backend&lt;/h3&gt;

&lt;p&gt;MPS backend provides GPU-accelerated PyTorch training on Mac platforms. This release brings improved correctness, stability, and operator coverage.&lt;/p&gt;

&lt;p&gt;MPS backend now includes support for the Top 60 most used ops, along with the most frequently requested operations by the community, bringing coverage to over 300 operators. The major focus of the release was to enable full OpInfo-based forward and gradient mode testing to address silent correctness issues. These changes have resulted in wider adoption of MPS backend by 3rd party networks such as Stable Diffusion, YoloV5, WhisperAI, along with increased coverage for Torchbench networks and Basic tutorials. We encourage developers to update to the latest macOS release to see the best performance and stability on the MPS backend.&lt;/p&gt;

&lt;p&gt;Links&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/mps.html&quot;&gt;MPS Backend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/MPS-Backend&quot;&gt;Developer information&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.apple.com/metal/pytorch/&quot;&gt;Accelerated PyTorch training on Mac&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.apple.com/documentation/metal?language=objc&quot;&gt;Metal&lt;/a&gt;, &lt;a href=&quot;https://developer.apple.com/documentation/metalperformanceshaders?language=objc&quot;&gt;Metal Performance Shaders&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://developer.apple.com/documentation/metalperformanceshadersgraph?language=objc&quot;&gt;Metal Performance Shaders Graph&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;beta-scaled-dot-product-attention-20&quot;&gt;[Beta] Scaled dot product attention 2.0&lt;/h3&gt;

&lt;p&gt;We are thrilled to announce the release of PyTorch 2.0, which introduces a powerful scaled dot product attention function as part of torch.nn.functional. This function includes multiple implementations that can be seamlessly applied depending on the input and hardware in use.&lt;/p&gt;

&lt;p&gt;In previous versions of PyTorch, you had to rely on third-party implementations and install separate packages to take advantage of memory-optimized algorithms like &lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;FlashAttention&lt;/a&gt;. With PyTorch 2.0, all these implementations are readily available by default.&lt;/p&gt;

&lt;p&gt;These implementations include &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; from HazyResearch, Memory-Efficient Attention from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; project, and a native C++ implementation that is ideal for non-CUDA devices or when high-precision is required.&lt;/p&gt;

&lt;p&gt;PyTorch 2.0 will automatically select the optimal implementation for your use case, but you can also toggle them individually for finer-grained control. Additionally, the scaled dot product attention function can be used to build common transformer architecture components.&lt;/p&gt;

&lt;p&gt;Learn more with the &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention&quot;&gt;documentation&lt;/a&gt; and this &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-functorch---torchfunc&quot;&gt;[Beta] functorch -&amp;gt; torch.func&lt;/h3&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://github.com/google/jax&quot;&gt;Google JAX&lt;/a&gt;, functorch is a library that offers composable vmap (vectorization) and autodiff transforms. It enables advanced autodiff use cases that would otherwise be tricky to express in PyTorch. Examples include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ensembling.html&quot;&gt;model ensembling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/jacobians_hessians.html&quot;&gt;efficiently computing jacobians and hessians&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/per_sample_grads.html&quot;&gt;computing per-sample-gradients (or other per-sample quantities)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re excited to announce that, as the final step of upstreaming and integrating functorch into PyTorch, the functorch APIs are now available in the torch.func module. Our function transform APIs are identical to before, but we have changed how the interaction with NN modules work. Please see the &lt;a href=&quot;https://pytorch.org/docs/master/func.html&quot;&gt;docs&lt;/a&gt; and the &lt;a href=&quot;https://pytorch.org/docs/master/func.migrating.html&quot;&gt;migration guide&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Furthermore, we have &lt;a href=&quot;https://pytorch.org/docs/master/notes/extending.func.html&quot;&gt;added support for torch.autograd.Function&lt;/a&gt;: one is now able to apply function transformations (e.g. vmap, grad, jvp) over torch.autograd.Function.&lt;/p&gt;

&lt;h3 id=&quot;beta-dispatchable-collectives&quot;&gt;[Beta] Dispatchable Collectives&lt;/h3&gt;

&lt;p&gt;Dispatchable collectives is an improvement to the existing init_process_group() API which changes backend to an optional argument. For users, the main advantage of this feature is that it will allow them to write code that can run on both GPU and CPU machines without having to change the backend specification. The dispatchability feature will also make it easier for users to support both GPU and CPU collectives, as they will no longer need to specify the backend manually (e.g. “NCCL” or “GLOO”). Existing backend specifications by users will be honored and will not require change.&lt;/p&gt;

&lt;p&gt;Usage example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch.distributed.dist
…
# old
dist.init_process_group(backend=”nccl”, ...)
dist.all_reduce(...) # with CUDA tensors works
dist.all_reduce(...) # with CPU tensors does not work

# new
dist.init_process_group(...) # backend is optional
dist.all_reduce(...) # with CUDA tensors works
dist.all_reduce(...) # with CPU tensors works
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/docs/master/distributed.html#torch.distributed.init_process_group&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchset_default_device-and-torchdevice-as-context-manager&quot;&gt;[Beta] torch.set_default_device and torch.device as context manager&lt;/h3&gt;

&lt;p&gt;torch.set_default_device allows users to change the default device that factory functions in PyTorch allocate on. For example, if you torch.set_default_device(‘cuda’), a call to torch.empty(2) will allocate on CUDA (rather than on CPU). You can also use torch.device as a context manager to change the default device on a local basis. This resolves a long standing feature request from PyTorch’s initial release for a way to do this.&lt;/p&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-x86-as-the-new-default-quantization-backend-for-x86-cpu&quot;&gt;[Beta] “X86” as the new default quantization backend for x86 CPU&lt;/h3&gt;

&lt;p&gt;The new X86 quantization backend, which utilizes FBGEMM and oneDNN kernel libraries, replaces FBGEMM as the default quantization backend for x86 CPU platforms and offers improved int8 inference performance compared to the original FBGEMM backend, leveraging the strengths of both libraries, with 1.3X – 2X inference performance speedup measured on 40+ deep learning models. The new backend is functionally compatible with the original FBGEMM backend.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table: Geomean Speedup of X86 Quantization Backend vs. FBGEMM Backend&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt; 
   &lt;/td&gt;
   &lt;td&gt;1 core/instance
   &lt;/td&gt;
   &lt;td&gt;2 cores/instance
   &lt;/td&gt;
   &lt;td&gt;4 cores/instance
   &lt;/td&gt;
   &lt;td&gt;1 socket (32 cores)/instance
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz
   &lt;/td&gt;
   &lt;td&gt;1.76X
   &lt;/td&gt;
   &lt;td&gt;1.80X
   &lt;/td&gt;
   &lt;td&gt;2.04X
   &lt;/td&gt;
   &lt;td&gt;1.34X
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;By default, users on x86 platforms will utilize the x86 quantization backend and their PyTorch programs will remain unchanged when using the default backend. Alternatively, users have the option to specify “X86” as the quantization backend explicitly. Example code is shown below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.ao.quantization import get_default_qconfig_mappingfrom torch.quantization.quantize_fx
import prepare_fx, convert_fx
 
# get default configuration
qconfig_mapping = get_default_qconfig_mapping()
 
# or explicitly specify the backend
# qengine = 'x86'
# torch.backends.quantized.engine = qengine
# qconfig_mapping = get_default_qconfig_mapping(qengine)
 
# construct fp32 model
model_fp32 = ...
 
# prepare
prepared_model = prepare_fx(model_fp32, qconfig_mapping, example_inputs=x)
 
# calibrate
...
 
# convert
quantized_model = convert_fx(prepared_model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Find more information: &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/83888&quot;&gt;https://github.com/pytorch/pytorch/issues/83888&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html&quot;&gt;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-gnn-inference-and-training-optimization-on-cpu&quot;&gt;[Beta] GNN inference and training optimization on CPU&lt;/h3&gt;

&lt;p&gt;PyTorch 2.0 includes several critical optimizations to improve GNN inference and training performance on CPU. Before 2.0, GNN models of PyG suffers from low efficiency on CPU due to lack of performance tuning for several critical kernels (scatter/gather, etc) and the lack of GNN-related sparse matrix multiplication ops. To be specific, optimizations include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;scatter_reduce: performance hotspot in Message Passing when the edge index is stored in Coordinate format (COO).&lt;/li&gt;
  &lt;li&gt;gather: backward of scatter_reduce, specially tuned for the GNN compute when the index is an expanded tensor.&lt;/li&gt;
  &lt;li&gt;torch.sparse.mm with reduce flag: performance hotspot in Message Passing when the edge index is stored in Compressed Sparse Row (CSR). Supported reduce flag of: sum, mean, amax, amin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On PyG benchmarks/examples, OGB benchmarks, a 1.12x - 4.07x performance speedup is measured (1.13.1 compared with 2.0) for single node inference and training.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Model-Dataset&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Speedup Ratio&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;5&quot;&gt;GCN-Reddit (inference)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.25x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.31x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.68x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt; 
GraphSage-ogbn-products (inference)
   &lt;/td&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.15x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.33x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;full-batch-sparse
   &lt;/td&gt;
   &lt;td&gt;4.07x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-PROTEINS (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-REDDIT-BINARY (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;GCN-Reddit (training)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.12x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Learn more: &lt;a href=&quot;https://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;PyG CPU Performance Optimization&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-accelerating-inference-on-cpu-with-pytorch-by-leveraging-onednn-graph&quot;&gt;[Beta] Accelerating inference on CPU with PyTorch by leveraging oneDNN Graph&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;https://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on AI hardware.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It automatically identifies the graph partitions to be accelerated via fusion.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul and their neighbor operations for both inference and training use cases.&lt;/li&gt;
  &lt;li&gt;Although work is ongoing to integrate oneDNN Graph with TorchDynamo as well, its integration with the PyTorch JIT Fuser attained beta status in PyTorch 2.0 for &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-float&quot;&gt;Float32&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16&lt;/a&gt; inference (on machines that support AVX512_BF16 ISA).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From a developer’s/researcher’s perspective, the usage is quite simple &amp;amp; intuitive, with the only change in code being an API invocation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Leverage oneDNN Graph, with &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.jit.trace.html&quot;&gt;JIT-tracing&lt;/a&gt;, a model is profiled with an example input.&lt;/li&gt;
  &lt;li&gt;The context manager &lt;em&gt;with torch.jit.fuser(“fuser3”):&lt;/em&gt; can also be used instead of invoking &lt;em&gt;torch.jit.enable_onednn_fusion(True)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;For accelerating &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16 inference&lt;/a&gt;, we rely on eager-mode AMP (Automatic Mixed Precision) support in PyTorch &amp;amp; disable JIT mode’s AMP, as both of them are currently divergent:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Assuming we have a model of the name 'model'
 
example_input = torch.rand(1, 3, 224, 224)
 
# enable oneDNN Graph
torch.jit.enable_onednn_fusion(True)
# Disable AMP for JIT
torch._C._jit_set_autocast_mode(False)
with torch.no_grad(), torch.cpu.amp.autocast():
	model = torch.jit.trace(model, (example_input))
	model = torch.jit.freeze(model)
 	# 2 warm-ups (2 for tracing/scripting with an example, 3 without an example)
	model(example_input)
	model(example_input)
 
	# speedup would be observed in subsequent runs.
	model(example_input)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;distributed-api&quot;&gt;Distributed API&lt;/h3&gt;

&lt;h4 id=&quot;prototype-dtensor&quot;&gt;[Prototype] DTensor&lt;/h4&gt;

&lt;p&gt;PyTorch &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/distributed/_tensor/README.md&quot;&gt;DistributedTensor&lt;/a&gt; (DTensor) is a prototyping effort with distributed tensor primitives to allow easier distributed computation authoring in the SPMD (Single Program Multiple Devices) paradigm. The primitives are simple but powerful when used to express tensor distributions with both sharded and replicated parallelism strategies. PyTorch DTensor empowered PyTorch &lt;a href=&quot;https://pytorch.org/docs/master/distributed.tensor.parallel.html&quot;&gt;Tensor Parallelism&lt;/a&gt; along with other advanced parallelism explorations. In addition, it also offers a uniform way to save/load state_dict for distributed checkpointing purposes, even when there’re complex tensor distribution strategies such as combining tensor parallelism with parameter sharding in FSDP. More details can be found in this &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/88838&quot;&gt;RFC&lt;/a&gt; and the &lt;a href=&quot;https://colab.research.google.com/drive/12Pl5fvh0eLPUrcVO7s6yY4n2_RZo8pLR#scrollTo=stYPKb9Beq4e&quot;&gt;DTensor examples notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-tensorparallel&quot;&gt;[Prototype] TensorParallel&lt;/h4&gt;

&lt;p&gt;We now support DTensor based Tensor Parallel which users can distribute their model parameters across different GPU devices. We also support Pairwise Parallel which shards two concatenated linear layers in a col-wise and row-wise style separately so that only one collective(all-reduce/reduce-scatter) is needed in the end. More details can be found in this &lt;a href=&quot;https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/example.py&quot;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-2d-parallel&quot;&gt;[Prototype] 2D Parallel&lt;/h4&gt;

&lt;p&gt;We implemented the integration of the aforementioned TP with FullyShardedDataParallel(FSDP) as 2D parallel to further scale large model training. More details can be found in this &lt;a href=&quot;https://docs.google.com/presentation/d/17g6WqrO00rP3MsxbRENsPpjrlSkwiA_QB4r93_eB5is/edit?usp=sharing&quot;&gt;slide&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/test/distributed/tensor/parallel/test_2d_parallel.py&quot;&gt;code example&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-torchcompiledynamictrue&quot;&gt;[Prototype] torch.compile(dynamic=True)&lt;/h4&gt;

&lt;p&gt;Experimental support for PT2 compilation with dynamic shapes is available in this release. Inference compilation with inductor for simple models is supported, but there are a lot of limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training available in a future release (This is partially fixed in nightlies!)&lt;/li&gt;
  &lt;li&gt;Minifier available in a future release.&lt;/li&gt;
  &lt;li&gt;It is easy to end up in a situation where the dimension you wanted to be dynamic gets specialized anyway. Some of these issues are fixed in nightlies, others are not.&lt;/li&gt;
  &lt;li&gt;We do not appropriately propagate Inductor guards to the top-level, this is tracked at &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/96296&quot;&gt;#96296&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Data-dependent operations like nonzero still require a graph break.&lt;/li&gt;
  &lt;li&gt;Dynamic does not work with non-standard modes like reduce-overhead or max-autotune.&lt;/li&gt;
  &lt;li&gt;There are many bugs in Inductor compilation. To track known bugs, check the &lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+dynamic+shapes%22&quot;&gt;dynamic shapes&lt;/a&gt; label on the PyTorch issue tracker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the latest and greatest news about dynamic shapes support on master, check out &lt;a href=&quot;https://dev-discuss.pytorch.org/t/state-of-symbolic-shapes-branch/777/43&quot;&gt;our status reports&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;highlightsperformance-improvements&quot;&gt;Highlights/Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;&lt;a href=&quot;https://pytorch.org/blog/deprecation-cuda-python-support/&quot;&gt;Deprecation of Cuda 11.6 and Python 3.7 support&lt;/a&gt; for PyTorch 2.0&lt;/h3&gt;

&lt;p&gt;If you are still using or depending on CUDA 11.6 or Python 3.7 builds, we strongly recommend moving to at least CUDA 11.7 and Python 3.8, as it would be the minimum versions required for PyTorch 2.0. For more detail, please refer to the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/RELEASE.md#release-compatibility-matrix&quot;&gt;Release Compatibility Matrix for PyTorch&lt;/a&gt; releases.&lt;/p&gt;

&lt;h3 id=&quot;python-311-support-on-anaconda-platform&quot;&gt;Python 3.11 support on Anaconda Platform&lt;/h3&gt;

&lt;p&gt;Due to lack of Python 3.11 support for packages that PyTorch depends on, including NumPy, SciPy, SymPy, Pillow and others on the Anaconda platform. We will not be releasing Conda binaries compiled with Python 3.11 for PyTorch Release 2.0. The Pip packages with Python 3.11 support will be released, hence if you intend to use PyTorch 2.0 with Python 3.11 please use our Pip packages. Please note: Conda packages with Python 3.11 support will be made available on our nightly channel. Also we are planning on releasing Conda Python 3.11 binaries as part of future release once Anaconda provides these key dependencies. More information and instructions on how to download the Pip packages can be found &lt;a href=&quot;https://dev-discuss.pytorch.org/t/pytorch-2-0-message-concerning-python-3-11-support-on-anaconda-platform/1087&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;optimized-pytorch-inference-with-aws-graviton-processors&quot;&gt;Optimized PyTorch Inference with AWS Graviton processors&lt;/h3&gt;

&lt;p&gt;The optimizations focused on three key areas: GEMM kernels, bfloat16 support, primitive caching and the memory allocator. For aarch64 platforms, PyTorch supports Arm Compute Library (ACL) GEMM kernels via Mkldnn(OneDNN) backend. The ACL library provides Neon/SVE GEMM kernels for fp32 and bfloat16 formats. The bfloat16 support on c7g allows efficient deployment of bfloat16 trained, AMP (Automatic Mixed Precision) trained, or even the standard fp32 trained models. The standard fp32 models leverage bfloat16 kernels via OneDNN fast math mode, without any model quantization. Next we implemented primitive caching for conv, matmul and inner product operators. More information on the updated PyTorch user guide with the upcoming 2.0 release improvements and TorchBench benchmark details can be found &lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.0 which we highlighted during the PyTorch Conference on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Democratizing AI with PyTorch Foundation and ROCm™ support for PyTorch</title>
      <link href="https://pytorch.org/blog/democratizing-ai-with-pytorch/" rel="alternate" type="text/html" title="Democratizing AI with PyTorch Foundation and ROCm™ support for PyTorch" />
      <published>2023-02-14T00:00:00-08:00</published>
      <updated>2023-02-14T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/democratizing-ai-with-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/democratizing-ai-with-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/2023-02-14-democratizing-ai-with-pytorch-1.png&quot; alt=&quot;AMD Founding Member&quot; width=&quot;50%&quot; style=&quot;display:block; margin-left:auto; margin-right:auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Last year, Meta announced that &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; joined the Linux Foundation as a neutral home for growing the machine learning project and community with AMD representation as a part of the founding membership and governing board.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;PyTorch Foundation’s&lt;/a&gt; mission is to drive AI adoption by democratizing its software ecosystem through open source principles aligning with the AMD core principle of an Open software ecosystem. AMD strives to foster innovation through the support for latest generations of hardware, tools, libraries, and other components to simplify and accelerate adoption of AI across a broad range of scientific discoveries.&lt;/p&gt;

&lt;div class=&quot;d-md-flex&quot;&gt;
&lt;div style=&quot;flex-basis: 60%;&quot;&gt;
&lt;p&gt;
AMD, along with key PyTorch codebase developers (including those at Meta AI), delivered a set of updates to the &lt;a href=&quot;https://www.amd.com/en/graphics/servers-solutions-rocm&quot; target=&quot;_blank&quot;&gt;ROCm™&lt;/a&gt; open software ecosystem that brings stable support for &lt;a href=&quot;https://www.amd.com/en/graphics/instinct-server-accelerators&quot; target=&quot;_blank&quot;&gt;AMD Instinct™&lt;/a&gt; accelerators as well as many Radeon™ GPUs. This now gives PyTorch developers the ability to build their next great AI solutions leveraging AMD GPU accelerators &amp;amp; ROCm. The support from PyTorch community in identifying gaps, prioritizing key updates, providing feedback for performance optimizing and supporting our journey from “Beta” to “Stable” was immensely helpful and we deeply appreciate the strong collaboration between the two teams at AMD and PyTorch. The move for ROCm support from “Beta” to “Stable” came in the PyTorch 1.12 release (June 2022) brings the added support to easily run PyTorch on native environment without having to configure custom dockers. This is a sign of confidence about the quality of support and performance of PyTorch using AMD Instinct and ROCm. The results of these collaborative efforts are evident in the performance measured on key industry benchmarks like Microsoft’s SuperBench shown below in Graph 1.
&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&quot;
  border: 1px solid #d0d0d0;
  border-radius: 10px;
  box-sizing: border-box;
  -webkit-filter: drop-shadow(0 2px 5px rgba(0,0,0,.1));
  filter: drop-shadow(0 2px 5px rgba(0,0,0,.1));
  padding: 30px;
  background-color: #f8f8f8;
  margin: 20px;
  color: black;
  font-size: 1.6rem;
  flex-basis: 40%;
&quot;&gt;
&lt;p&gt;
&lt;em&gt;“We are excited to see the significant impact of developers at AMD to contribute to and extend features within PyTorch to make AI models run in a more performant, efficient, and scalable way. A great example of this is the thought-leadership around unified memory approaches between the framework and future hardware systems, and we look forward to seeing that feature progress.”&lt;/em&gt;&lt;br /&gt; 
- Soumith Chintala, PyTorch lead-maintainer and Director of Engineering, Meta AI
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The progressive improvements on both the AMD CDNA™ architecture as well as ROCm and PyTorch shows single GPU model throughput increase from AMD Instinct MI100 to the latest generation AMD Instinct MI200 family GPUs going from ROCm 4.2 to ROCm 5.3 and from PyTorch 1.7 to PyTorch 1.12.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-02-14-democratizing-ai-with-pytorch-2.png&quot; alt=&quot;Graph 1: ML model performance over generation using Microsoft Superbench Suite&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;Graph 1: ML model performance over generation using Microsoft Superbench Suite &lt;sup&gt;1, 2, 3&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Below are a few of the key updates for ROCm support since the PyTorch 1.12 release&lt;/p&gt;

&lt;h2 id=&quot;full-continuous-integration-ci-for-rocm-on-pytorch&quot;&gt;Full Continuous Integration (CI) for ROCm on PyTorch&lt;/h2&gt;

&lt;p&gt;With the ROCm support for PyTorch move from “Beta” to “Stable,” all the functions and features commits are now verified through a full Continuous Integration (CI) process. The CI process helps ensure the proper build and test process ahead of an expected Docker and PIP wheel release with stable commits forthcoming.&lt;/p&gt;

&lt;h2 id=&quot;support-for-kineto-profiler&quot;&gt;Support for &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto Profiler&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The addition of Kineto profiler support to ROCm now helps developers and users understand performance bottlenecks through effective diagnosis and profiling tools. The tool also provides recommendations to improve known issues and visualization through TensorBoard UI.&lt;/p&gt;

&lt;h2 id=&quot;key-pytorch-libraries-support-added&quot;&gt;Key PyTorch Libraries support added&lt;/h2&gt;

&lt;p&gt;PyTorch ecosystem libraries like &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;TorchText&lt;/a&gt; (Text classification), &lt;a href=&quot;https://pytorch.org/torchrec/&quot;&gt;TorchRec&lt;/a&gt; (libraries for recommender systems - RecSys), &lt;a href=&quot;https://pytorch.org/vision/stable/index.html&quot;&gt;TorchVision&lt;/a&gt; (Computer Vision), &lt;a href=&quot;https://pytorch.org/audio/stable/index.html&quot;&gt;TorchAudio&lt;/a&gt; (audio and signal processing) are fully supported since ROCm 5.1 and upstreamed with PyTorch 1.12.&lt;/p&gt;

&lt;p&gt;Key libraries provided with the ROCm software stack including &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/MIOpen&quot;&gt;MIOpen&lt;/a&gt; (Convolution models), &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/rccl&quot;&gt;RCCL&lt;/a&gt; (ROCm Collective Communications) and &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/rocBLAS&quot;&gt;rocBLAS&lt;/a&gt; (BLAS for transformers) were further optimized to offer new potential efficiencies and higher performance.&lt;/p&gt;

&lt;p&gt;MIOpen innovates on several fronts, such as implementing fusion to optimize for memory bandwidth and GPU launch overheads, providing an auto-tuning infrastructure to overcome the large design space of problem configurations, and implementing different algorithms to optimize convolutions for different filter and input sizes. MIOpen is one of the first libraries to publicly support the bfloat16 data-type for convolutions, allowing efficient training at lower precision maintaining expected accuracy.&lt;/p&gt;

&lt;p&gt;RCCL (pronounced “Rickle”) is a stand-alone library of standard collective communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, reduce-scatter, gather, scatter, and all-to-all. There is support for direct GPU-to-GPU send and receive operations. It has been optimized to achieve high bandwidth on platforms using PCIe®, Infinity Fabric™ (GPU to GPU) as well as networking using InfiniBand Verbs or TCP/IP sockets. RCCL supports an arbitrary number of GPUs installed in single or multiple nodes and can be used in either single- or multi-process (e.g., MPI) applications.&lt;/p&gt;

&lt;p&gt;Along with the above key highlights, over 50 features and functionality improvements were completed jointly between AMD and PyTorch to add stable support for ROCm. These include improvements to tools, compilers, runtime, graph optimizations through TorchScript, INT8 quant path usage, and &lt;a href=&quot;https://onnxruntime.ai/&quot;&gt;ONNX runtime integration&lt;/a&gt; including support for Navi 21 based Radeon™ PRO datacenter graphics card to name a few.&lt;/p&gt;

&lt;h2 id=&quot;aitemplate-inference-engine&quot;&gt;&lt;a href=&quot;https://github.com/facebookincubator/AITemplate&quot;&gt;AITemplate&lt;/a&gt; Inference Engine&lt;/h2&gt;

&lt;p&gt;MetaAI recently published a blog announcing the release of its open source AITemplate (&lt;a href=&quot;https://ai.facebook.com/blog/gpu-inference-engine-nvidia-amd-open-source/&quot;&gt;link&lt;/a&gt;) for a unified inference system supporting AMD Instinct GPU accelerators using the AMD ROCm stack. This Python based framework can help significantly improve performance through increased utilization of AMD matrix cores for transformer blocks. This is achieved through the AMD &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/composable_kernel&quot;&gt;Composable Kernel (CK) library&lt;/a&gt; which provides performance critical Kernels for ML AI workloads across multiple architectures including GPUs and CPUs through HIP &amp;amp; C++.&lt;/p&gt;

&lt;p&gt;Moreover, the AITemplate also provides out-of-the-box support for widely used AI models like BERT, ResNET, Vision Transformer, Stable Diffusion etc. simplifying deployment process through these pretrained models.&lt;/p&gt;

&lt;h2 id=&quot;whats-coming-with-future-rocm-releases&quot;&gt;What’s coming with future ROCm releases?&lt;/h2&gt;

&lt;h3 id=&quot;unified-memory-models-for-cpu--gpu&quot;&gt;Unified memory models for CPU + GPU&lt;/h3&gt;

&lt;p&gt;As system architecture evolves to address the complexity of large problem sizes and data sets, memory management becomes a key performance bottle neck that needs a cohesive strategy to be addressed through innovations at both hardware and software levels. AMD is uniquely positioned to address this problem with its effective data center solutions integrating AMD EPYC™ CPU cores with its AMD Instinct GPU compute units in a truly unified datacenter APU (Accelerated Processing Unit) form factor set to be launched in 2H 2023.&lt;/p&gt;

&lt;p&gt;The software work to leverage the unified CPU + GPU memory has already started in collaboration with the PyTorch team, to enable the usage of a fast, low latency, synchronized memory model that enables not only AMD but also other AI accelerators to address the complex memory management problem of today. We are looking forward to this joint effort and announcement soon.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The content in this blog highlights the joint work between AMD and key PyTorch contributors including Meta, working on many of the core features, as well as Microsoft enabling ONNX Runtime support. We are looking forward to working with the other founding members at the PyTorch Foundation on the next steps and improvements to democratize and grow adoption of PyTorch across the industry.&lt;/p&gt;

&lt;h2 id=&quot;cautionary-statement&quot;&gt;CAUTIONARY STATEMENT&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;
This blog contains forward-looking statements concerning Advanced Micro Devices, Inc. (AMD) such as the availability, timing and expected benefits of an AMD datacenter APU form factor, which are made pursuant to the Safe Harbor provisions of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are commonly identified by words such as “would,” “may,” “expects,” “believes,” “plans,” “intends,” “projects” and other terms with similar meaning. Investors are cautioned that the forward-looking statements in this blog are based on current beliefs, assumptions and expectations, speak only as of the date of this blog and involve risks and uncertainties that could cause actual results to differ materially from current expectations. Such statements are subject to certain known and unknown risks and uncertainties, many of which are difficult to predict and generally beyond AMD’s control, that could cause actual results and other future events to differ materially from those expressed in, or implied or projected by, the forward-looking information and statements. Investors are urged to review in detail the risks and uncertainties in AMD’s Securities and Exchange Commission filings, including but not limited to AMD’s most recent reports on Forms 10-K and 10-Q. AMD does not assume, and hereby disclaims, any obligation to update forward-looking statements made in this blog, except as may be required by law. 
&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;endnotes&quot;&gt;Endnotes&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;small&gt;MI100D-01 SuperBench v0.5 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™ 7763 CPU server tested with 1x AMD Instinct™ MI100 (32GB HBM2e) 300W GPU, SBIOS 2.2, Ubuntu® 20.04.5 LTS, host ROCm™ 5.2.0, guest ROCm 4.2,    PyTorch 1.7.0. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
  &lt;li&gt;&lt;small&gt;MI200D-01 SuperBench v0.6 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™ 7763 CPU server tested with 1x AMD Instinct™ MI210 (64GB HBM2e) 300W GPU, SBIOS 2.2, Ubuntu 20.04.5 LTS, host ROCm 5.3.0, guest ROCm 5.3, PyTorch 1.12. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
  &lt;li&gt;&lt;small&gt;MI200D-02: SuperBench v0.6 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™️ 7763 CPU server tested with 1x AMD Instinct™️ MI250 (128GB HBM2e) 560W GPU, SBIOS M12, Ubuntu 20.04 LTS, host ROCm 5.3.0, guest ROCm 5.3, PyTorch 1.12. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deprecation of CUDA 11.6 and Python 3.7 Support</title>
      <link href="https://pytorch.org/blog/deprecation-cuda-python-support/" rel="alternate" type="text/html" title="Deprecation of CUDA 11.6 and Python 3.7 Support" />
      <published>2023-02-02T00:00:00-08:00</published>
      <updated>2023-02-02T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/deprecation-cuda-python-support</id>
      <content type="html" xml:base="https://pytorch.org/blog/deprecation-cuda-python-support/">&lt;p&gt;For the upcoming PyTorch 2.0 feature release (target March 2023), we will target CUDA 11.7 as the stable version and CUDA 11.8 as the experimental version of CUDA and Python &amp;gt;=3.8, &amp;lt;=3.11.&lt;/p&gt;

&lt;p&gt;If you are still using or depending on CUDA 11.6 or Python 3.7 builds, we strongly recommend moving to at least CUDA 11.7 and Python 3.8, as it would be the minimum versions required for PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note that as of Feb 1, CUDA 11.6 and Python 3.7  are no longer included in the nightlies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please refer to the Release Compatibility Matrix for PyTorch releases:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch Version&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Python&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Stable CUDA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Experimental CUDA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2.0
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.8, &amp;lt;=3.11
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.7, CUDNN 8.5.0.96
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.8, CUDNN 8.7.0.84
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.13
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.7, &amp;lt;=3.10
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.6, CUDNN 8.3.2.44
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.7, CUDNN 8.5.0.96
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.12
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.7, &amp;lt;=3.10
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.3, CUDNN 8.3.2.44
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.6, CUDNN 8.3.2.44
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As of 2/1/2023&lt;/p&gt;

&lt;p&gt;For more information on PyTorch releases, updated compatibility matrix and release policies, please see (and bookmark) &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/RELEASE.md#release-compatibility-matrix&quot;&gt;Readme&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">For the upcoming PyTorch 2.0 feature release (target March 2023), we will target CUDA 11.7 as the stable version and CUDA 11.8 as the experimental version of CUDA and Python &amp;gt;=3.8, &amp;lt;=3.11.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Trace Analysis for the Masses</title>
      <link href="https://pytorch.org/blog/trace-analysis-for-masses/" rel="alternate" type="text/html" title="PyTorch Trace Analysis for the Masses" />
      <published>2023-01-09T00:00:00-08:00</published>
      <updated>2023-01-09T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/trace-analysis-for-masses</id>
      <content type="html" xml:base="https://pytorch.org/blog/trace-analysis-for-masses/">&lt;p&gt;We are excited to announce the public release of Holistic Trace Analysis (HTA), an open source performance analysis and visualization Python library for PyTorch users. HTA takes as input &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto traces&lt;/a&gt; collected by the &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/&quot;&gt;PyTorch profiler&lt;/a&gt;, which are complex and challenging to interpret, and up-levels the performance information contained in these traces. It was initially developed internally at Meta to understand and debug performance problems for large-scale distributed training jobs on GPUs. The multidisciplinary team has made a number of enhancements to HTA’s features and scaled them to support state-of-the-art ML workloads.&lt;/p&gt;

&lt;p&gt;ML researchers and systems engineers often struggle to computationally scale up their models because they are not aware of the performance bottlenecks in their workloads. The resources requested for a job (e.g. GPUs, memory) are often misaligned with the resources actually required due to lack of visibility “under the hood”. To achieve the best performance from the hardware stack, it is imperative to understand the resource utilization and bottlenecks for distributed training workloads.&lt;/p&gt;

&lt;p&gt;The initial HTA implementation was specifically targeted at Deep Learning Based Recommendation Models (DLRM). To make the features in HTA generic and applicable to use cases such as analyzing Vision and NLP models, we decided to refactor the HTA codebase and make the library available to the larger community. This new codebase has implemented several important ideas which lead to significant efficiency and performance improvements.&lt;/p&gt;

&lt;p&gt;In this blog, we present several features implemented in the open source version of HTA, which can be used as a Python script as well as interactively in a Jupyter notebook. HTA provides the following features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Breakdown by Dimensions&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Temporal&lt;/strong&gt;: Breakdown of GPU time in terms of time spent in computation, communication, memory events, and idle time on a single node and across all ranks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Idle Time&lt;/strong&gt;: Breakdown of GPU idle time into waiting for the host, waiting for another kernel or attributed to an unknown cause.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Kernel&lt;/strong&gt;: Find kernels with the longest duration on each rank.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Communication Computation Overlap&lt;/strong&gt;: Calculate the percentage of time when communication overlaps computation.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical Analysis&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Kernel Duration Distribution&lt;/strong&gt;: Distribution of average time taken by longest kernels across different ranks.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;CUDA Kernel Launch&lt;/strong&gt;: Distributions of GPU kernels with very small duration, large duration, and excessive launch time.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Augmented Counters (Memory bandwidth, Queue length)&lt;/strong&gt;: Augmented trace files which provide insights into memory copy bandwidth and number of outstanding operations on each CUDA stream.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Patterns&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Frequent CUDA Kernels&lt;/strong&gt;: Find the CUDA kernels most frequently launched by any given PyTorch or user defined operator.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Trace Comparison&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Trace Diff&lt;/strong&gt;: A trace comparison tool to identify and visualize the differences between traces.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;HTA source code is available to users via &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis&quot;&gt;Github&lt;/a&gt;. Users can request new features or build their own analysis using the core libraries and data structures provided in the codebase in addition to the features mentioned above.&lt;/p&gt;

&lt;h2 id=&quot;gpu-training-performance-debugging-101&quot;&gt;GPU Training Performance Debugging 101&lt;/h2&gt;

&lt;p&gt;To understand the GPU performance in distributed training jobs, we consider how the model operators interact with the GPU devices and how such interactions are reflected in certain measurable metrics.&lt;/p&gt;

&lt;p&gt;At a high level, we can break down the GPU operations in a model execution into three broad categories, henceforth referred to as kernel types:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Computation (COMP)&lt;/strong&gt; - Compute kernels execute compiled routines for matrix multiplication and similar numeric calculations. They are responsible for all of the number-crunching necessary for model execution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Communication (COMM)&lt;/strong&gt; - Communication kernels are routines which are responsible for exchanging and synchronizing data between different GPU devices in a distributed training job. The NVIDIA Collective Communication Library (NCCL) is a widely used communication library and all its kernels have the prefix “nccl”. Example NCCL kernels include NCCL_AllGather, NCCL_ReduceScatter, NCCL_AllReduce, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory (MEM)&lt;/strong&gt; - Memory kernels manage the memory allocations/deallocations on the GPU devices and data movement between the memory space on the host and the GPUs. The memory kernels include Memcpy_H2D, Memcpy_D2H, Memcpy_D2D, Memset, etc. Here, H represents the Host and D represents the GPU Device. Thus, H2D, D2H, D2D stands for Host to Device, Device to Host and Device to Device respectively.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Because a modern GPU device like the NVIDIA A100 GPU is a massively parallel device which is capable of running multiple kernels simultaneously, it is possible to overlap the computation, communication, and memory kernels to reduce the model execution time. One common technique to achieve the overlap is to utilize multiple CUDA streams. A CUDA stream is a sequence of operations that execute on a GPU device in the order in which they are issued by the host code. Different CUDA streams can be interleaved and even run concurrently, thus achieving the effect of kernel overlap.&lt;/p&gt;

&lt;p&gt;To help understand the above concepts, Figure 1 provides a timeline of the GPU kernels in a sample distributed training job on 8 GPUs for one iteration. In the figure below, each rank represents one GPU and the kernels on each GPU run on 6 CUDA streams. In the right column of the figure, you can see names of the GPU kernels used. In the middle of the figure, you see the overlap between compute and communicate kernels. This figure is created using the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/plot_timeline.ipynb&quot;&gt;plot_timeline example notebook&lt;/a&gt; available in HTA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image6.png&quot; alt=&quot;Figure 1. An example of the execution timeline of GPU Kernels across multiple ranks&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 1. An example of the execution timeline of GPU Kernels across multiple ranks&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The performance of multiple GPU training jobs is affected by multiple factors. Among these factors, how does a model execution create and orchestrate the GPU kernels plays a critical role. HTA provides insights on how the model execution interacts with the GPU devices and highlights the opportunities for performance improvement.&lt;/p&gt;

&lt;p&gt;With the features we built in HTA, we aim to provide users insights into “what is happening under the hood in a distributed GPU training?” We briefly describe these features in the next few paragraphs.&lt;/p&gt;

&lt;h2 id=&quot;features-in-holistic-trace-analysis&quot;&gt;Features in Holistic Trace Analysis&lt;/h2&gt;

&lt;p&gt;For most users, understanding the performance of GPU training jobs is nontrivial. Thus, we built this library to simplify the task of trace analysis and provide the user useful insights by examining the model execution traces. As the first step, we developed features which are important and generic enough so that most users can benefit from this library.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal Breakdown&lt;/strong&gt;: We begin by asking whether the GPU is spending time on computation, communication, memory events, or is it idle? To answer this question, the temporal breakdown feature presents a breakdown in terms of these categories. To achieve high training efficiency the code should maximize time used by computation kernels and minimize idle time and non-compute time (time used by communication or memory kernels). This is accomplished by implementing concurrent execution of computation kernels with communication or memory kernels. &lt;em&gt;Note that, during concurrent execution of computation kernels with communication/memory kernels the time spent by communication/memory kernels is accounted for under compute time.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image3.png&quot; alt=&quot;Figure 2: Temporal Breakdown across 8 GPUs&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 2: Temporal Breakdown across 8 GPUs&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel Breakdown&lt;/strong&gt;: It is natural to ask which kernels are taking the most amount of time. The next feature breaks down the time spent within each kernel type (COMM, COMP, MEM) and sorts them by duration. We present this information for each kernel type and for each rank as a pie chart. See figure 3 below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image1.png&quot; alt=&quot;Figure 3: Pie chart of top computation and communication kernels&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 3: Pie chart of top computation and communication kernels&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kernel Duration Distribution&lt;/strong&gt;: Subsequently, one can also ask - for any given kernel, what is the distribution of the time spent across the ranks? To answer this, HTA generates bar graphs for the average duration of a given kernel across all ranks. Additionally, the error bars in the bar graphs show the minimum and maximum amount of time taken by a given kernel on a given rank. Figure 4 below shows a discrepancy between average duration on rank 0 as compared to other ranks. This anomalous behavior on rank 0 guides the user on where to look for possible bugs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image4.png&quot; alt=&quot;Figure 4: Average duration of NCCL AllReduce Kernel across 8 ranks&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 4: Average duration of NCCL AllReduce Kernel across 8 ranks&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Communication Computation Overlap&lt;/strong&gt;: In distributed training, a significant amount of time is spent in communication and synchronization events among multiple GPU devices. To achieve high GPU efficiency (i.e. TFLOPS/GPU) it is vital to keep the GPU doing actual computation work. In other words, a GPU should not be blocked because of waiting for data from other GPUs. One way to measure the extent to which computation is blocked by data dependencies is to calculate the computation-communication overlap. Higher GPU efficiency is observed if communication events overlap computation events. Lack of communication and computation overlap will lead to the GPU being idle, thus the efficiency would be low. Thus, the communication computation overlap feature calculates the percentage of time communication and computation overlap in a job for each rank and generates a bar graph representation. See figure below. More precisely, we measure the following ratio&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;(time spent in computation while communicating) / (time spent in communication)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image5.png&quot; alt=&quot;Figure 5: Communication computation overlap&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 5: Communication computation overlap&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Augmented Counters (Queue length, Memory bandwidth)&lt;/strong&gt;: To aid in debugging, HTA calculates the memory bandwidth statistics for D2H, H2D and D2D memory copy (memcpy) and memory set (memset) events. Additionally, HTA also computes the number of outstanding CUDA operations on each CUDA stream. We refer to this as queue length. When the queue length on a stream is 1024 or larger new events cannot be scheduled on that stream and the CPU will stall until the GPU events have processed. Additionally, HTA generates a new trace file containing tracks with the memory bandwidth and queue length time series. See Figure 6 below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/trace-image2.png&quot; alt=&quot;Figure 6: Memory Bandwidth and Queue Length&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;em&gt;Figure 6: Memory Bandwidth and Queue Length&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These primary features give us a peek into the system performance and help answer “what is happening in the system?”. As HTA evolves, we hope to address “why is X happening?” and also suggest possible solutions to overcome the bottlenecks.&lt;/p&gt;

&lt;h2 id=&quot;installation-and-usage&quot;&gt;Installation and Usage&lt;/h2&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;p&gt;For installing the HTA please refer to the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/README.md&quot;&gt;README&lt;/a&gt;. In brief, the user is required to clone the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis&quot;&gt;repo&lt;/a&gt; and install the necessary Python packages via pip.&lt;/p&gt;

&lt;h3 id=&quot;usage&quot;&gt;Usage&lt;/h3&gt;

&lt;p&gt;This version of Holistic Trace Analysis is currently in beta and we recommend using HTA in a Jupyter notebook. A &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_analysis_demo.ipynb&quot;&gt;demo notebook&lt;/a&gt; is provided for your convenience. To get started, import the hta package in a Jupyter notebook, create a TraceAnalysis object and off we go in exactly two lines of code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hta.trace_analysis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TraceAnalysis&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TraceAnalysis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;All trace files for a training or inference job must be stored in a unique folder.&lt;/li&gt;
  &lt;li&gt;Trace files are in json or gzipped json format.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;faq&quot;&gt;FAQ&lt;/h2&gt;

&lt;h4 id=&quot;q-how-can-i-install-hta&quot;&gt;Q. How can I install HTA?&lt;/h4&gt;

&lt;p&gt;Please see the &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/README.md&quot;&gt;README&lt;/a&gt; in the root directory of the repository.&lt;/p&gt;

&lt;h4 id=&quot;q-is-there-any-documentation-on-the-features-and-api-in-hta&quot;&gt;Q. Is there any documentation on the features and API in HTA?&lt;/h4&gt;

&lt;p&gt;The documentation and detailed API is available &lt;a href=&quot;https://hta.readthedocs.io/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;q-can-you-implement-feature-x&quot;&gt;Q. Can you implement feature X?&lt;/h4&gt;

&lt;p&gt;Depending on how widely the feature is needed and the level of effort required to implement it we would consider developing the feature. Please open a &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/issues&quot;&gt;Github Issue&lt;/a&gt; and tag it with the feature-request label.&lt;/p&gt;

&lt;h4 id=&quot;q-can-i-modify-the-code&quot;&gt;Q. Can I modify the code?&lt;/h4&gt;

&lt;p&gt;Please do and &lt;a href=&quot;https://github.com/facebookresearch/HolisticTraceAnalysis/pulls&quot;&gt;send a PR&lt;/a&gt; along the way, if you think it would be useful for others.&lt;/p&gt;

&lt;h4 id=&quot;q-how-can-i-collect-traces-in-pytorch&quot;&gt;Q. How can I collect traces in PyTorch?&lt;/h4&gt;

&lt;p&gt;Please refer to this tutorial &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#use-profiler-to-record-execution-events&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;q-can-hta-be-used-at-production-scale&quot;&gt;Q. Can HTA be used at production scale?&lt;/h4&gt;

&lt;p&gt;Yes, please see a use case study &lt;a href=&quot;https://pytorch.org/blog/performance-debugging-of-production-pytorch-models-at-meta/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Anupam Bhatnagar, Xizhou Feng,  Brian Coutinho, Yifan Liu, Sung-Han Lin, Louis Feng, and Yuzhen Huang</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the public release of Holistic Trace Analysis (HTA), an open source performance analysis and visualization Python library for PyTorch users. HTA takes as input Kineto traces collected by the PyTorch profiler, which are complex and challenging to interpret, and up-levels the performance information contained in these traces. It was initially developed internally at Meta to understand and debug performance problems for large-scale distributed training jobs on GPUs. The multidisciplinary team has made a number of enhancements to HTA’s features and scaled them to support state-of-the-art ML workloads.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Compromised PyTorch-nightly dependency chain between December 25th and December 30th, 2022.</title>
      <link href="https://pytorch.org/blog/compromised-nightly-dependency/" rel="alternate" type="text/html" title="Compromised PyTorch-nightly dependency chain between December 25th and December 30th, 2022." />
      <published>2022-12-31T00:00:00-08:00</published>
      <updated>2022-12-31T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/compromised-nightly-dependency</id>
      <content type="html" xml:base="https://pytorch.org/blog/compromised-nightly-dependency/">&lt;p&gt;If you installed PyTorch-nightly on Linux via pip between December 25, 2022 and December 30, 2022, please uninstall it and torchtriton immediately, and use the latest nightly binaries (newer than Dec 30th 2022).&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;pip3 uninstall &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; torch torchvision torchaudio torchtriton
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;pip3 cache purge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch-nightly Linux packages installed via pip during that time installed a dependency, torchtriton, which was compromised on the Python Package Index (PyPI) code repository and ran a malicious binary. This is what is known as a supply chain attack and directly affects dependencies for packages that are hosted on public package indices.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Users of the PyTorch &lt;strong&gt;stable&lt;/strong&gt; packages &lt;strong&gt;are not&lt;/strong&gt; affected by this issue.&lt;/p&gt;

&lt;h2 id=&quot;how-to-check-if-your-python-environment-is-affected&quot;&gt;How to check if your Python environment is affected&lt;/h2&gt;

&lt;p&gt;The following command searches for the malicious binary in the torchtriton package (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PYTHON_SITE_PACKAGES/triton/runtime/triton&lt;/code&gt;) and prints out whether your current Python environment is affected or not.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;import pathlib;import importlib.util;s=importlib.util.find_spec('triton'); affected=any(x.name == 'triton' for x in (pathlib.Path(s.submodule_search_locations[0] if s is not None else '/' ) / 'runtime').glob('*'));print('You are {}affected'.format('' if affected else 'not '))&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The malicious binary is executed when the triton package is imported, which requires explicit code to do and is not PyTorch’s default behavior.&lt;/p&gt;

&lt;h2 id=&quot;the-background&quot;&gt;The Background&lt;/h2&gt;

&lt;p&gt;At around 4:40pm GMT on December 30 (Friday), we learned about a malicious dependency package (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchtriton&lt;/code&gt;) that was uploaded to the Python Package Index (PyPI) code repository with the same package name as the one we ship on the &lt;a href=&quot;https://download.pytorch.org/whl/nightly&quot;&gt;PyTorch nightly package index&lt;/a&gt;. Since the &lt;a href=&quot;https://github.com/pypa/pip/issues/8606&quot;&gt;PyPI index takes precedence&lt;/a&gt;, this malicious package was being installed instead of the version from our official repository. This design enables somebody to register a package by the same name as one that exists in a third party index, and pip will install their version by default.&lt;/p&gt;

&lt;p&gt;This malicious package has the same name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchtriton&lt;/code&gt; but added in code that uploads sensitive data from the machine.&lt;/p&gt;

&lt;h2 id=&quot;what-we-know&quot;&gt;What we know&lt;/h2&gt;

&lt;p&gt;torchtriton on PyPI contains a malicious triton binary which is installed at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PYTHON_SITE_PACKAGES/triton/runtime/triton&lt;/code&gt;. Its SHA256 hash is listed below.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHA256(triton)= 2385b29489cd9e35f92c072780f903ae2e517ed422eae67246ae50a5cc738a0e&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The binary’s main function does the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get system information
    &lt;ul&gt;
      &lt;li&gt;nameservers from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/resolv.conf&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;hostname from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gethostname()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;current username from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getlogin()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;current working directory name from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;getcwd()&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;environment variables&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Read the following files
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/hosts&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/passwd&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;The first 1,000 files in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/*&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/.gitconfig&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/.ssh/*&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Upload all of this information, including file contents, via encrypted DNS queries to the domain *.h4ck[.]cfd, using the DNS server wheezy[.]io&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The binary’s file upload functionality is limited to files less than 99,999 bytes in size. It also uploads only the first 1,000 files in $HOME (but all files &amp;lt; 99,999 bytes in the .ssh directory).&lt;/p&gt;

&lt;h2 id=&quot;steps-taken-towards-mitigation&quot;&gt;Steps taken towards mitigation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;torchtriton has been removed as a dependency for our nightly packages and replaced with pytorch-triton (&lt;a href=&quot;https://github.com/pytorch/pytorch/pull/91539&quot;&gt;pytorch/pytorch#91539&lt;/a&gt;) and a dummy package registered on PyPI (so that this issue doesn’t repeat)&lt;/li&gt;
  &lt;li&gt;All nightly packages that depend on torchtriton have been removed from our package indices at https://download.pytorch.org until further notice&lt;/li&gt;
  &lt;li&gt;We have reached out to the PyPI security team to get proper ownership of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchtriton&lt;/code&gt; package on PyPI and to delete the malicious version&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>The PyTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html">If you installed PyTorch-nightly on Linux via pip between December 25, 2022 and December 30, 2022, please uninstall it and torchtriton immediately, and use the latest nightly binaries (newer than Dec 30th 2022).</summary>
      

      
      
    </entry>
  
</feed>


