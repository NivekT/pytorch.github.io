<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2023-04-12T08:59:29-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Celebrate PyTorch 2.0 with New Performance Features for AI Developers</title>
      <link href="https://pytorch.org/blog/celebrate-pytorch-2.0/" rel="alternate" type="text/html" title="Celebrate PyTorch 2.0 with New Performance Features for AI Developers" />
      <published>2023-04-07T00:00:00-07:00</published>
      <updated>2023-04-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/celebrate-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/celebrate-pytorch-2.0/">&lt;p&gt;Congratulations to the PyTorch Foundation for its release of &lt;strong&gt;PyTorch 2.0&lt;/strong&gt;! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;TorchInductor&lt;/li&gt;
  &lt;li&gt;GNN&lt;/li&gt;
  &lt;li&gt;INT8 Inference Optimization&lt;/li&gt;
  &lt;li&gt;oneDNN Graph API&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We at Intel are delighted to be part of the PyTorch community and appreciate the collaboration with and feedback from our colleagues at &lt;a href=&quot;http://www.meta.com/&quot;&gt;Meta&lt;/a&gt; as we co-developed these features.&lt;/p&gt;

&lt;p&gt;Let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;1-torchinductor-cpu-fp32-inference-optimized&quot;&gt;1. TorchInductor CPU FP32 Inference Optimized&lt;/h2&gt;

&lt;p&gt;As part of the PyTorch 2.0 compilation stack, TorchInductor CPU backend optimization brings notable performance improvements via graph compilation over the PyTorch eager mode.&lt;/p&gt;

&lt;p&gt;The TorchInductor CPU backend is sped up by leveraging the technologies from the &lt;a href=&quot;http://github.com/intel/intel-extension-for-pytorch&quot;&gt;Intel® Extension for PyTorch&lt;/a&gt; for Conv/GEMM ops with post-op fusion and weight prepacking, and PyTorch ATen CPU kernels for memory-bound ops with explicit vectorization on top of OpenMP*-based thread parallelization.&lt;/p&gt;

&lt;p&gt;With these optimizations on top of the powerful loop fusions in TorchInductor codegen, we achieved up to a &lt;strong&gt;1.7x&lt;/strong&gt; FP32 inference performance boost over three representative deep learning benchmarks: TorchBench, HuggingFace, and timm1. Training and low-precision support are under development.&lt;/p&gt;

&lt;h3 id=&quot;see-the-improvements&quot;&gt;See the Improvements&lt;/h3&gt;

&lt;p&gt;The performance improvements on various backends are tracked on this &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890&quot;&gt;TouchInductor CPU Performance Dashboard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;improve-graph-neural-network-gnn-in-pyg-for-inference-and-training-performance-on-cpu&quot;&gt;Improve Graph Neural Network (GNN) in PyG for Inference and Training Performance on CPU&lt;/h2&gt;

&lt;p&gt;GNN is a powerful tool to analyze graph structure data. This feature is designed to improve GNN inference and training performance on Intel® CPUs, including the new 4th Gen Intel® Xeon® Scalable processors.&lt;/p&gt;

&lt;p&gt;PyTorch Geometric (PyG) is a very popular library built upon PyTorch to perform GNN workflows. Currently on CPU, GNN models of PyG run slowly due to the lack of GNN-related sparse matrix multiplication operations (i.e., SpMM_reduce) and the lack of several critical kernel-level optimizations (scatter/gather, etc.) tuned for GNN compute.&lt;/p&gt;

&lt;p&gt;To address this, optimizations are provided for message passing between adjacent neural network nodes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;scatter_reduce:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in coordinate format (COO).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;gather:&lt;/strong&gt; backward computation of scatter_reduce, specially tuned for the GNN compute when the index is an expanded tensor.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;torch.sparse.mm with reduce flag:&lt;/strong&gt; performance hotspot in message-passing when the edge index is stored in compressed sparse row (CSR). Supported reduce flag for: sum, mean, amax, amin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;End-to-end performance benchmark results for both inference and training on 3rd Gen Intel® Xeon® Scalable processors 8380 platform and on 4th Gen 8480+ platform are discussed in &lt;a href=&quot;http://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;Accelerating PyG on Intel CPUs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;optimize-int8-inference-with-unified-quantization-backend-for-x86-cpu-platforms&quot;&gt;Optimize int8 Inference with Unified Quantization Backend for x86 CPU Platforms&lt;/h2&gt;

&lt;p&gt;The new X86 quantization backend is a combination of &lt;a href=&quot;http://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt; (Facebook General Matrix-Matrix Multiplication) and &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneAPI Deep Neural Network Library (oneDNN&lt;/a&gt;) backends and replaces FBGEMM as the default quantization backend for x86 platforms. The result: better end-to-end int8 inference performance than FBGEMM.&lt;/p&gt;

&lt;p&gt;Users access the x86 quantization backend by default for x86 platforms, and the selection between different kernels is automatically done behind the scenes. The rules of selection are based on prior performance testing data done by Intel during feature development. Thus, the x86 backend replaces FBGEMM and may offer better performance, depending on the use case.&lt;/p&gt;

&lt;p&gt;The selection rules are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On platforms without VNNI (e.g., Intel® Core™ i7 processors), FBGEMM is always used.&lt;/li&gt;
  &lt;li&gt;On platforms with VNNI (e.g., 2nd-4th Gen Intel® Xeon® Scalable processors and future platforms):
    &lt;ul&gt;
      &lt;li&gt;For linear, FBGEMM is always used.&lt;/li&gt;
      &lt;li&gt;For convolution layers, FBGEMM is used for depth-wise convolution whose layers &amp;gt; 100; otherwise, oneDNN is used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that as the kernels continue to evolve.&lt;/p&gt;

&lt;p&gt;The selection rules above are subject to change to achieve better performance. Performance metrics for through-put speed-up ratios of unified x86 backend vs. pure FBGEMM are discussed in &lt;a href=&quot;http://github.com/pytorch/pytorch/issues/83888&quot;&gt;[RFC] Unified quantization backend for x86 CPU platforms #83888&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;leverage-onednn-graph-api-to-accelerate-inference-on-cpu&quot;&gt;Leverage oneDNN Graph API to Accelerate Inference on CPU&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;http://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on Intel® AI hardware. It automatically identifies the graph partitions to be accelerated via fusion. The &lt;a href=&quot;http://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul, and their neighbor operations for both inference and training use cases.&lt;/p&gt;

&lt;p&gt;Currently, BFloat16 and Float32 datatypes are supported and only inference workloads can be optimized.  BF16 is only optimized on machines with Intel® Advanced Vector Extensions 512 (Intel® AVX-512) BF16 support.&lt;/p&gt;

&lt;p&gt;Few or no modifications are needed in PyTorch to support newer oneDNN Graph fusions/optimized kernels. To use oneDNN Graph, users can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Either use the API &lt;em&gt;torch.jit.enable_onednn_fusion(True)&lt;/em&gt; before JIT tracing a model, OR …&lt;/li&gt;
  &lt;li&gt;Use its context manager, viz. &lt;em&gt;with torch.jit.fuser(“fuser3”).&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;For accelerating &lt;a href=&quot;http://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16 inference&lt;/a&gt;, we rely on eager-mode AMP (Automatic Mixed Precision) support in PyTorch and disable JIT mode’s AMP.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the &lt;a href=&quot;http://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;PyTorch performance tuning guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;h3 id=&quot;get-the-software&quot;&gt;Get the Software&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://pytorch.org/get-started/locally/&quot;&gt;Try out PyTorch 2.0&lt;/a&gt; and realize the performance benefits for yourself from these Intel-contributed features.&lt;/p&gt;

&lt;p&gt;We encourage you to check out Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;AI Tools&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;Framework&lt;/a&gt; optimizations and learn about the open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.&lt;/p&gt;

&lt;p&gt;For more details about 4th Gen Intel Xeon Scalable processor, visit &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;AI Platform&lt;/a&gt; where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.&lt;/p&gt;

&lt;h3 id=&quot;pytorch-resources&quot;&gt;PyTorch Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch Get Started&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077&quot;&gt;Dev Discussions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://pytorch.org/docs/2.0/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Congratulations to the PyTorch Foundation for its release of PyTorch 2.0! In this blog, I discuss the four features for which Intel made significant contributions to PyTorch 2.0:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Straggler Mitigation On PyTorch DDP By Hierarchical SGD</title>
      <link href="https://pytorch.org/blog/straggler-mitigation/" rel="alternate" type="text/html" title="Straggler Mitigation On PyTorch DDP By Hierarchical SGD" />
      <published>2023-04-07T00:00:00-07:00</published>
      <updated>2023-04-07T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/straggler-mitigation</id>
      <content type="html" xml:base="https://pytorch.org/blog/straggler-mitigation/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/ddp.html&quot;&gt;PyTorch DDP&lt;/a&gt; has been widely adopted across the industry for distributed training, which by default runs synchronous SGD to synchronize gradients across model replicas at every step. The performance of this technique is critical for fast iteration during model exploration as well as resource and cost saving. The performance is critical for fast iteration and cost saving of model development and exploration. To resolve a ubiquitous performance bottleneck introduced by slow nodes in large-scale training, Cruise and Meta co-developed a solution based on the &lt;a href=&quot;https://arxiv.org/abs/2007.13819&quot;&gt;Hierarchical SGD&lt;/a&gt; algorithm to significantly accelerate training in the presence of these stragglers.&lt;/p&gt;

&lt;h2 id=&quot;the-need-for-straggler-mitigation&quot;&gt;The Need For Straggler Mitigation&lt;/h2&gt;

&lt;p&gt;In DDP setup, a straggler problem can occur when one or more processes run much slower (“stragglers”) than other processes. When this happens, all the processes have to wait for the stragglers before synchronizing gradients and completing the communication, which essentially bottlenecks distributed performance to the slowest worker.As a result, even for the cases of training relatively small models, the communication cost can still be a major performance bottleneck.&lt;/p&gt;

&lt;h3 id=&quot;potential-causes-of-stragglers&quot;&gt;Potential Causes of Stragglers&lt;/h3&gt;

&lt;p&gt;Severe straggler issues are usually caused by workload imbalance before synchronization, and many factors can contribute to this imbalance. For instance, some data loader workers in the distributed environment can become stragglers, because some input examples can be outliers in terms of the data size, or the data transfer of some examples can be drastically slowed down due to unstable network I/O, or the on-the-fly data transformation costs can have a high variance.&lt;/p&gt;

&lt;p&gt;Besides data loading, other phases before gradient synchronization can also cause stragglers, such as unbalanced workloads of embedding table lookup during the forward pass in recommendation systems.&lt;/p&gt;

&lt;h3 id=&quot;the-appearance-of-stragglers&quot;&gt;The Appearance of Stragglers&lt;/h3&gt;

&lt;p&gt;If we profile DDP training jobs that have stragglers, we can find that some processes may have much higher gradient synchronization costs (a.k.a., allreducing gradients) than other processes at a certain step. As a result, the distributed performance can be dominated by the communication cost even if the model size is very small. In this case, some processes run faster than the straggler(s) at a step, and hence they have to wait for the stragglers and spend a much longer time on allreduce.&lt;/p&gt;

&lt;p&gt;The below shows screenshots of two trace files output by PyTorch profiler in a use case. Each screenshot profiles 3 steps.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first screenshot shows that a process has a very high allreduce cost in both the first and the third steps, because this process reaches the synchronization phase earlier than the straggler(s), and it spends more time on waiting. On the other hand, the allreduce cost is relatively small in the second step, this suggests that 1) there is no straggler at this step; or 2) this process is the straggler among all the processes, so it does not need to wait for any other process.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-1.png&quot; alt=&quot;chart showing allreduce cost&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Both the 1st and the 3rd Steps Are Slowed Down by Stragglers&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The second screenshot shows a normal case without stragglers. In this case, all the gradient synchronizations are relatively short.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-2.png&quot; alt=&quot;chart showing normal case without stragglers&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Normal Case Without Stragglers&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;hierarchical-sgd-in-pytorch&quot;&gt;Hierarchical SGD in PyTorch&lt;/h2&gt;

&lt;p&gt;Recently hierarchical SGD has been proposed to optimize the communication costs by mainly reducing the total amount of data transfer in large-scale distributed training, and multiple convergence analyses have been provided (&lt;a href=&quot;https://arxiv.org/pdf/2010.12998.pdf&quot;&gt;example&lt;/a&gt;). As a main novelty of this post, at Cruise we could leverage hierarchical SGD to mitigate stragglers, which may also occur on training relatively small models. Our implementation has been upstreamed by Cruise to PyTorch in early 2022.&lt;/p&gt;

&lt;h3 id=&quot;how-does-hierarchical-sgd-work&quot;&gt;How Does Hierarchical SGD Work?&lt;/h3&gt;

&lt;p&gt;As the name implies, hierarchical SGD organizes all the processes into groups at different levels as a hierarchy, and runs synchronization by following the rules below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the groups at the same level have the same number of processes, and the processes in these groups synchronize at the same frequency concurrently, where the synchronization period is pre-defined by the user.&lt;/li&gt;
  &lt;li&gt;The higher level a group is, the larger synchronization period is used, as the synchronization becomes more expensive.&lt;/li&gt;
  &lt;li&gt;When multiple overlapping groups are supposed to synchronize according to their periods, to reduce redundant synchronization and avoid data race across groups, only the highest-level group runs synchronization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following figure illustrates an example of 4-level hierarchy SGD among 16 processes on 8 machines, each of which has 2 GPUs:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Level 1:&lt;/strong&gt; Each process runs mini-batch SGD locally;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 2:&lt;/strong&gt; Each 4-process group across 2 machines runs synchronization every 2 steps;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 3:&lt;/strong&gt; Each 8-process group across 4 machines runs synchronization every 4 steps;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Level 4:&lt;/strong&gt; The global process group of all 16 processes over 8 machines runs synchronization every 8 steps.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Particularly, when the step number can be divided by 8, only the synchronization at 3) is executed, and when the step number can be divided by 4 but not 8, only the synchronization at 2) is executed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-3.png&quot; alt=&quot;An example of 4-level hierarchy SGD among 16 processes on 8 machines, each of which has 2 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Intuitively, hierarchical SGD can be viewed as an extension of &lt;a href=&quot;https://core.ac.uk/download/pdf/211998087.pdf&quot;&gt;local SGD&lt;/a&gt;, which only has a two-level hierarchy – every process runs mini-batch SGD locally and then synchronizes globally at a certain frequency. This can also help explain that, just like local SGD, hierarchical SGD synchronizes model parameters instead of gradients. Otherwise the gradient descent will be mathematically incorrect when the frequency is greater than 1.&lt;/p&gt;

&lt;h3 id=&quot;why-can-hierarchical-sgd-mitigate-stragglers&quot;&gt;Why Can Hierarchical SGD Mitigate Stragglers?&lt;/h3&gt;

&lt;p&gt;The key insight here is that, when there is a random straggler, it only directly slows down a relatively small group of processes instead of all the processes. Next time another random straggler is very likely to slow down a different small group, and hence a hierarchy can help smooth out the straggler effect.&lt;/p&gt;

&lt;p&gt;The example below assumes that there is a random straggler among totally 8 processes at every step. After 4 steps, vanilla DDP that runs synchronous SGD will be slowed down by straggler 4 times, because it runs global synchronization at every step. In contrast, hierarchical SGD runs synchronization with the groups of 4 processes after the first two steps, and then a global synchronization after another two steps. We can see that both the first two and the last two stragglers have a large overlap, and hence the performance loss can be mitigated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-4.png&quot; alt=&quot;flow diagram&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Essentially, the mitigation effect of this hierarchical SGD example actually is between local SGD at a frequency of every 2 steps and every 4 steps. The main advantage of hierarchical SGD over local SGD is a better convergence efficiency of the same global synchronization frequency, because hierarchical SGD allows more low-level synchronization. Moreover, it is possible for hierarchical SGD to provide a global synchronization frequency lower than local SGD with model parity, leading to a higher training performance, especially in a large-scale distributed training.&lt;/p&gt;

&lt;h3 id=&quot;ease-of-use&quot;&gt;Ease of Use&lt;/h3&gt;

&lt;p&gt;Straggler mitigation is not a novel study in distributed training. Multiple approaches have been proposed, such as &lt;a href=&quot;https://arxiv.org/pdf/1705.09056.pdf&quot;&gt;gossip SGD&lt;/a&gt;, &lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf&quot;&gt;data encoding&lt;/a&gt;, &lt;a href=&quot;http://proceedings.mlr.press/v70/tandon17a/tandon17a.pdf&quot;&gt;gradient coding&lt;/a&gt;, as well as some particularly designed for parameter-server architecture, including &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45187.pdf&quot;&gt;backup workers&lt;/a&gt; and &lt;a href=&quot;http://www.cs.cmu.edu/~seunghak/SSPTable_NIPS2013.pdf&quot;&gt;stale synchronous parallel&lt;/a&gt;. However, to the best of our knowledge, before this effort we have not found a good open-source PyTorch implementation of straggler mitigation that can work like a plugin to our training system at Cruise. In contrast, our implementation only requires the minimal changes – no need to modify the existing code or tune any existing hyperparameters. This is a very appealing advantage for industry users.&lt;/p&gt;

&lt;p&gt;As the code example below shows, only a few lines need to be added to the setup of DDP model, and the training loop code can keep untouched. As explained previously, hierarchical SGD is an extended form of local SGD, so the enablement can be quite similar to local SGD (see PyTorch docs of &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.PostLocalSGDOptimizer&quot;&gt;PostLocalSGDOptimizer&lt;/a&gt;):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Register a post-local SGD communication hook to run a warmup stage of fully synchronous SGD and defer hierarchical SGD.&lt;/li&gt;
  &lt;li&gt;Create a post-local SGD optimizer that wraps an existing local optimizer and a hierarchical SGD configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch.distributed.algorithms.model_averaging.hierarchical_model_averager as hierarchicalSGD
from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (
    PostLocalSGDState,
    post_localSGD_hook,
)
from torch.distributed.optim import PostLocalSGDOptimizer

ddp_model = nn.parallel.DistributedDataParallel(
    module=model,
    device_ids=[rank],
)

# Register a post-local SGD communication hook for the warmup.
subgroup, _ = torch.distributed.new_subgroups()
state = PostLocalSGDState(subgroup=subgroup, start_localSGD_iter=1_000)
ddp_model.register_comm_hook(state, post_localSGD_hook)

# Wraps the existing (local) optimizer to run hierarchical model averaging.
optim = PostLocalSGDOptimizer(
  optim=optim,
  averager=hierarchicalSGD.HierarchicalModelAverager(
    # The config runs a 4-level hierarchy SGD among 128 processes:
    # 1) Each process runs mini-batch SGD locally;
    # 2) Each 8-process group synchronize every 2 steps;
    # 3) Each 32-process group synchronize every 4 steps;
    # 4) All 128 processes synchronize every 8 steps.
    period_group_size_dict=OrderedDict([(2, 8), (4, 32), (8, 128)]),
    # Do not run hierarchical SGD until 1K steps for model parity.
    warmup_steps=1_000)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;algorithm-hyperparameters&quot;&gt;Algorithm Hyperparameters&lt;/h3&gt;

&lt;p&gt;Hierarchical SGD has two major hyperparameters: &lt;em&gt;period_group_size_dict&lt;/em&gt; and &lt;em&gt;warmup_steps&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;period_group_size_dict&lt;/strong&gt; is an ordered dictionary mapping from synchronization period to process group size, used for initializing process groups of different sizes in a hierarchy to synchronize parameters concurrently. A larger group is expected to use a larger synchronization period.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;warmup_steps&lt;/strong&gt; specifies a number of steps as the warmup stage to run synchronous SGD before hierarchical SGD. Similar to &lt;a href=&quot;https://arxiv.org/pdf/1808.07217.pdf&quot;&gt;post-local SGD&lt;/a&gt; algorithm, a warmup stage is usually recommended to achieve a higher accuracy. The value should be the same as &lt;em&gt;start_localSGD_iter&lt;/em&gt; arg used in &lt;em&gt;PostLocalSGDState&lt;/em&gt; when post_localSGD_hook is registered. Typically the warmup stage should at least cover the beginning of training when the loss is decreased drastically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A subtle difference between the PyTorch implementation and the initial design proposed by relevant papers is that, after the warmup stage, by default the processes within each host still run intra-host gradient synchronization at every step. This is because that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The intra-host communication is relatively cheap, and it can usually significantly accelerate the convergence;&lt;/li&gt;
  &lt;li&gt;The intra-host group (of size 4 or 8 for most industry users) can usually be a good choice of the smallest group of processes that synchronize most frequently in hierarchical SGD. If the synchronization period is 1, then gradient synchronization is faster than model parameter synchronization (a.k.a., model averaging), because DDP automatically overlaps gradient synchronization and the backward pass.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Such intra-host gradient synchronization can be disabled by unsetting &lt;em&gt;post_local_gradient_allreduce&lt;/em&gt; arg in &lt;em&gt;PostLocalSGDState&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;demonstration&quot;&gt;Demonstration&lt;/h2&gt;

&lt;p&gt;Now we demonstrate that hierarchical SGD can accelerate distributed training by mitigating stragglers.&lt;/p&gt;

&lt;h3 id=&quot;experimental-setup&quot;&gt;Experimental Setup&lt;/h3&gt;

&lt;p&gt;We compared the performance of hierarchical SGD against local SGD and synchronous SGD on &lt;a href=&quot;https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html&quot;&gt;ResNet18&lt;/a&gt; (model size: 45MB). Since the model is so small, the training is not bottlenecked by data transfer cost during synchronization. To avoid the noises incurred by data loading from remote storage, the input data was randomly simulated from memory. We varied the number of GPUs used by training from 64 to 256. The batch size per worker is 32, and the number of iterations of training is 1,000. Since we don’t evaluate convergence efficiency in this set of experiments, warmup is not enabled.&lt;/p&gt;

&lt;p&gt;We also emulated stragglers at a rate of 1% on 128 and 256 GPUs, and 2% on 64 GPUs, to make sure at least one stragglers at every step on average. These stragglers randomly appear on different CUDA devices. Each straggler stalls for 1 second besides the normal per-step training time (~55ms in our setup). This can be perceived as a practical scenario where 1% or 2% of input data are outliers in terms of the data pre-processing cost (I/O and/or data transformation on the fly) during training, and such cost is 20X+ larger than the average.&lt;/p&gt;

&lt;p&gt;The code snippet below shows how a straggler can be emulated in the training loop. We applied it to a ResNet model, and it can be easily applied to the other models as well.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     loss = loss_fn(y_pred, y)
     # Emulate a straggler that lags for 1 second at a rate of 1%.
     if random.randint(1, 100) == 1:
         time.sleep(1)
     loss.backward()
     optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The experiments are conducted on us-central1 GCP cluster. Each machine has 4 NVIDIA Tesla T4 GPUs with 16 GB memory per GPU, connected through a 32 Gbit/s ethernet network. Each instance also features 96 vCPUs, 360 GB RAM.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot; style=&quot;max-width: 450px;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Architecture
   &lt;/td&gt;
   &lt;td&gt;ResNet18 (45MB)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Workers
   &lt;/td&gt;
   &lt;td&gt;64, 128, 256
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Backend
   &lt;/td&gt;
   &lt;td&gt;NCCL
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU
   &lt;/td&gt;
   &lt;td&gt;Tesla T4, 16 GB memory
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Batch size
   &lt;/td&gt;
   &lt;td&gt;32 x ## of workers
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Straggler Duration
   &lt;/td&gt;
   &lt;td&gt;1 sec
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Straggler Rate
   &lt;/td&gt;
   &lt;td&gt;1% on 128 and 256 GPUs, 2% on 64 GPUs
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We used multiple configurations for both local SGD and hierarchical SGD. Local SGD runs global synchronization every 2, 4, and 8 steps, respectively.&lt;/p&gt;

&lt;p&gt;We ran hierarchical SGD with the following configurations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;On 64 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 8-process group, 32-process, and the global 64-process group synchronizes every 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-32,8-64&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 32-process group and the global 64-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-32,8-64&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;On 128 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 8-process group, 32-process group, and the global 128-process group synchronizes every 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-32,8-128&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 32-process group and the global 128-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-32,8-128&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;On 256 GPUs:
    &lt;ol&gt;
      &lt;li&gt;Each 4-process group, 16-process group, 64-process group, and the global 256-process group synchronizes every 1, 2, 4, and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 1-4,2-16,4-64,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 8-process group, 64-process group, and the global 256-process group synchronizes every 2, 4, and 8 steps. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 2-8,4-64,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
      &lt;li&gt;Each 16-process group and the global 256-process group synchronizes every 4 and 8 steps, respectively. Denoted as “&lt;em&gt;&lt;strong&gt;HSGD 4-16,8-256&lt;/strong&gt;&lt;/em&gt;”.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h3&gt;

&lt;p&gt;The figures below show the speedups of different communication schemes against the baseline of synchronous SGD, with the emulated stragglers. We can make the following observations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As expected, we can see that both hierarchical SGD and local SGD can achieve a higher speedup with a lower synchronization frequency.&lt;/li&gt;
  &lt;li&gt;The speedups of the hierarchical SGD schemes are &lt;strong&gt;2.08X-2.45X&lt;/strong&gt; on 64 GPUs, &lt;strong&gt;2.57X-2.68X&lt;/strong&gt; on 128 GPUs, and &lt;strong&gt;2.63X-3.25X&lt;/strong&gt; on 256 GPUs, respectively. This shows that hierarchical SGD can significantly mitigate stragglers, and such mitigation can be more effective at a larger scale.&lt;/li&gt;
  &lt;li&gt;The performance of local SGD with the synchronization period of 2 steps and 8 steps can be perceived as the lower bound and upper bound of the experimented hierarchical SGD schemes, respectively. This is because the hierarchical SGD schemes synchronize less frequently than every 2 steps globally, but their low-level synchronization at small groups are the extra overheads in comparison with the global synchronization every 8 steps.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Overall, hierarchical SGD can provide a finer-grained trade-off between communication cost and model quality than local SGD. Therefore, when local SGD at a relatively large synchronization period like 8 or 4 cannot give a satisfactory convergence efficiency, hierarchical SGD can have a much better chance to achieve both a good speedup and a model parity.&lt;/p&gt;

&lt;p&gt;Since only simulated data is used in the experiments, we did not demonstrate the model parity here, which in practice can be achieved in two ways:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Tuning the hyperparameters including both hierarchy and warmup steps;&lt;/li&gt;
  &lt;li&gt;For some cases, hierarchical SGD could lead to a slightly lower quality than the original model for the same number of training steps (i.e., lower convergence rate), but with a speedup like 2X+ per training step, it is still possible to achieve model parity with more steps but still less total training time.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-5.png&quot; alt=&quot;Speedups on 64 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-6.png&quot; alt=&quot;Speedups on 128 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/straggler-mitigation/straggler-mitigation-7.png&quot; alt=&quot;Speedups on 256 GPUs&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Before applying hierarchical SGD to straggler mitigation, the user should be aware of a few limitations of this approach:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This approach can only mitigate non-persistent stragglers, which occur to different workers at different times. However, for the case of persistent stragglers, which can be caused by hardware degradation or a network issue on a specific host, these stragglers will slow down the same low-level subgroup at every time, leading to nearly no straggler mitigation.&lt;/li&gt;
  &lt;li&gt;This approach can only mitigate low-frequency stragglers. E.g., if 30% workers can randomly become stragglers at every step, then most low-level synchronizations will still be slowed down by stragglers. As a result, hierarchical SGD may not show an obvious performance advantage over synchronous SGD.&lt;/li&gt;
  &lt;li&gt;Since hierarchical SGD applies model averaging that does not overlap with backward like gradient averaging used by vanilla DDP, its performance gain of straggler mitigation must outweigh the performance loss of no overlap between communication and backward pass. Therefore, if stragglers only slow down training by less than 10%, hierarchical SGD may not be able to bring much speedup. This limitation can be addressed by &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/release/1.13/torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py&quot;&gt;overlapping optimizer step and backward pass&lt;/a&gt; in the future.&lt;/li&gt;
  &lt;li&gt;Since hierarchical SGD is less well-studied than local SGD, there is no guarantee that hierarchical SGD with a finer-grained synchronization granularity can converge faster than certain advanced forms of local SGD, such as &lt;a href=&quot;https://openreview.net/pdf?id=SkxJ8REYPH&quot;&gt;SlowMo&lt;/a&gt;, which can improve convergence efficiency with slow momentum. However, to the best of our knowledge, these advanced algorithms cannot be natively supported as a PyTorch DDP plugin like hierarchical SGD yet.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank Cruise teammates &lt;strong&gt;Bo Tian&lt;/strong&gt;, &lt;strong&gt;Sergei Vorobev&lt;/strong&gt;, &lt;strong&gt;Eugene Selivonchyk, Tsugn-Hsien Lee&lt;/strong&gt;, &lt;strong&gt;Dan Ring&lt;/strong&gt;, &lt;strong&gt;Ian Ackerman&lt;/strong&gt;, &lt;strong&gt;Lei Chen&lt;/strong&gt;, &lt;strong&gt;Maegan Chew&lt;/strong&gt;, &lt;strong&gt;Viet Anh To&lt;/strong&gt;, &lt;strong&gt;Xiaohui Long&lt;/strong&gt;, &lt;strong&gt;Zeyu Chen&lt;/strong&gt;, &lt;strong&gt;Alexander Sidorov&lt;/strong&gt;, &lt;strong&gt;Igor Tsvetkov&lt;/strong&gt;, &lt;strong&gt;Xin Hu&lt;/strong&gt;, &lt;strong&gt;Manav Kataria&lt;/strong&gt;, &lt;strong&gt;Marina Rubtsova&lt;/strong&gt;, and &lt;strong&gt;Mohamed Fawzy&lt;/strong&gt;, as well as Meta teammates &lt;strong&gt;Shen Li, Yanli Zhao, Suraj Subramanian, Hamid Shojanzeri, Anjali Sridhar&lt;/strong&gt; and &lt;strong&gt;Bernard Nguyen&lt;/strong&gt; for the support.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yi Wang (Cruise AI), Rohan Varma (Meta AI)</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch DDP has been widely adopted across the industry for distributed training, which by default runs synchronous SGD to synchronize gradients across model replicas at every step. The performance of this technique is critical for fast iteration during model exploration as well as resource and cost saving. The performance is critical for fast iteration and cost saving of model development and exploration. To resolve a ubiquitous performance bottleneck introduced by slow nodes in large-scale training, Cruise and Meta co-developed a solution based on the Hierarchical SGD algorithm to significantly accelerate training in the presence of these stragglers.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch &amp;amp; OpenXLA: The Path Forward</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/" rel="alternate" type="text/html" title="PyTorch &amp; OpenXLA: The Path Forward" />
      <published>2023-04-03T00:00:00-07:00</published>
      <updated>2023-04-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-xla-path-forward</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-xla-path-forward/">&lt;p&gt;As we celebrate the release of &lt;a href=&quot;https://opensource.googleblog.com/2023/03/openxla-is-ready-to-accelerate-and-simplify-ml-development.html&quot;&gt;OpenXLA&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0&lt;/a&gt;, and &lt;a href=&quot;https://pytorch.org/blog/pytorch-2.0-xla/&quot;&gt;PyTorch/XLA 2.0&lt;/a&gt;, it’s worth taking a step back and sharing where we see it all going in the short to medium term. With PyTorch adoption leading in the AI space and XLA supporting best-in-class compiler features, PyTorch/XLA is well positioned to provide a cutting edge development stack for both model training and inference. To achieve this, we see investments in three main areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Training Large Models&lt;/strong&gt; - Large language models (LLM) and diffusion models have quickly risen in popularity and many cutting edge applications today are built on them. Further to this, training these models requires scale and more specifically the ability to train across thousands of accelerators. To achieve this we are investing in features such as AMP for mixed precision training, PjRt for increased runtime performance, SPMD / FSDP for efficient model sharding, Dynamic Shapes to enable new research approaches, faster data loading through Ray and tf.data, and a toolchain that packages all of these features together into a seamless workflow. Some of these features are already available in experimental or beta stages, and others are coming up this year with many heavily leveraging the underlying OpenXLA compiler stack.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Inference&lt;/strong&gt; - With large models continuing to grow in size and computational cost, deployment becomes the next challenge as these models continue to find their way into applications. With the introduction of Dynamo in the PyTorch 2.0 release, PyTorch/XLA delivers performance competitive inference. We are, however, incorporating additional inference-oriented including model serving support, Dynamo for sharded large models, quantization via Torch.Export and StableHLO.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ecosystem integration&lt;/strong&gt; - We are expanding integration with &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; and &lt;a href=&quot;https://lightning.ai/docs/pytorch/stable/&quot;&gt;PyTorch Lightning&lt;/a&gt; so users can take advantage of upcoming PyTorch/XLA cutting edge features (e.g. FSDP support in Hugging Face) and the downstream OpenXLA features (e.g. Quantization) through familiar APIs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally, PyTorch/XLA is set to migrate to the open source &lt;a href=&quot;https://github.com/openxla&quot;&gt;OpenXLA&lt;/a&gt; as its default downstream compiler; allowing the PyTorch community to gain access to a leading, framework-agnostic compiler stack that enjoys industry-wide contribution and innovation. To achieve this, we will begin supporting StableHLO. As a result, OpenXLA will replace the existing TF:XLA dependency, overall streamlining the dependencies and creating leverage from the broader compiler ecosystem. PyTorch/XLA will also sunset the XRT runtime after migration. You can see the resulting high level stack below with the TensorFlow dependency stricken out:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/PyTorch_XLA Future Stack.svg&quot; alt=&quot;the upcoming PyTorch/XLA features and integrations&quot; style=&quot;max-height:800px; width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;&lt;strong&gt;Figure:&lt;/strong&gt; the upcoming PyTorch/XLA features and integrations are illustrated here&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;We cannot be more excited about what’s ahead for PyTorch/XLA and invite the community to join us. PyTorch/XLA is developed fully in open source so please file issues, submit pull requests, and send RFCs to &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt; such that we can openly collaborate. You can also &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;try out&lt;/a&gt; PyTorch/XLA for yourself on various XLA devices including TPUs and GPUs.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br /&gt;
The PyTorch/XLA Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Milad Mohammadi, Jack Cao, Shauheen Zahirazami, Joe Spisak, and Jiewen Tan</name>
        
        
      </author>

      

      

      
        <summary type="html">As we celebrate the release of OpenXLA, PyTorch 2.0, and PyTorch/XLA 2.0, it’s worth taking a step back and sharing where we see it all going in the short to medium term. With PyTorch adoption leading in the AI space and XLA supporting best-in-class compiler features, PyTorch/XLA is well positioned to provide a cutting edge development stack for both model training and inference. To achieve this, we see investments in three main areas:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated PyTorch 2 Transformers</title>
      <link href="https://pytorch.org/blog/accelerated-pytorch-2/" rel="alternate" type="text/html" title="Accelerated PyTorch 2 Transformers" />
      <published>2023-03-28T00:00:00-07:00</published>
      <updated>2023-03-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-pytorch-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-pytorch-2/">&lt;p&gt;The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API with the goal of making training and deployment of state-of-the-art Transformer models affordable.  Following the successful release of “fastpath” inference execution (“Better Transformer”), this release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA).&lt;/p&gt;

&lt;p&gt;You can take advantage of the new fused SDPA kernels either by calling the new SDPA operator directly (as described in the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#beta-implementing-high-performance-transformers-with-scaled-dot-product-attention-sdpa&quot;&gt;SDPA tutorial&lt;/a&gt;), or transparently via integration into the pre-existing PyTorch Transformer API.  All features of the PyTorch Transformer API will continue to work compatibly, with many features mapped to high-performance SDPA kernels, while other features are impossible to support with higher performance (e.g., need_weights, as per below) while expanded high-performance support for other features may still be under active development.  &lt;br /&gt;
 &lt;br /&gt;
Similar to the “fastpath” architecture, custom kernels are fully integrated into the PyTorch Transformer API – thus, using the native Transformer and MultiHeadAttention API will enable users to transparently see significant speed improvements.  Unlike the “fastpath” architecture, the newly introduced “custom kernels” support many more use cases including models using Cross-Attention, Transformer Decoders, and for training models, in addition to the existing fastpath inference for fixed and variable sequence length Transformer Encoder and Self Attention use cases.&lt;/p&gt;

&lt;p&gt;To take full advantage of different hardware models and Transformer use cases, multiple SDPA custom kernels are supported, with custom kernel selection logic that will pick the highest-performance kernel for a given model and hardware type.  In particular, the first custom kernels included with the PyTorch 2.0 release are the &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;Flash Attention&lt;/a&gt; kernel (sdpa_flash, for 16-bit floating point training and inference on Nvidia GPUs with SM80+ architecture level) and the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers memory-efficient attention&lt;/a&gt; kernel (sdpa_mem_eff, for 16-bit and 32-bit floating point training and inference on a broad range of Nvidia GPUs).  A general-purpose kernel sdpa_math provides an implementation when the custom kernels are not applicable.&lt;/p&gt;

&lt;p&gt;As mentioned, custom kernels provide a wider range of support for execution scenarios To ensure efficient execution (e,g., to use GPU tensor cores), model configurations need to meet a small number of requirements.  This list of requirements will evolve over time, prospectively relaxing constraints limiting the usage of currently supported custom kernels, or providing additional kernels in the future.&lt;/p&gt;

&lt;p&gt;For the most up to date list of custom kernels and dispatch constraints, you can refer to &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/transformers/cuda/sdp_utils.h&quot;&gt;sdp_utils.h&lt;/a&gt;.  As of PyTorch 2.0, the existing fused SDPA kernels have the following constraints:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Flash Attention only supports 16 bit floating point data types (float16 and bfloat16).&lt;/li&gt;
  &lt;li&gt;The head dimension must be a multiple of 8 for 16-bit floating point numbers and a multiple of 4 for 32-bit floating point numbers. At present, the maximum head_dim support for the Flash Attention custom kernel is 128.&lt;/li&gt;
  &lt;li&gt;The CUDA architecture level must be sm5x or better for the mem_efficient kernel, and sm80 for Flash Attention.&lt;/li&gt;
  &lt;li&gt;Flash Attention supports arbitrary dropout, in PyTorch 2.0 the mem_efficient kernel does not support dropout (i.e., dropout must be set to zero for this kernel to be selected in PyTorch 2.0).&lt;/li&gt;
  &lt;li&gt;To support variable-sequence length batches, all SDPA kernels support Nested Tensor inputs that combine input data and padding information using variable sequence length tensors for forward. (You can find more information about Nested Tensors in the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/nestedtensor.html&quot;&gt;Nested Tensor tutorial&lt;/a&gt;.)&lt;/li&gt;
  &lt;li&gt;You can specify both a &lt;em&gt;key_padding_mask&lt;/em&gt; and an &lt;em&gt;attn_mask&lt;/em&gt; by combining them before passing them to the SDPA operator. In particular, you can use the per-batch-element key padding mask of the nn.Transformer API to implement training for variable-sequence length inputs in a batch.&lt;/li&gt;
  &lt;li&gt;At present, the only attention mask supported by fused kernel implementation is the causal mask commonly used for training. To specify the causal mask in custom kernels, it must be specified with the &lt;em&gt;is_causal&lt;/em&gt; boolean and &lt;em&gt;attn_mask&lt;/em&gt; must be None.&lt;/li&gt;
  &lt;li&gt;Support for Nested Tensors is still under development.  Specifically, in PyTorch 2.0, only the sdpa_math kernel supports training with Nested Tensors. Also, PyTorch 2.0 does not support Nested Tensors as part of code being compiled with torch.compile().&lt;/li&gt;
  &lt;li&gt;The SDPA operator does not support returning averaged attention weights because computing them defeats the optimizations that enabled fused kernels to execute more efficiently.  The argument &lt;em&gt;need_weights&lt;/em&gt; for torch.nn.MultiheadAttention’s forward function defaults to True. In order to use the fused kernels, &lt;em&gt;need_weights&lt;/em&gt; needs to be set to &lt;em&gt;need_weights=False&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We find that an attention mask is rarely used in real-world applications, except for the causal mask during training.  Consequently, we reduce kernel complexity and compute cost by building in the option to use a causal mask as attention mask, and select this new capability with the &lt;em&gt;is_causal&lt;/em&gt; parameter introduced in conjunction with the new SDPA operator.&lt;/p&gt;

&lt;p&gt;Providing the &lt;em&gt;is_causal&lt;/em&gt; Boolean flag for the frequently used causal mask also obviates the expensive and memory-intensive allocation of a causal mask, increasing training memory efficiency by allowing more memory to be used for large batch sizes, and reduce memory bandwidth and cache contention – which are both at a premium in GPU accelerators – by not needing to load an attention mask tensor.&lt;/p&gt;

&lt;p&gt;If the constraints of none of the available custom kernels are met, then training falls back to using the default sdpa_math kernel, implementing the mathematical equations for scaled dot product attention using a sequence of PyTorch operator to implement SDPA.  This is the most general “catch-all” fallback kernel to ensure successful training for all models.&lt;/p&gt;

&lt;p&gt;In addition to the existing Transformer API, model developers may also use the scaled dot product attention kernels directly by calling the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention()&lt;/code&gt; operator.  This operator may be used to efficiently implement multi-head attention by combining it with in-projection and outprojection, as described in the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;SDPA tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In addition to adding custom kernels, Accelerated PyTorch 2 Transformers are integrated with PyTorch 2.0 compilation.  To use your model while benefiting from the additional acceleration of PT2-compilation (for inference or training), pre-process the model with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torch.compile(model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have achieved major speedups for training transformer models and in particular large language models with Accelerated PyTorch 2 Transformers using a combination of custom kernels and torch.compile().&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch_better_transformer_chart1.png&quot; alt=&quot;Better Transformer chart&quot; width=&quot;100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Figure: Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Finally, because the custom kernels are much more memory efficient, try to increase the size of training batches to achieve faster training with increased batch size.&lt;/p&gt;

&lt;p&gt;In addition to automatic kernel selection, a context manager enables developers to override the kernel selection algorithm – this is not required for day to day operation, but enables developers to debug their code as well as enable performance engineers to override kernel selection. The SDPA tutorial provides additional information on using the SDPA context manager.&lt;/p&gt;

&lt;p&gt;In addition to availability as part of the nn.Transformer API, Accelerated PyTorch 2 Transformer custom kernels are also available in conjunction with the torchtext, torchvision, and fairseq domain libraries with the launch of PyTorch 2.0.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Michael Gschwind, Driss Guessous, Christian Puhrsch</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API with the goal of making training and deployment of state-of-the-art Transformer models affordable. Following the successful release of “fastpath” inference execution (“Better Transformer”), this release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA).</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.0 &amp;amp; XLA—The Latest Cutting Edge Features</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-xla/" rel="alternate" type="text/html" title="PyTorch 2.0 &amp; XLA—The Latest Cutting Edge Features" />
      <published>2023-03-22T00:00:00-07:00</published>
      <updated>2023-03-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-xla</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-xla/">&lt;p&gt;Today, we are excited to share our latest work for &lt;a href=&quot;https://github.com/pytorch/xla/releases/tag/v2.0.0&quot;&gt;PyTorch/XLA 2.0&lt;/a&gt;. The release of &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch 2.0&lt;/a&gt; is yet another major milestone for this storied community and we are excited to continue to be part of it. When the &lt;a href=&quot;https://github.com/pytorch/xla&quot;&gt;PyTorch/XLA&lt;/a&gt; project started in 2018 between Google and Meta, the focus was on bringing cutting edge Cloud TPUs to help support the PyTorch community. Along the way, others in the community such as Amazon joined the project and very quickly the community expanded. We are excited about XLA’s &lt;a href=&quot;https://opensource.googleblog.com/2023/03/openxla-is-ready-to-accelerate-and-simplify-ml-development.html&quot;&gt;direction&lt;/a&gt; and the benefits this project continues to bring to the PyTorch community. In this blog we’d like to showcase some key features that have been in development, show code snippets, and illustrate the benefit through some benchmarks.&lt;/p&gt;

&lt;h2 id=&quot;torchdynamo--torchcompile-experimental&quot;&gt;TorchDynamo / torch.compile (Experimental)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/torchdynamo&quot;&gt;TorchDynamo&lt;/a&gt; (Dynamo) is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. It provides a clean API for compiler backends to hook in; its biggest feature is to dynamically modify Python bytecode just before execution. In the PyTorch/XLA 2.0 release, an experimental backend for Dynamo is provided for both inference and training.&lt;/p&gt;

&lt;p&gt;Dynamo provides a &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;Torch FX&lt;/a&gt; (FX) graph when it recognizes a model pattern and PyTorch/XLA uses a Lazy Tensor approach to compile the FX graph and return the compiled function. To get more insight regarding the technical details about PyTorch/XLA’s dynamo implementation, check out &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchdynamo-update-10-integrating-with-pytorch-xla-for-inference-and-training/935&quot;&gt;this&lt;/a&gt; dev-discuss post and &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md&quot;&gt;dynamo doc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a small code example of running ResNet18 with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torch_xla.core.xla_model as xm

def eval_model(loader):
  device = xm.xla_device()
  xla_resnet18 = torchvision.models.resnet18().to(device)
  xla_resnet18.eval()
  dynamo_resnet18 = torch.compile(
      xla_resnet18, backend='torchxla_trace_once')
  for data, _ in loader:
    output = dynamo_resnet18(data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; PyTorch/XLA only traces the ResNet18 model once during the init time and executes the compiled binary everytime &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamo_resnet18&lt;/code&gt; is invoked, instead of tracing the model every step. To illustrate the benefits of Dynamo+XLA, below is an inference speedup analysis to compare Dynamo and LazyTensor (without Dynamo) using TorchBench on a Cloud TPU v4-8 where the y-axis is the speedup multiplier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-inferencespeedup.svg&quot; alt=&quot;Inference Speedup - PyTorch/XLA Dynamo on TPU&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dynamo for training is in the development stage with its implementation being at an earlier stage than inference. Developers are welcome to test this early feature, however, in the 2.0 release, PyTorch/XLA supports the forward and backward pass graphs and not the optimizer graph; the optimizer graph is available in the nightly builds and will land in the PyTorch/XLA 2.1 release. Below is an example of what training looks like using the ResNet18 example with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torchvision
import torch_xla.core.xla_model as xm

def train_model(model, data, target):
  loss_fn = torch.nn.CrossEntropyLoss()
  pred = model(data)
  loss = loss_fn(pred, target)
  loss.backward()
  return pred

def train_model_main(loader):
  device = xm.xla_device()
  xla_resnet18 = torchvision.models.resnet18().to(device)
  xla_resnet18.train()
  dynamo_train_model = torch.compile(
        train_model, backend='aot_torchxla_trace_once')
  for data, target in loader:
    output = dynamo_train_model(xla_resnet18, data, target)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the backend for training is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aot_torchxla_trace_once&lt;/code&gt; (API will be updated for stable release) whereas the inference backend is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchxla_trace_once&lt;/code&gt; (name subject to change). We expect to extract and execute 3 graphs per training step instead of 1 training step if you use the Lazy tensor. Below is a training speedup analysis to compare Dynamo and Lazy using the TorchBench on Cloud TPU v4-8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-trainingspeedup.svg&quot; alt=&quot;Training Speedup - PyTorch/XLA Dynamo on TPU&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pjrt-runtime-beta&quot;&gt;PJRT Runtime (Beta)&lt;/h2&gt;

&lt;p&gt;PyTorch/XLA is migrating from XRT to the new PJRT runtime. PJRT is a better-maintained stack, with demonstrated performance advantages, including, on average, a 35% performance for training on TorchBench 2.0 models. It also supports a richer set of features enabling technologies like SPMD. In the PyTorch/XLA 2.0 release, PJRT is the default runtime for TPU and CPU; GPU support is in experimental state. The PJRT features included in the PyTorch/XLA 2.0 release are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TPU runtime implementation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libtpu&lt;/code&gt; using the &lt;a href=&quot;https://github.com/openxla/community/blob/main/rfcs/20230123-pjrt-plugin.md#rfc-openxla-pjrt-plugin&quot;&gt;PJRT Plugin API&lt;/a&gt; improves performance by up to 30%&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed&lt;/code&gt; support for TPU v2 and v3, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pjrt://&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;init_method&lt;/code&gt; (Experimental)&lt;/li&gt;
  &lt;li&gt;Single-host GPU support. Multi-host support coming soon. (Experimental)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Switching to PJRT requires no change (or minimal change for GPUs) to user code (see &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/docs/pjrt.md&quot;&gt;pjrt.md&lt;/a&gt; for more details). Runtime configuration is as simple as setting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PJRT_DEVICE&lt;/code&gt; environment variable to the local device type (i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TPU&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GPU&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CPU&lt;/code&gt;). Below are examples of using PJRT runtimes on different devices.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TPU Device
PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# TPU Pod Device
gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;git clone --depth=1 --branch r2.0 https://github.com/pytorch/xla.git&quot;

gcloud alpha compute tpus tpu-vm ssh $USER-pjrt --zone=us-central2-b --project=$PROJECT --worker=all --command=&quot;PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# GPU Device (Experimental)
PJRT_DEVICE=GPU GPU_NUM_DEVICES=4 python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=128 --num_epochs=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is a performance comparison between XRT and PJRT by task on TorchBench 2.0 on v4-8 TPU. To learn more about PJRT vs. XRT please review the &lt;a href=&quot;https://github.com/pytorch/xla/blob/r2.0/docs/pjrt.md#tpu&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-torchbenchtraining.svg&quot; alt=&quot;TorchBench Training Time&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;parallelization&quot;&gt;Parallelization&lt;/h2&gt;

&lt;h3 id=&quot;gspmd-experimental&quot;&gt;GSPMD (Experimental)&lt;/h3&gt;

&lt;p&gt;We are delighted to introduce General and Scalable Parallelization for ML Computation Graphs (&lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt;) in PyTorch as a new experimental data &amp;amp; model sharding solution. &lt;a href=&quot;https://arxiv.org/abs/2105.04663&quot;&gt;GSPMD&lt;/a&gt; provides automatic parallelization for common ML workloads, allowing developers to write PyTorch programs as if on a single large device and without custom sharded computation ops and/or collective communication ops. The XLA compiler transforms the single device program into a partitioned one with proper collectives, based on the user provided sharding hints. The API (&lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt;) will be available in the PyTorch/XLA 2.0 release as an experimental feature on a single TPU VM host.&lt;/p&gt;

&lt;h4 id=&quot;next-steps-for-gspmd&quot;&gt;Next Steps for GSPMD&lt;/h4&gt;

&lt;p&gt;GSPMD is experimental in 2.0 release. To bring it to Stable status, we plan to address a number of feature gaps and known issues in the following releases, including multi-host support, DTensor integration, partial replication sharding, asynchronous data loading, and checkpointing.&lt;/p&gt;

&lt;h3 id=&quot;fsdp-beta&quot;&gt;FSDP (Beta)&lt;/h3&gt;

&lt;p&gt;PyTorch/XLA &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/&quot;&gt;introduced&lt;/a&gt; fully sharded data parallel (FSDP) experimental support in version 1.12. This feature is a parallel representation of PyTorch FSDP and there are subtle differences in how XLA and upstream CUDA kernels are set up. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt; is a new argument that enables developers to automatically specify conditions for propagating partitioning specifications to neural network submodules. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt;s may be simply passed in as an argument when wrapping a model with FSDP. Two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto_wrap_policy&lt;/code&gt; callables worth noting are: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_auto_wrap_policy&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transformer_auto_wrap_policy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_based_auto_wrap_policy&lt;/code&gt; enables users to wrap submodules with a minimum number of parameters. The example below wraps model submodules having at least 10M parameters.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto_wrap_policy = partial(size_based_auto_wrap_policy, min_num_params=1e7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transformer_auto_wrap_policy&lt;/code&gt; enables users to wrap all submodules that match a specific layer type. The example below wraps model submodules named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Conv2d&lt;/code&gt;. To learn more, review &lt;a href=&quot;https://github.com/pytorch/xla/blob/master/test/test_train_mp_imagenet_fsdp.py#L237-L255&quot;&gt;this ResNet example&lt;/a&gt; by Ronghang Hu.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auto_wrap_policy = partial(transformer_auto_wrap_policy, transformer_layer_cls={torch.nn.Conv2d})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;PyTorch/XLA FSDP is now integrated in HuggingFace trainer class (&lt;a href=&quot;https://github.com/huggingface/transformers/pull/21406&quot;&gt;PR&lt;/a&gt;) enabling users to train much larger models on PyTorch/XLA (&lt;a href=&quot;https://huggingface.co/docs/transformers/main/en/main_classes/trainer#pytorchxla-fully-sharded-data-parallel&quot;&gt;official Hugging Face documentation&lt;/a&gt;). A 16B parameters GPT2 model trained on Cloud TPU v4-64 with this FSDP configuration achieved 39% hardware utilization.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot; style=&quot;max-width: 450px;&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TPU Accelerator - Num Devices&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;v4-64
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;GPT2 Parameter Count&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;16B
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Layers Wrapped with FSDP&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;GPT2Block
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;TFLOPs / Chip&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;275
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PFLOPs / Step&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;50
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Hardware Utilization&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;39%
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;differences-between-fsdp--gspmd&quot;&gt;Differences Between FSDP &amp;amp; GSPMD&lt;/h3&gt;

&lt;p&gt;FSDP is a data parallelism technique that reduces device memory footprint by storing model parameters, optimizer states, and gradients all sharded. Note that the actual computation is still local to the device and requires all-gathering the sharded model parameters for both forward and backward passes, hence the name “data parallel”. FSDP is one of the newest additions to PyTorch/XLA to scale large model training.&lt;/p&gt;

&lt;p&gt;GSPMD on the other hand, is a general parallelization system that enables various types of parallelisms, including both data and model parallelisms. PyTorch/XLA provides a sharding annotation API and XLAShardedTensor abstraction, so a user can annotate any tensor with sharding specs in the PyTorch program. Developers don’t need to manually implement sharded computations or inject collective communications ops to get it right. The XLA compiler does the work so that each computation can run in a distributed manner on multiple devices.&lt;/p&gt;

&lt;h3 id=&quot;examples--preliminary-results&quot;&gt;Examples &amp;amp; Preliminary Results&lt;/h3&gt;

&lt;p&gt;To learn about PyTorch/XLA parallelism sharding API, visit our &lt;a href=&quot;https://github.com/pytorch/xla/issues/3871&quot;&gt;RFC&lt;/a&gt; and see the &lt;a href=&quot;https://github.com/pytorch/xla/tree/r2.0/test/spmd&quot;&gt;Sample Code&lt;/a&gt; references. Below is a simple example to enable data and model parallelism.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = SimpleLinear().to(xm.xla_device())
# Sharding annotate the linear layer weights.
xs.mark_sharding(model.fc1.weight, mesh, partition_spec)
# Training loop
model.train()
for step, (data, target) in enumerate(loader):
  optimizer.zero_grad()
  data = data.to(xm.xla_device())
  target = target.to(xm.xla_device())
  # Sharding annotate input data, we can shard any input
  # dimensions. Sharidng the batch dimension enables 
  # data parallelism, sharding the feature dimension enables
  # spatial partitioning.
  xs.mark_sharding(data, mesh, partition_spec)
  ouput = model(data)
  loss = loss_fn(output, target)
  optimizer.step()
  xm.mark_step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following graph highlights the memory efficiency benefits of PyTorch/XLA FSDP and SPMD on Cloud TPU v4-8 running ResNet50.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-03-22-batchsizescaling.svg&quot; alt=&quot;Batch Size Scaling with Spatial Partitioning&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing Thoughts…&lt;/h2&gt;

&lt;p&gt;We are excited to bring these features to the PyTorch community, and this is really just the beginning. Areas like dynamic shapes, deeper support for OpenXLA and many others are in development and we plan to put out more blogs to dive into the details. PyTorch/XLA is developed fully open source and we invite you to join the community of developers by filing issues, submitting pull requests, and sending RFCs on &lt;a href=&quot;github.com/pytorch/xla&quot;&gt;GitHub&lt;/a&gt;. You can try PyTorch/XLA on a variety of XLA devices including TPUs and GPUs. &lt;a href=&quot;https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb&quot;&gt;Here&lt;/a&gt; is how to get started.&lt;/p&gt;

&lt;p&gt;Congratulations again to the PyTorch community on this milestone!&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;The PyTorch Team at Google&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jack Cao, Milad Mohammadi, Alex Wertheim, Yeounoh Chung, Joe Spisak, Will Cromar, Shauheen Zahirazami</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we are excited to share our latest work for PyTorch/XLA 2.0. The release of PyTorch 2.0 is yet another major milestone for this storied community and we are excited to continue to be part of it. When the PyTorch/XLA project started in 2018 between Google and Meta, the focus was on bringing cutting edge Cloud TPUs to help support the PyTorch community. Along the way, others in the community such as Amazon joined the project and very quickly the community expanded. We are excited about XLA’s direction and the benefits this project continues to bring to the PyTorch community. In this blog we’d like to showcase some key features that have been in development, show code snippets, and illustrate the benefit through some benchmarks.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated Diffusers with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/accelerated-diffusers-pt-20/" rel="alternate" type="text/html" title="Accelerated Diffusers with PyTorch 2.0" />
      <published>2023-03-16T00:00:00-07:00</published>
      <updated>2023-03-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-diffusers-pt-20</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-diffusers-pt-20/">&lt;p&gt;PyTorch 2.0 has just been released. Its flagship new feature is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;, a one-line code change that promises to automatically improve performance across codebases. We have previously &lt;a href=&quot;https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/&quot;&gt;checked on that promise in Hugging Face Transformers and TIMM models&lt;/a&gt;, and delved deep into its &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;motivation, architecture and the road ahead&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As important as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is, there’s much more to PyTorch 2.0. Notably, PyTorch 2.0 incorporates several strategies to accelerate transformer blocks, and these improvements are very relevant for diffusion models too. Techniques such as &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt;, for example, have become very popular in the diffusion community thanks to their ability to significantly speed up Stable Diffusion and achieve larger batch sizes, and they are now part of PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;In this post we discuss how attention layers are optimized in PyTorch 2.0 and how these optimization are applied to the popular &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;🧨 Diffusers library&lt;/a&gt;. We finish with a benchmark that shows how the use of PyTorch 2.0 and Diffusers immediately translates to significant performance improvements across different hardware.&lt;/p&gt;

&lt;h2 id=&quot;accelerating-transformer-blocks&quot;&gt;Accelerating transformer blocks&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 includes a &lt;em&gt;scaled dot-product attention&lt;/em&gt; function as part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.functional&lt;/code&gt;. This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. Before PyTorch 2.0, you had to search for third-party implementations and install separate packages in order to take advantage of memory optimized algorithms, such as FlashAttention. The available implementations are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;FlashAttention, from the official &lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;FlashAttention project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Memory-Efficient Attention, from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A native C++ implementation suitable for non-CUDA devices or when high-precision is required.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these methods are available by default, and PyTorch will try to select the optimal one automatically through the use of the new scaled dot-product attention (SDPA) API. You can also individually toggle them for finer-grained control, see &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention&quot;&gt;the documentation&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;using-scaled-dot-product-attention-in-diffusers&quot;&gt;Using scaled dot-product attention in diffusers&lt;/h2&gt;

&lt;p&gt;The incorporation of Accelerated PyTorch 2.0 Transformer attention to the Diffusers library was achieved through the use of the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/api/models#diffusers.UNet2DConditionModel.set_attn_processor&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_attn_processor&lt;/code&gt; method&lt;/a&gt;, which allows for pluggable attention modules to be configured. In this case, a &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57/src/diffusers/models/cross_attention.py#L469&quot;&gt;new attention processor was created&lt;/a&gt;, which is &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/856dad57bb7a9ee13af4a08492e524b0a145a2c5/src/diffusers/models/cross_attention.py#L105&quot;&gt;enabled by default when PyTorch 2.0 is available&lt;/a&gt;. For clarity, this is how you could enable it manually (but it’s usually not necessary since diffusers will automatically take care of it):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionPipeline
from diffusers.models.cross_attention import AttnProcessor2_0

pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;)
pipe.to(&quot;cuda&quot;)
pipe.unet.set_attn_processor(AttnProcessor2_0())

prompt = &quot;a photo of an astronaut riding a horse on mars&quot;
image = pipe(prompt).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;stable-diffusion-benchmark&quot;&gt;Stable Diffusion Benchmark&lt;/h2&gt;

&lt;p&gt;We ran a number of tests using accelerated dot-product attention from PyTorch 2.0 in Diffusers. We installed diffusers from pip and used nightly versions of PyTorch 2.0, since our tests were performed before the official release. We also used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.set_float32_matmul_precision('high')&lt;/code&gt; to enable additional fast matrix multiplication algorithms.&lt;/p&gt;

&lt;p&gt;We compared results with the traditional attention implementation in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diffusers&lt;/code&gt; (referred to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vanilla&lt;/code&gt; below) as well as with the best-performing solution in pre-2.0 PyTorch: PyTorch 1.13.1 with the xFormers package (v0.0.16) installed.&lt;/p&gt;

&lt;p&gt;Results were measured without compilation (i.e., no code changes at all), and also with a single call to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to wrap the UNet module. We did not compile the image decoder because most of the time is spent in the 50 denoising iterations that run UNet evaluations.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float32&quot;&gt;Results in float32&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig1-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float32&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following figures explore performance improvement vs batch size for various representative GPUs belonging to different generations. We collected data for each combination until we reached maximum memory utilization. Vanilla attention runs out of memory earlier than xFormers or PyTorch 2.0, which explains the missing bars for larger batch sizes. Similarly, A100 (we used the 40 GB version) is capable of running batch sizes of 64, but the other GPUs could only reach 32 in our tests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig2-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig3-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig4-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig5-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (V100, float32)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We found very significant performance improvements over vanilla attention across the board, without even using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt;. An out of the box installation of PyTorch 2.0 and diffusers yields about 50% speedup on A100 and between 35% and 50% on 4090 GPUs, depending on batch size. Performance improvements are more pronounced for modern CUDA architectures such as Ada (4090) or Ampere (A100), but they are still very significant for older architectures still heavily in use in cloud services.&lt;/p&gt;

&lt;p&gt;In addition to faster speeds, the accelerated transformers implementation in PyTorch 2.0 allows much larger batch sizes to be used. A single 40GB A100 GPU runs out of memory with a batch size of 10, and 24 GB high-end consumer cards such as 3090 and 4090 cannot generate 8 images at once. Using PyTorch 2.0 and diffusers we could achieve batch sizes of &lt;strong&gt;48&lt;/strong&gt; for 3090 and 4090, and &lt;strong&gt;64&lt;/strong&gt; for A100. This is of great significance for cloud services and applications, as they can efficiently process more images at a time.&lt;/p&gt;

&lt;p&gt;When compared with PyTorch 1.13.1 + xFormers, the new accelerated transformers implementation is still faster and requires no additional packages or dependencies. In this case we found moderate speedups of up to 2% on datacenter cards such as A100 or T4, but performance was great on the two last generations of consumer cards: up to 20% speed improvement on 3090 and between 10% and 45% on 4090, depending on batch size.&lt;/p&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; is used, we get an additional performance boost of (typically) 2% and 3% over the previous improvements. As compilation takes some time, this is better geared towards user-facing inference services or training.&lt;/p&gt;

&lt;h3 id=&quot;results-in-float16&quot;&gt;Results in float16&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig6-latest.png&quot; alt=&quot;Diffusers Speedup vs xFormers float16&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig7-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (A100, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig8-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (4090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/3-16-accelerated-d/fig9-latest.png&quot; alt=&quot;Diffusers Inference Speedup vs Vanilla and xFormers Attention (3090, float16)&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we consider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;float16&lt;/code&gt; inference, the performance improvements of the accelerated transformers implementation in PyTorch 2.0 are between 20% and 28% over standard attention, across all the GPUs we tested, except for the 4090, which belongs to the more modern Ada architecture. This GPU benefits from a dramatic performance improvement when using PyTorch 2.0 nightlies. With respect to optimized SDPA vs xFormers, results are usually on par for most GPUs, except again for the 4090. Adding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to the mix boosts performance a few more percentage points across the board.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;PyTorch 2.0 comes with multiple features to optimize the crucial components of the foundational transformer block, and they can be further improved with the use of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. These optimizations lead to significant memory and time improvements for diffusion models, and remove the need for third-party library installations.&lt;/p&gt;

&lt;p&gt;To take advantage of these speed and memory improvements all you have to do is upgrade to PyTorch 2.0 and use diffusers &amp;gt;= 0.13.0.&lt;/p&gt;

&lt;p&gt;For more examples and in-detail benchmark numbers, please also have a look at the &lt;a href=&quot;https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/torch2.0&quot;&gt;Diffusers with PyTorch 2.0&lt;/a&gt; docs.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The authors are grateful to the PyTorch team for creating such excellent software.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pedro Cuenca, Patrick von Platen, Suraj Patil</name>
        
        
      </author>

      

      

      
        <summary type="html">PyTorch 2.0 has just been released. Its flagship new feature is torch.compile(), a one-line code change that promises to automatically improve performance across codebases. We have previously checked on that promise in Hugging Face Transformers and TIMM models, and delved deep into its motivation, architecture and the road ahead.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">New Library Updates in PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/" rel="alternate" type="text/html" title="New Library Updates in PyTorch 2.0" />
      <published>2023-03-15T00:00:00-07:00</published>
      <updated>2023-03-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/new-library-updates-in-pytorch-2.0</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We are bringing a number of improvements to the current PyTorch libraries, alongside the &lt;a href=&quot;/blog/pytorch-2.0-release/&quot;&gt;PyTorch 2.0 release&lt;/a&gt;. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.&lt;/p&gt;

&lt;p&gt;Along with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. Please find the list of the latest stable versions and updates below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Latest Stable Library Versions (&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;Full List&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchArrow 0.1.0
   &lt;/td&gt;
   &lt;td&gt;TorchRec 0.4.0
   &lt;/td&gt;
   &lt;td&gt;TorchVision 0.15
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchAudio 2.0
   &lt;/td&gt;
   &lt;td&gt;TorchServe 0.7.1
   &lt;/td&gt;
   &lt;td&gt;TorchX 0.4.0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchData 0.6.0
   &lt;/td&gt;
   &lt;td&gt;TorchText 0.15.0
   &lt;/td&gt;
   &lt;td&gt;PyTorch on XLA Devices 1.14
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;prior versions&lt;/a&gt; or (unstable) nightlies, click on versions in the top left menu above ‘Search Docs’.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio&quot;&gt;TorchAudio&lt;/h2&gt;

&lt;h3 id=&quot;beta-data-augmentation-operators&quot;&gt;[Beta] Data augmentation operators&lt;/h3&gt;

&lt;p&gt;The release adds several data augmentation operators under torchaudio.functional and torchaudio.transforms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;torchaudio.functional.add_noise&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.convolve&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.deemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.fftconvolve&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.preemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.functional.speed&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.AddNoise&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Convolve&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Deemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.FFTConvolve&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Preemphasis&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.Speed&lt;/li&gt;
  &lt;li&gt;torchaudio.transforms.SpeedPerturbation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The operators can be used to synthetically diversify training data to improve the generalizability of downstream models.&lt;/p&gt;

&lt;p&gt;For usage details, please refer to the &lt;a href=&quot;https://pytorch.org/audio/2.0.0/functional.html&quot;&gt;functional&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/transforms.html&quot;&gt;transform&lt;/a&gt; documentation and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/tutorials/audio_data_augmentation_tutorial.html&quot;&gt;Audio Data Augmentation&lt;/a&gt; tutorial.&lt;/p&gt;

&lt;h3 id=&quot;beta-wavlm-and-xls-r-models&quot;&gt;[Beta] WavLM and XLS-R models&lt;/h3&gt;

&lt;p&gt;The release adds two self-supervised learning models for speech and audio.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/9814838&quot;&gt;WavLM&lt;/a&gt; that is robust to noise and reverberation.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.09296&quot;&gt;XLS-R&lt;/a&gt; that is trained on cross-lingual datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides the model architectures, torchaudio also supports corresponding pre-trained pipelines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_BASE&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_BASE_PLUS&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAVLM_LARGE&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_300M&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_1B&lt;/li&gt;
  &lt;li&gt;torchaudio.pipelines.WAV2VEC_XLSR_2B&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For usage details, please refer to the &lt;a href=&quot;https://pytorch.org/audio/2.0.0/generated/torchaudio.models.Wav2Vec2Model.html#factory-functions&quot;&gt;factory function&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/2.0.0/pipelines.html#id3&quot;&gt;pre-trained pipelines&lt;/a&gt; documentation.&lt;/p&gt;

&lt;h2 id=&quot;torchrl&quot;&gt;TorchRL&lt;/h2&gt;

&lt;p&gt;The initial release of torchrl includes several features that span across the entire RL domain. TorchRL can already be used in online, offline, multi-agent, multi-task and distributed RL settings, among others. See below:&lt;/p&gt;

&lt;h3 id=&quot;beta-environment-wrappers-and-transforms&quot;&gt;[Beta] Environment wrappers and transforms&lt;/h3&gt;

&lt;p&gt;torchrl.envs includes several wrappers around common environment libraries. This allows users to swap one library with another without effort. These wrappers build an interface between these simulators and torchrl:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;dm_control:&lt;/li&gt;
  &lt;li&gt;Gym&lt;/li&gt;
  &lt;li&gt;Brax&lt;/li&gt;
  &lt;li&gt;EnvPool&lt;/li&gt;
  &lt;li&gt;Jumanji&lt;/li&gt;
  &lt;li&gt;Habitat&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It also comes with many commonly used transforms and vectorized environment utilities that allow for a fast execution across simulation libraries. Please refer to the &lt;a href=&quot;https://pytorch.org/rl/reference/envs.html&quot;&gt;documentation&lt;/a&gt; for more detail.&lt;/p&gt;

&lt;h3 id=&quot;beta-datacollectors&quot;&gt;[Beta] Datacollectors&lt;/h3&gt;

&lt;p&gt;Data collection in RL is made easy via the usage of single process or multiprocessed/distributed data collectors that execute the policy in the environment over a desired duration and deliver samples according to the user’s needs. These can be found in torchrl.collectors and are documented &lt;a href=&quot;https://pytorch.org/rl/reference/collectors.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-objective-modules&quot;&gt;[Beta] Objective modules&lt;/h3&gt;

&lt;p&gt;Several objective functions are included in torchrl.objectives, among which:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A generic PPOLoss class and derived ClipPPOLoss and KLPPOLoss&lt;/li&gt;
  &lt;li&gt;SACLoss and DiscreteSACLoss&lt;/li&gt;
  &lt;li&gt;DDPGLoss&lt;/li&gt;
  &lt;li&gt;DQNLoss&lt;/li&gt;
  &lt;li&gt;REDQLoss&lt;/li&gt;
  &lt;li&gt;A2CLoss&lt;/li&gt;
  &lt;li&gt;TD3Loss&lt;/li&gt;
  &lt;li&gt;ReinforceLoss&lt;/li&gt;
  &lt;li&gt;Dreamer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vectorized value function operators also appear in the library. Check the documentation &lt;a href=&quot;https://pytorch.org/rl/reference/objectives.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-models-and-exploration-strategies&quot;&gt;[Beta] Models and exploration strategies&lt;/h3&gt;

&lt;p&gt;We provide multiple models, modules and exploration strategies. Get a detailed description in &lt;a href=&quot;https://pytorch.org/rl/reference/modules.html&quot;&gt;the doc&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-composable-replay-buffer&quot;&gt;[Beta] Composable replay buffer&lt;/h3&gt;

&lt;p&gt;A composable replay buffer class is provided that can be used to store data in multiple contexts including single and multi-agent, on and off-policy and many more.. Components include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Storages (list, physical or memory-based contiguous storages)&lt;/li&gt;
  &lt;li&gt;Samplers (Prioritized, sampler without repetition)&lt;/li&gt;
  &lt;li&gt;Writers&lt;/li&gt;
  &lt;li&gt;Possibility to add transforms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Replay buffers and other data utilities are documented &lt;a href=&quot;https://pytorch.org/rl/reference/data.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-logging-tools-and-trainer&quot;&gt;[Beta] Logging tools and trainer&lt;/h3&gt;

&lt;p&gt;We support multiple logging tools including tensorboard, wandb and mlflow.&lt;/p&gt;

&lt;p&gt;We provide a generic Trainer class that allows for easy code recycling and checkpointing.&lt;/p&gt;

&lt;p&gt;These features are documented &lt;a href=&quot;https://pytorch.org/rl/reference/trainers.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;tensordict&quot;&gt;TensorDict&lt;/h2&gt;

&lt;p&gt;TensorDict is a new data carrier for PyTorch.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensordict-specialized-dictionary-for-pytorch&quot;&gt;[Beta] TensorDict: specialized dictionary for PyTorch&lt;/h3&gt;

&lt;p&gt;TensorDict allows you to execute many common operations across batches of tensors carried by a single container. TensorDict supports many shape and device or storage operations, and  can readily be used in distributed settings. Check the &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/&quot;&gt;documentation&lt;/a&gt; to know more.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensorclass-a-dataclass-for-pytorch&quot;&gt;[Beta] @tensorclass: a dataclass for PyTorch&lt;/h3&gt;

&lt;p&gt;Like TensorDict, &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/reference/prototype.html&quot;&gt;tensorclass&lt;/a&gt; provides the opportunity to write dataclasses with built-in torch features such as shape or device operations.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensordictnn-specialized-modules-for-tensordict&quot;&gt;[Beta] tensordict.nn: specialized modules for TensorDict&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://pytorch-labs.github.io/tensordict/reference/nn.html&quot;&gt;tensordict.nn module&lt;/a&gt; provides specialized nn.Module subclasses that make it easy to build arbitrarily complex graphs that can be executed with TensorDict inputs. It is compatible with the latest PyTorch features such as functorch, torch.fx and torch.compile.&lt;/p&gt;

&lt;h2 id=&quot;torchrec&quot;&gt;TorchRec&lt;/h2&gt;

&lt;h3 id=&quot;beta-keyedjaggedtensor-all-to-all-redesign-and-input-dist-fusion&quot;&gt;[Beta] KeyedJaggedTensor All-to-All Redesign and Input Dist Fusion&lt;/h3&gt;

&lt;p&gt;We observed performance regression due to a bottleneck in sparse data distribution for models that have multiple, large KJTs to redistribute.&lt;/p&gt;

&lt;p&gt;To combat this we altered the comms pattern to transport the minimum data required in the initial collective to support the collective calls for the actual KJT tensor data. This data sent in the initial collective, ‘splits’ means more data is transmitted over the comms stream overall, but the CPU is blocked for significantly shorter amounts of time leading to better overall QPS.&lt;/p&gt;

&lt;p&gt;Furthermore, we altered the TorchRec train pipeline to group the initial collective calls for the splits together before launching the more expensive KJT tensor collective calls. This fusion minimizes the CPU blocked time as launching each subsequent input distribution is no longer dependent on the previous input distribution.&lt;/p&gt;

&lt;p&gt;With this feature, variable batch sizes are now natively supported across ranks. These features are documented &lt;a href=&quot;https://github.com/pytorch/torchrec/commit/d0d23bef8aef5a79a1061fbc842c97bb68b91463&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchvision&quot;&gt;TorchVision&lt;/h2&gt;

&lt;h3 id=&quot;beta-extending-torchvisions-transforms-to-object-detection-segmentation--video-tasks&quot;&gt;[Beta] Extending TorchVision’s Transforms to Object Detection, Segmentation &amp;amp; Video tasks&lt;/h3&gt;

&lt;p&gt;TorchVision is extending its Transforms API! Here is what’s new:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You can use them not only for Image Classification but also for Object Detection, Instance &amp;amp; Semantic Segmentation and Video Classification.&lt;/li&gt;
  &lt;li&gt;You can use new functional transforms for transforming Videos, Bounding Boxes and Segmentation Masks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Learn more about these new transforms &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/&quot;&gt;from our docs&lt;/a&gt;, and submit any feedback in our &lt;a href=&quot;https://github.com/pytorch/vision/issues/6753&quot;&gt;dedicated issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchtext&quot;&gt;TorchText&lt;/h2&gt;

&lt;h3 id=&quot;beta-adding-scriptable-t5-and-flan-t5-to-the-torchtext-library-with-incremental-decoding-support&quot;&gt;[Beta] Adding scriptable T5 and Flan-T5 to the TorchText library with incremental decoding support!&lt;/h3&gt;

&lt;p&gt;TorchText has added the T5 model architecture with pre-trained weights for both the &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;original T5 paper&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2210.11416&quot;&gt;Flan-T5&lt;/a&gt;. The model is fully torchscriptable and features an optimized &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.ao.nn.quantizable.MultiheadAttention.html?highlight=multihead#torch.ao.nn.quantizable.MultiheadAttention&quot;&gt;multiheaded attention implementation&lt;/a&gt;. We include several examples of how to utilize the model including summarization, classification, and translation.&lt;/p&gt;

&lt;p&gt;For more details, please refer to &lt;a href=&quot;https://pytorch.org/text/stable/models.html&quot;&gt;our docs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchx&quot;&gt;TorchX&lt;/h2&gt;

&lt;p&gt;TorchX is moving to community supported mode. More details will be coming in at a later time.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever</title>
      <link href="https://pytorch.org/blog/pytorch-2.0-release/" rel="alternate" type="text/html" title="PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever" />
      <published>2023-03-15T00:00:00-07:00</published>
      <updated>2023-03-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-2.0-release</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2.0-release/">&lt;p&gt;We are excited to announce the release of &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.0.0&quot;&gt;PyTorch® 2.0&lt;/a&gt; which we highlighted during the &lt;a href=&quot;https://www.youtube.com/@PyTorch/playlists?view=50&amp;amp;sort=dd&amp;amp;shelf_id=2&quot;&gt;PyTorch Conference&lt;/a&gt; on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.&lt;/p&gt;

&lt;p&gt;This next-generation release includes a Stable version of Accelerated Transformers (formerly called Better Transformers); Beta includes torch.compile as the main API for PyTorch 2.0,  the scaled_dot_product_attention function as part of torch.nn.functional, the MPS backend, functorch APIs in the torch.func module; and other Beta/Prototype improvements across various inferences, performance and training optimization features on GPUs and CPUs. For a comprehensive introduction and technical overview of torch.compile, please visit the 2.0 &lt;a href=&quot;/get-started/pytorch-2.0&quot;&gt;Get Started page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Along with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. More details can be found in this &lt;a href=&quot;/blog/new-library-updates-in-pytorch-2.0/&quot;&gt;library blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This release is composed of over 4,541 commits and 428 contributors since 1.13.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.0 and the overall 2-series this year.&lt;/p&gt;

&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.&lt;/li&gt;
  &lt;li&gt;As an underpinning technology of torch.compile, TorchInductor with Nvidia and AMD GPUs will rely on OpenAI Triton deep learning compiler to generate performant code and hide low level hardware details. OpenAI Triton-generated kernels achieve performance that’s on par with hand-written kernels and specialized cuda libraries such as cublas.&lt;/li&gt;
  &lt;li&gt;Accelerated Transformers introduce high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA). The API is integrated with torch.compile() and model developers may also use the &lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;scaled dot product attention&lt;/a&gt; kernels directly by calling the new scaled_dot_product_attention() operator.&lt;/li&gt;
  &lt;li&gt;Metal Performance Shaders (MPS) backend provides GPU accelerated PyTorch training on Mac platforms with added support for Top 60 most used ops, bringing coverage to over 300 operators.&lt;/li&gt;
  &lt;li&gt;Amazon AWS optimizes the PyTorch CPU inference on AWS Graviton3 based &lt;a href=&quot;https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7g-instances-powered-by-aws-graviton3-processors/&quot;&gt;C7g instances&lt;/a&gt;. PyTorch 2.0 improves inference performance on Graviton compared to the previous releases, including improvements for Resnet50 and Bert.&lt;/li&gt;
  &lt;li&gt;New prototype features and technologies across TensorParallel, DTensor, 2D parallel, TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.&lt;/li&gt;
&lt;/ul&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
&lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;
&lt;strong&gt;Stable&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;a href=&quot;#stable-features&quot;&gt;Accelerated PT 2 Transformers&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-features&quot;&gt;torch.compile&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-features&quot;&gt;DTensor&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;CUDA support for 11.7 &amp;amp; 11.8 (deprecating CUDA 11.6) &lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-pytorch-mps-backend&quot;&gt;PyTorch MPS Backend&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-tensorparallel&quot;&gt;TensorParallel&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt; 

&lt;a href=&quot;#deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;Python 3.8 (deprecating Python 3.7)&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;Scaled dot product attention&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#prototype-2d-parallel&quot;&gt;2D Parallel&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#optimized-pytorch-inference-with-aws-graviton-processors&quot;&gt;AWS Graviton3&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-functorch---torchfunc&quot;&gt;functorch&lt;/a&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;

&lt;a href=&quot;#beta-torchcompile&quot;&gt;Torch.compile (dynamic=True)&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#beta-dispatchable-collectives&quot;&gt;Dispatchable Collectives&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#beta-torchset_default_device-and-torchdevice-as-context-manager&quot;&gt;Torch.set_default &amp;amp; torch.device&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-x86-as-the-new-default-quantization-backend-for-x86-cpu&quot;&gt;X86 quantization backend&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;a href=&quot;#beta-gnn-inference-and-training-optimization-on-cpu&quot;&gt;GNN inference and training performance&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public 2.0, 1.13 and 1.12 feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1H3jazwO8BBCwK8JwLNYspLiHfUrzshEtyqjL-X93I9g/edit#gid=790902532&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;stable-features&quot;&gt;Stable Features&lt;/h2&gt;

&lt;h3 id=&quot;stable-accelerated-pytorch-2-transformers&quot;&gt;[Stable] Accelerated PyTorch 2 Transformers&lt;/h3&gt;

&lt;p&gt;The PyTorch 2.0 release includes a new high-performance implementation of the PyTorch Transformer API. In releasing Accelerated PT2 Transformers, our goal is to make training and deployment of state-of-the-art Transformer models affordable across the industry. This release introduces high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA), extending the inference “fastpath” architecture, previously known as “Better Transformer.”&lt;/p&gt;

&lt;p&gt;Similar to the “fastpath” architecture, custom kernels are fully integrated into the PyTorch Transformer API – thus, using the native Transformer and MultiHeadAttention API will enable users to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;transparently see significant speed improvements;&lt;/li&gt;
  &lt;li&gt;support many more use cases including models using Cross-Attention, Transformer Decoders, and for training models; and&lt;/li&gt;
  &lt;li&gt;continue to use fastpath inference for fixed and variable sequence length Transformer Encoder and Self Attention use cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To take full advantage of different hardware models and Transformer use cases, multiple SDPA custom kernels are supported (see below), with custom kernel selection logic that will pick the highest-performance kernel for a given model and hardware type. In addition to the existing Transformer API, model developers may also use the &lt;a href=&quot;#beta-scaled-dot-product-attention-20&quot;&gt;scaled dot product attention&lt;/a&gt; kernels directly by calling the new scaled_dot_product_attention() operator. Accelerated PyTorch 2 Transformers are integrated with torch.compile() .  To use your model while benefiting from the additional acceleration of PT2-compilation (for inference or training), pre-process the model with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model = torch.compile(model)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We have achieved major speedups for training transformer models and in particular large language models with Accelerated PyTorch 2 Transformers using a combination of custom kernels and torch.compile().&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch20post.png&quot; alt=&quot;alt_text&quot; title=&quot;Accelerated PyTorch 2 speed&quot; width=&quot;100%&quot; /&gt;
&lt;small style=&quot;line-height: 1.1&quot;&gt;&lt;em&gt;Figure: Using scaled dot product attention with custom kernels and torch.compile delivers significant speedups for training large language models, such as for &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt; shown here.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-torchcompile&quot;&gt;[Beta] torch.compile&lt;/h3&gt;

&lt;p&gt;torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.&lt;/p&gt;

&lt;p&gt;Underpinning torch.compile are new technologies – TorchDynamo, AOTAutograd, PrimTorch and TorchInductor:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TorchDynamo captures PyTorch programs safely using Python Frame Evaluation Hooks and is a significant innovation that was a result of 5 years of our R&amp;amp;D into safe graph capture.&lt;/li&gt;
  &lt;li&gt;AOTAutograd overloads PyTorch’s autograd engine as a tracing autodiff for generating ahead-of-time backward traces.&lt;/li&gt;
  &lt;li&gt;PrimTorch canonicalizes ~2000+ PyTorch operators down to a closed set of ~250 primitive operators that developers can target to build a complete PyTorch backend. This substantially lowers the barrier of writing a PyTorch feature or backend.&lt;/li&gt;
  &lt;li&gt;TorchInductor is a deep learning compiler that generates fast code for multiple accelerators and backends. For NVIDIA and AMD GPUs, it uses OpenAI Triton as a key building block. For intel CPUs, we generate C++ code using multithreading, vectorized instructions and offloading appropriate operations to mkldnn when possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With all the new technologies, torch.compile is able to work 93% of time across 165 open-source models and runs 20% faster on average at float32 precision and 36% faster on average at AMP precision.&lt;/p&gt;

&lt;p&gt;For more information, please refer to &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;https://pytorch.org/get-started/pytorch-2.0/&lt;/a&gt; and for TorchInductor CPU with Intel &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-update-5-cpu-backend-backend-performance-update-and-deep-dive-on-key-optimizations/1117&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-pytorch-mps-backend&quot;&gt;[Beta] PyTorch MPS Backend&lt;/h3&gt;

&lt;p&gt;MPS backend provides GPU-accelerated PyTorch training on Mac platforms. This release brings improved correctness, stability, and operator coverage.&lt;/p&gt;

&lt;p&gt;MPS backend now includes support for the Top 60 most used ops, along with the most frequently requested operations by the community, bringing coverage to over 300 operators. The major focus of the release was to enable full OpInfo-based forward and gradient mode testing to address silent correctness issues. These changes have resulted in wider adoption of MPS backend by 3rd party networks such as Stable Diffusion, YoloV5, WhisperAI, along with increased coverage for Torchbench networks and Basic tutorials. We encourage developers to update to the latest macOS release to see the best performance and stability on the MPS backend.&lt;/p&gt;

&lt;p&gt;Links&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/mps.html&quot;&gt;MPS Backend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/MPS-Backend&quot;&gt;Developer information&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.apple.com/metal/pytorch/&quot;&gt;Accelerated PyTorch training on Mac&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.apple.com/documentation/metal?language=objc&quot;&gt;Metal&lt;/a&gt;, &lt;a href=&quot;https://developer.apple.com/documentation/metalperformanceshaders?language=objc&quot;&gt;Metal Performance Shaders&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://developer.apple.com/documentation/metalperformanceshadersgraph?language=objc&quot;&gt;Metal Performance Shaders Graph&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;beta-scaled-dot-product-attention-20&quot;&gt;[Beta] Scaled dot product attention 2.0&lt;/h3&gt;

&lt;p&gt;We are thrilled to announce the release of PyTorch 2.0, which introduces a powerful scaled dot product attention function as part of torch.nn.functional. This function includes multiple implementations that can be seamlessly applied depending on the input and hardware in use.&lt;/p&gt;

&lt;p&gt;In previous versions of PyTorch, you had to rely on third-party implementations and install separate packages to take advantage of memory-optimized algorithms like &lt;a href=&quot;https://github.com/HazyResearch/flash-attention&quot;&gt;FlashAttention&lt;/a&gt;. With PyTorch 2.0, all these implementations are readily available by default.&lt;/p&gt;

&lt;p&gt;These implementations include &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; from HazyResearch, Memory-Efficient Attention from the &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; project, and a native C++ implementation that is ideal for non-CUDA devices or when high-precision is required.&lt;/p&gt;

&lt;p&gt;PyTorch 2.0 will automatically select the optimal implementation for your use case, but you can also toggle them individually for finer-grained control. Additionally, the scaled dot product attention function can be used to build common transformer architecture components.&lt;/p&gt;

&lt;p&gt;Learn more with the &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention&quot;&gt;documentation&lt;/a&gt; and this &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-functorch---torchfunc&quot;&gt;[Beta] functorch -&amp;gt; torch.func&lt;/h3&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://github.com/google/jax&quot;&gt;Google JAX&lt;/a&gt;, functorch is a library that offers composable vmap (vectorization) and autodiff transforms. It enables advanced autodiff use cases that would otherwise be tricky to express in PyTorch. Examples include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ensembling.html&quot;&gt;model ensembling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/jacobians_hessians.html&quot;&gt;efficiently computing jacobians and hessians&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/per_sample_grads.html&quot;&gt;computing per-sample-gradients (or other per-sample quantities)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re excited to announce that, as the final step of upstreaming and integrating functorch into PyTorch, the functorch APIs are now available in the torch.func module. Our function transform APIs are identical to before, but we have changed how the interaction with NN modules work. Please see the &lt;a href=&quot;https://pytorch.org/docs/master/func.html&quot;&gt;docs&lt;/a&gt; and the &lt;a href=&quot;https://pytorch.org/docs/master/func.migrating.html&quot;&gt;migration guide&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Furthermore, we have &lt;a href=&quot;https://pytorch.org/docs/master/notes/extending.func.html&quot;&gt;added support for torch.autograd.Function&lt;/a&gt;: one is now able to apply function transformations (e.g. vmap, grad, jvp) over torch.autograd.Function.&lt;/p&gt;

&lt;h3 id=&quot;beta-dispatchable-collectives&quot;&gt;[Beta] Dispatchable Collectives&lt;/h3&gt;

&lt;p&gt;Dispatchable collectives is an improvement to the existing init_process_group() API which changes backend to an optional argument. For users, the main advantage of this feature is that it will allow them to write code that can run on both GPU and CPU machines without having to change the backend specification. The dispatchability feature will also make it easier for users to support both GPU and CPU collectives, as they will no longer need to specify the backend manually (e.g. “NCCL” or “GLOO”). Existing backend specifications by users will be honored and will not require change.&lt;/p&gt;

&lt;p&gt;Usage example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch.distributed.dist
…
# old
dist.init_process_group(backend=”nccl”, ...)
dist.all_reduce(...) # with CUDA tensors works
dist.all_reduce(...) # with CPU tensors does not work

# new
dist.init_process_group(...) # backend is optional
dist.all_reduce(...) # with CUDA tensors works
dist.all_reduce(...) # with CPU tensors works
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/docs/master/distributed.html#torch.distributed.init_process_group&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchset_default_device-and-torchdevice-as-context-manager&quot;&gt;[Beta] torch.set_default_device and torch.device as context manager&lt;/h3&gt;

&lt;p&gt;torch.set_default_device allows users to change the default device that factory functions in PyTorch allocate on. For example, if you torch.set_default_device(‘cuda’), a call to torch.empty(2) will allocate on CUDA (rather than on CPU). You can also use torch.device as a context manager to change the default device on a local basis. This resolves a long standing feature request from PyTorch’s initial release for a way to do this.&lt;/p&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-x86-as-the-new-default-quantization-backend-for-x86-cpu&quot;&gt;[Beta] “X86” as the new default quantization backend for x86 CPU&lt;/h3&gt;

&lt;p&gt;The new X86 quantization backend, which utilizes FBGEMM and oneDNN kernel libraries, replaces FBGEMM as the default quantization backend for x86 CPU platforms and offers improved int8 inference performance compared to the original FBGEMM backend, leveraging the strengths of both libraries, with 1.3X – 2X inference performance speedup measured on 40+ deep learning models. The new backend is functionally compatible with the original FBGEMM backend.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table: Geomean Speedup of X86 Quantization Backend vs. FBGEMM Backend&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt; 
   &lt;/td&gt;
   &lt;td&gt;1 core/instance
   &lt;/td&gt;
   &lt;td&gt;2 cores/instance
   &lt;/td&gt;
   &lt;td&gt;4 cores/instance
   &lt;/td&gt;
   &lt;td&gt;1 socket (32 cores)/instance
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz
   &lt;/td&gt;
   &lt;td&gt;1.76X
   &lt;/td&gt;
   &lt;td&gt;1.80X
   &lt;/td&gt;
   &lt;td&gt;2.04X
   &lt;/td&gt;
   &lt;td&gt;1.34X
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;By default, users on x86 platforms will utilize the x86 quantization backend and their PyTorch programs will remain unchanged when using the default backend. Alternatively, users have the option to specify “X86” as the quantization backend explicitly. Example code is shown below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from torch.ao.quantization import get_default_qconfig_mappingfrom torch.quantization.quantize_fx
import prepare_fx, convert_fx
 
# get default configuration
qconfig_mapping = get_default_qconfig_mapping()
 
# or explicitly specify the backend
# qengine = 'x86'
# torch.backends.quantized.engine = qengine
# qconfig_mapping = get_default_qconfig_mapping(qengine)
 
# construct fp32 model
model_fp32 = ...
 
# prepare
prepared_model = prepare_fx(model_fp32, qconfig_mapping, example_inputs=x)
 
# calibrate
...
 
# convert
quantized_model = convert_fx(prepared_model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Find more information: &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/83888&quot;&gt;https://github.com/pytorch/pytorch/issues/83888&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html&quot;&gt;https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-pytorch-int8-inf-with-new-x86-backend.html&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-gnn-inference-and-training-optimization-on-cpu&quot;&gt;[Beta] GNN inference and training optimization on CPU&lt;/h3&gt;

&lt;p&gt;PyTorch 2.0 includes several critical optimizations to improve GNN inference and training performance on CPU. Before 2.0, GNN models of PyG suffers from low efficiency on CPU due to lack of performance tuning for several critical kernels (scatter/gather, etc) and the lack of GNN-related sparse matrix multiplication ops. To be specific, optimizations include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;scatter_reduce: performance hotspot in Message Passing when the edge index is stored in Coordinate format (COO).&lt;/li&gt;
  &lt;li&gt;gather: backward of scatter_reduce, specially tuned for the GNN compute when the index is an expanded tensor.&lt;/li&gt;
  &lt;li&gt;torch.sparse.mm with reduce flag: performance hotspot in Message Passing when the edge index is stored in Compressed Sparse Row (CSR). Supported reduce flag of: sum, mean, amax, amin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On PyG benchmarks/examples, OGB benchmarks, a 1.12x - 4.07x performance speedup is measured (1.13.1 compared with 2.0) for single node inference and training.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;thead&gt;
  &lt;tr&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Model-Dataset&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Option&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td scope=&quot;col&quot;&gt;&lt;strong&gt;Speedup Ratio&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;5&quot;&gt;GCN-Reddit (inference)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.25x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.31x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.68x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.22x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;4&quot;&gt; 
GraphSage-ogbn-products (inference)
   &lt;/td&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.15x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512-2-64-sparse
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-sparse
   &lt;/td&gt;
   &lt;td&gt;1.33x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;full-batch-sparse
   &lt;/td&gt;
   &lt;td&gt;4.07x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-PROTEINS (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GCN-REDDIT-BINARY (training)
   &lt;/td&gt;
   &lt;td&gt;3-32
   &lt;/td&gt;
   &lt;td&gt;1.67x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;GCN-Reddit (training)
   &lt;/td&gt;
   &lt;td&gt;512-2-64-dense
   &lt;/td&gt;
   &lt;td&gt;1.20x
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1024-3-128-dense
   &lt;/td&gt;
   &lt;td&gt;1.12x
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Learn more: &lt;a href=&quot;https://www.pyg.org/ns-newsarticle-accelerating-pyg-on-intel-cpus&quot;&gt;PyG CPU Performance Optimization&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-accelerating-inference-on-cpu-with-pytorch-by-leveraging-onednn-graph&quot;&gt;[Beta] Accelerating inference on CPU with PyTorch by leveraging oneDNN Graph&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://spec.oneapi.io/onednn-graph/latest/introduction.html&quot;&gt;oneDNN Graph API&lt;/a&gt; extends &lt;a href=&quot;https://spec.oneapi.io/versions/latest/elements/oneDNN/source/index.html&quot;&gt;oneDNN&lt;/a&gt; with a flexible graph API to maximize the optimization opportunity for generating efficient code on AI hardware.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It automatically identifies the graph partitions to be accelerated via fusion.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/blob/dev-graph/doc/programming_model/ops_and_patterns.md#fusion-patterns&quot;&gt;fusion patterns&lt;/a&gt; focus on fusing compute-intensive operations such as convolution, matmul and their neighbor operations for both inference and training use cases.&lt;/li&gt;
  &lt;li&gt;Although work is ongoing to integrate oneDNN Graph with TorchDynamo as well, its integration with the PyTorch JIT Fuser attained beta status in PyTorch 2.0 for &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-float&quot;&gt;Float32&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16&lt;/a&gt; inference (on machines that support AVX512_BF16 ISA).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From a developer’s/researcher’s perspective, the usage is quite simple &amp;amp; intuitive, with the only change in code being an API invocation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Leverage oneDNN Graph, with &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.jit.trace.html&quot;&gt;JIT-tracing&lt;/a&gt;, a model is profiled with an example input.&lt;/li&gt;
  &lt;li&gt;The context manager &lt;em&gt;with torch.jit.fuser(“fuser3”):&lt;/em&gt; can also be used instead of invoking &lt;em&gt;torch.jit.enable_onednn_fusion(True)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;For accelerating &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/onednn#example-with-bfloat16&quot;&gt;BFloat16 inference&lt;/a&gt;, we rely on eager-mode AMP (Automatic Mixed Precision) support in PyTorch &amp;amp; disable JIT mode’s AMP, as both of them are currently divergent:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Assuming we have a model of the name 'model'
 
example_input = torch.rand(1, 3, 224, 224)
 
# enable oneDNN Graph
torch.jit.enable_onednn_fusion(True)
# Disable AMP for JIT
torch._C._jit_set_autocast_mode(False)
with torch.no_grad(), torch.cpu.amp.autocast():
	model = torch.jit.trace(model, (example_input))
	model = torch.jit.freeze(model)
 	# 2 warm-ups (2 for tracing/scripting with an example, 3 without an example)
	model(example_input)
	model(example_input)
 
	# speedup would be observed in subsequent runs.
	model(example_input)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Learn more &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-onednn-graph-with-torchscript-for-inference&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;distributed-api&quot;&gt;Distributed API&lt;/h3&gt;

&lt;h4 id=&quot;prototype-dtensor&quot;&gt;[Prototype] DTensor&lt;/h4&gt;

&lt;p&gt;PyTorch &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/distributed/_tensor/README.md&quot;&gt;DistributedTensor&lt;/a&gt; (DTensor) is a prototyping effort with distributed tensor primitives to allow easier distributed computation authoring in the SPMD (Single Program Multiple Devices) paradigm. The primitives are simple but powerful when used to express tensor distributions with both sharded and replicated parallelism strategies. PyTorch DTensor empowered PyTorch &lt;a href=&quot;https://pytorch.org/docs/master/distributed.tensor.parallel.html&quot;&gt;Tensor Parallelism&lt;/a&gt; along with other advanced parallelism explorations. In addition, it also offers a uniform way to save/load state_dict for distributed checkpointing purposes, even when there’re complex tensor distribution strategies such as combining tensor parallelism with parameter sharding in FSDP. More details can be found in this &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/88838&quot;&gt;RFC&lt;/a&gt; and the &lt;a href=&quot;https://colab.research.google.com/drive/12Pl5fvh0eLPUrcVO7s6yY4n2_RZo8pLR#scrollTo=stYPKb9Beq4e&quot;&gt;DTensor examples notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-tensorparallel&quot;&gt;[Prototype] TensorParallel&lt;/h4&gt;

&lt;p&gt;We now support DTensor based Tensor Parallel which users can distribute their model parameters across different GPU devices. We also support Pairwise Parallel which shards two concatenated linear layers in a col-wise and row-wise style separately so that only one collective(all-reduce/reduce-scatter) is needed in the end. More details can be found in this &lt;a href=&quot;https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/example.py&quot;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-2d-parallel&quot;&gt;[Prototype] 2D Parallel&lt;/h4&gt;

&lt;p&gt;We implemented the integration of the aforementioned TP with FullyShardedDataParallel(FSDP) as 2D parallel to further scale large model training. More details can be found in this &lt;a href=&quot;https://docs.google.com/presentation/d/17g6WqrO00rP3MsxbRENsPpjrlSkwiA_QB4r93_eB5is/edit?usp=sharing&quot;&gt;slide&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/test/distributed/tensor/parallel/test_2d_parallel.py&quot;&gt;code example&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;prototype-torchcompiledynamictrue&quot;&gt;[Prototype] torch.compile(dynamic=True)&lt;/h4&gt;

&lt;p&gt;Experimental support for PT2 compilation with dynamic shapes is available in this release. Inference compilation with inductor for simple models is supported, but there are a lot of limitations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training available in a future release (This is partially fixed in nightlies!)&lt;/li&gt;
  &lt;li&gt;Minifier available in a future release.&lt;/li&gt;
  &lt;li&gt;It is easy to end up in a situation where the dimension you wanted to be dynamic gets specialized anyway. Some of these issues are fixed in nightlies, others are not.&lt;/li&gt;
  &lt;li&gt;We do not appropriately propagate Inductor guards to the top-level, this is tracked at &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/96296&quot;&gt;#96296&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Data-dependent operations like nonzero still require a graph break.&lt;/li&gt;
  &lt;li&gt;Dynamic does not work with non-standard modes like reduce-overhead or max-autotune.&lt;/li&gt;
  &lt;li&gt;There are many bugs in Inductor compilation. To track known bugs, check the &lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+dynamic+shapes%22&quot;&gt;dynamic shapes&lt;/a&gt; label on the PyTorch issue tracker.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the latest and greatest news about dynamic shapes support on master, check out &lt;a href=&quot;https://dev-discuss.pytorch.org/t/state-of-symbolic-shapes-branch/777/43&quot;&gt;our status reports&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;highlightsperformance-improvements&quot;&gt;Highlights/Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;deprecation-of-cuda-116-and-python-37-support-for-pytorch-20&quot;&gt;&lt;a href=&quot;https://pytorch.org/blog/deprecation-cuda-python-support/&quot;&gt;Deprecation of Cuda 11.6 and Python 3.7 support&lt;/a&gt; for PyTorch 2.0&lt;/h3&gt;

&lt;p&gt;If you are still using or depending on CUDA 11.6 or Python 3.7 builds, we strongly recommend moving to at least CUDA 11.7 and Python 3.8, as it would be the minimum versions required for PyTorch 2.0. For more detail, please refer to the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/RELEASE.md#release-compatibility-matrix&quot;&gt;Release Compatibility Matrix for PyTorch&lt;/a&gt; releases.&lt;/p&gt;

&lt;h3 id=&quot;python-311-support-on-anaconda-platform&quot;&gt;Python 3.11 support on Anaconda Platform&lt;/h3&gt;

&lt;p&gt;Due to lack of Python 3.11 support for packages that PyTorch depends on, including NumPy, SciPy, SymPy, Pillow and others on the Anaconda platform. We will not be releasing Conda binaries compiled with Python 3.11 for PyTorch Release 2.0. The Pip packages with Python 3.11 support will be released, hence if you intend to use PyTorch 2.0 with Python 3.11 please use our Pip packages. Please note: Conda packages with Python 3.11 support will be made available on our nightly channel. Also we are planning on releasing Conda Python 3.11 binaries as part of future release once Anaconda provides these key dependencies. More information and instructions on how to download the Pip packages can be found &lt;a href=&quot;https://dev-discuss.pytorch.org/t/pytorch-2-0-message-concerning-python-3-11-support-on-anaconda-platform/1087&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;optimized-pytorch-inference-with-aws-graviton-processors&quot;&gt;Optimized PyTorch Inference with AWS Graviton processors&lt;/h3&gt;

&lt;p&gt;The optimizations focused on three key areas: GEMM kernels, bfloat16 support, primitive caching and the memory allocator. For aarch64 platforms, PyTorch supports Arm Compute Library (ACL) GEMM kernels via Mkldnn(OneDNN) backend. The ACL library provides Neon/SVE GEMM kernels for fp32 and bfloat16 formats. The bfloat16 support on c7g allows efficient deployment of bfloat16 trained, AMP (Automatic Mixed Precision) trained, or even the standard fp32 trained models. The standard fp32 models leverage bfloat16 kernels via OneDNN fast math mode, without any model quantization. Next we implemented primitive caching for conv, matmul and inner product operators. More information on the updated PyTorch user guide with the upcoming 2.0 release improvements and TorchBench benchmark details can be found &lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.0 which we highlighted during the PyTorch Conference on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Democratizing AI with PyTorch Foundation and ROCm™ support for PyTorch</title>
      <link href="https://pytorch.org/blog/democratizing-ai-with-pytorch/" rel="alternate" type="text/html" title="Democratizing AI with PyTorch Foundation and ROCm™ support for PyTorch" />
      <published>2023-02-14T00:00:00-08:00</published>
      <updated>2023-02-14T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/democratizing-ai-with-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/democratizing-ai-with-pytorch/">&lt;p&gt;&lt;img src=&quot;/assets/images/2023-02-14-democratizing-ai-with-pytorch-1.png&quot; alt=&quot;AMD Founding Member&quot; width=&quot;50%&quot; style=&quot;display:block; margin-left:auto; margin-right:auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Last year, Meta announced that &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; joined the Linux Foundation as a neutral home for growing the machine learning project and community with AMD representation as a part of the founding membership and governing board.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;PyTorch Foundation’s&lt;/a&gt; mission is to drive AI adoption by democratizing its software ecosystem through open source principles aligning with the AMD core principle of an Open software ecosystem. AMD strives to foster innovation through the support for latest generations of hardware, tools, libraries, and other components to simplify and accelerate adoption of AI across a broad range of scientific discoveries.&lt;/p&gt;

&lt;div class=&quot;d-md-flex&quot;&gt;
&lt;div style=&quot;flex-basis: 60%;&quot;&gt;
&lt;p&gt;
AMD, along with key PyTorch codebase developers (including those at Meta AI), delivered a set of updates to the &lt;a href=&quot;https://www.amd.com/en/graphics/servers-solutions-rocm&quot; target=&quot;_blank&quot;&gt;ROCm™&lt;/a&gt; open software ecosystem that brings stable support for &lt;a href=&quot;https://www.amd.com/en/graphics/instinct-server-accelerators&quot; target=&quot;_blank&quot;&gt;AMD Instinct™&lt;/a&gt; accelerators as well as many Radeon™ GPUs. This now gives PyTorch developers the ability to build their next great AI solutions leveraging AMD GPU accelerators &amp;amp; ROCm. The support from PyTorch community in identifying gaps, prioritizing key updates, providing feedback for performance optimizing and supporting our journey from “Beta” to “Stable” was immensely helpful and we deeply appreciate the strong collaboration between the two teams at AMD and PyTorch. The move for ROCm support from “Beta” to “Stable” came in the PyTorch 1.12 release (June 2022) brings the added support to easily run PyTorch on native environment without having to configure custom dockers. This is a sign of confidence about the quality of support and performance of PyTorch using AMD Instinct and ROCm. The results of these collaborative efforts are evident in the performance measured on key industry benchmarks like Microsoft’s SuperBench shown below in Graph 1.
&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&quot;
  border: 1px solid #d0d0d0;
  border-radius: 10px;
  box-sizing: border-box;
  -webkit-filter: drop-shadow(0 2px 5px rgba(0,0,0,.1));
  filter: drop-shadow(0 2px 5px rgba(0,0,0,.1));
  padding: 30px;
  background-color: #f8f8f8;
  margin: 20px;
  color: black;
  font-size: 1.6rem;
  flex-basis: 40%;
&quot;&gt;
&lt;p&gt;
&lt;em&gt;“We are excited to see the significant impact of developers at AMD to contribute to and extend features within PyTorch to make AI models run in a more performant, efficient, and scalable way. A great example of this is the thought-leadership around unified memory approaches between the framework and future hardware systems, and we look forward to seeing that feature progress.”&lt;/em&gt;&lt;br /&gt; 
- Soumith Chintala, PyTorch lead-maintainer and Director of Engineering, Meta AI
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The progressive improvements on both the AMD CDNA™ architecture as well as ROCm and PyTorch shows single GPU model throughput increase from AMD Instinct MI100 to the latest generation AMD Instinct MI200 family GPUs going from ROCm 4.2 to ROCm 5.3 and from PyTorch 1.7 to PyTorch 1.12.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2023-02-14-democratizing-ai-with-pytorch-2.png&quot; alt=&quot;Graph 1: ML model performance over generation using Microsoft Superbench Suite&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;Graph 1: ML model performance over generation using Microsoft Superbench Suite &lt;sup&gt;1, 2, 3&lt;/sup&gt;&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;Below are a few of the key updates for ROCm support since the PyTorch 1.12 release&lt;/p&gt;

&lt;h2 id=&quot;full-continuous-integration-ci-for-rocm-on-pytorch&quot;&gt;Full Continuous Integration (CI) for ROCm on PyTorch&lt;/h2&gt;

&lt;p&gt;With the ROCm support for PyTorch move from “Beta” to “Stable,” all the functions and features commits are now verified through a full Continuous Integration (CI) process. The CI process helps ensure the proper build and test process ahead of an expected Docker and PIP wheel release with stable commits forthcoming.&lt;/p&gt;

&lt;h2 id=&quot;support-for-kineto-profiler&quot;&gt;Support for &lt;a href=&quot;https://github.com/pytorch/kineto&quot;&gt;Kineto Profiler&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The addition of Kineto profiler support to ROCm now helps developers and users understand performance bottlenecks through effective diagnosis and profiling tools. The tool also provides recommendations to improve known issues and visualization through TensorBoard UI.&lt;/p&gt;

&lt;h2 id=&quot;key-pytorch-libraries-support-added&quot;&gt;Key PyTorch Libraries support added&lt;/h2&gt;

&lt;p&gt;PyTorch ecosystem libraries like &lt;a href=&quot;https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html&quot;&gt;TorchText&lt;/a&gt; (Text classification), &lt;a href=&quot;https://pytorch.org/torchrec/&quot;&gt;TorchRec&lt;/a&gt; (libraries for recommender systems - RecSys), &lt;a href=&quot;https://pytorch.org/vision/stable/index.html&quot;&gt;TorchVision&lt;/a&gt; (Computer Vision), &lt;a href=&quot;https://pytorch.org/audio/stable/index.html&quot;&gt;TorchAudio&lt;/a&gt; (audio and signal processing) are fully supported since ROCm 5.1 and upstreamed with PyTorch 1.12.&lt;/p&gt;

&lt;p&gt;Key libraries provided with the ROCm software stack including &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/MIOpen&quot;&gt;MIOpen&lt;/a&gt; (Convolution models), &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/rccl&quot;&gt;RCCL&lt;/a&gt; (ROCm Collective Communications) and &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/rocBLAS&quot;&gt;rocBLAS&lt;/a&gt; (BLAS for transformers) were further optimized to offer new potential efficiencies and higher performance.&lt;/p&gt;

&lt;p&gt;MIOpen innovates on several fronts, such as implementing fusion to optimize for memory bandwidth and GPU launch overheads, providing an auto-tuning infrastructure to overcome the large design space of problem configurations, and implementing different algorithms to optimize convolutions for different filter and input sizes. MIOpen is one of the first libraries to publicly support the bfloat16 data-type for convolutions, allowing efficient training at lower precision maintaining expected accuracy.&lt;/p&gt;

&lt;p&gt;RCCL (pronounced “Rickle”) is a stand-alone library of standard collective communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, reduce-scatter, gather, scatter, and all-to-all. There is support for direct GPU-to-GPU send and receive operations. It has been optimized to achieve high bandwidth on platforms using PCIe®, Infinity Fabric™ (GPU to GPU) as well as networking using InfiniBand Verbs or TCP/IP sockets. RCCL supports an arbitrary number of GPUs installed in single or multiple nodes and can be used in either single- or multi-process (e.g., MPI) applications.&lt;/p&gt;

&lt;p&gt;Along with the above key highlights, over 50 features and functionality improvements were completed jointly between AMD and PyTorch to add stable support for ROCm. These include improvements to tools, compilers, runtime, graph optimizations through TorchScript, INT8 quant path usage, and &lt;a href=&quot;https://onnxruntime.ai/&quot;&gt;ONNX runtime integration&lt;/a&gt; including support for Navi 21 based Radeon™ PRO datacenter graphics card to name a few.&lt;/p&gt;

&lt;h2 id=&quot;aitemplate-inference-engine&quot;&gt;&lt;a href=&quot;https://github.com/facebookincubator/AITemplate&quot;&gt;AITemplate&lt;/a&gt; Inference Engine&lt;/h2&gt;

&lt;p&gt;MetaAI recently published a blog announcing the release of its open source AITemplate (&lt;a href=&quot;https://ai.facebook.com/blog/gpu-inference-engine-nvidia-amd-open-source/&quot;&gt;link&lt;/a&gt;) for a unified inference system supporting AMD Instinct GPU accelerators using the AMD ROCm stack. This Python based framework can help significantly improve performance through increased utilization of AMD matrix cores for transformer blocks. This is achieved through the AMD &lt;a href=&quot;https://github.com/ROCmSoftwarePlatform/composable_kernel&quot;&gt;Composable Kernel (CK) library&lt;/a&gt; which provides performance critical Kernels for ML AI workloads across multiple architectures including GPUs and CPUs through HIP &amp;amp; C++.&lt;/p&gt;

&lt;p&gt;Moreover, the AITemplate also provides out-of-the-box support for widely used AI models like BERT, ResNET, Vision Transformer, Stable Diffusion etc. simplifying deployment process through these pretrained models.&lt;/p&gt;

&lt;h2 id=&quot;whats-coming-with-future-rocm-releases&quot;&gt;What’s coming with future ROCm releases?&lt;/h2&gt;

&lt;h3 id=&quot;unified-memory-models-for-cpu--gpu&quot;&gt;Unified memory models for CPU + GPU&lt;/h3&gt;

&lt;p&gt;As system architecture evolves to address the complexity of large problem sizes and data sets, memory management becomes a key performance bottle neck that needs a cohesive strategy to be addressed through innovations at both hardware and software levels. AMD is uniquely positioned to address this problem with its effective data center solutions integrating AMD EPYC™ CPU cores with its AMD Instinct GPU compute units in a truly unified datacenter APU (Accelerated Processing Unit) form factor set to be launched in 2H 2023.&lt;/p&gt;

&lt;p&gt;The software work to leverage the unified CPU + GPU memory has already started in collaboration with the PyTorch team, to enable the usage of a fast, low latency, synchronized memory model that enables not only AMD but also other AI accelerators to address the complex memory management problem of today. We are looking forward to this joint effort and announcement soon.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The content in this blog highlights the joint work between AMD and key PyTorch contributors including Meta, working on many of the core features, as well as Microsoft enabling ONNX Runtime support. We are looking forward to working with the other founding members at the PyTorch Foundation on the next steps and improvements to democratize and grow adoption of PyTorch across the industry.&lt;/p&gt;

&lt;h2 id=&quot;cautionary-statement&quot;&gt;CAUTIONARY STATEMENT&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;
This blog contains forward-looking statements concerning Advanced Micro Devices, Inc. (AMD) such as the availability, timing and expected benefits of an AMD datacenter APU form factor, which are made pursuant to the Safe Harbor provisions of the Private Securities Litigation Reform Act of 1995. Forward-looking statements are commonly identified by words such as “would,” “may,” “expects,” “believes,” “plans,” “intends,” “projects” and other terms with similar meaning. Investors are cautioned that the forward-looking statements in this blog are based on current beliefs, assumptions and expectations, speak only as of the date of this blog and involve risks and uncertainties that could cause actual results to differ materially from current expectations. Such statements are subject to certain known and unknown risks and uncertainties, many of which are difficult to predict and generally beyond AMD’s control, that could cause actual results and other future events to differ materially from those expressed in, or implied or projected by, the forward-looking information and statements. Investors are urged to review in detail the risks and uncertainties in AMD’s Securities and Exchange Commission filings, including but not limited to AMD’s most recent reports on Forms 10-K and 10-Q. AMD does not assume, and hereby disclaims, any obligation to update forward-looking statements made in this blog, except as may be required by law. 
&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;endnotes&quot;&gt;Endnotes&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;small&gt;MI100D-01 SuperBench v0.5 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™ 7763 CPU server tested with 1x AMD Instinct™ MI100 (32GB HBM2e) 300W GPU, SBIOS 2.2, Ubuntu® 20.04.5 LTS, host ROCm™ 5.2.0, guest ROCm 4.2,    PyTorch 1.7.0. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
  &lt;li&gt;&lt;small&gt;MI200D-01 SuperBench v0.6 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™ 7763 CPU server tested with 1x AMD Instinct™ MI210 (64GB HBM2e) 300W GPU, SBIOS 2.2, Ubuntu 20.04.5 LTS, host ROCm 5.3.0, guest ROCm 5.3, PyTorch 1.12. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
  &lt;li&gt;&lt;small&gt;MI200D-02: SuperBench v0.6 model training results based on AMD internal testing as of 11/09/2022 measuring the total training throughput, at half precision, using a 2P AMD EPYC™️ 7763 CPU server tested with 1x AMD Instinct™️ MI250 (128GB HBM2e) 560W GPU, SBIOS M12, Ubuntu 20.04 LTS, host ROCm 5.3.0, guest ROCm 5.3, PyTorch 1.12. Server manufacturers may vary configurations, yielding different results. Performance may vary based factors including use of latest drivers and optimizations.&lt;/small&gt;&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>AMD</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deprecation of CUDA 11.6 and Python 3.7 Support</title>
      <link href="https://pytorch.org/blog/deprecation-cuda-python-support/" rel="alternate" type="text/html" title="Deprecation of CUDA 11.6 and Python 3.7 Support" />
      <published>2023-02-02T00:00:00-08:00</published>
      <updated>2023-02-02T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/deprecation-cuda-python-support</id>
      <content type="html" xml:base="https://pytorch.org/blog/deprecation-cuda-python-support/">&lt;p&gt;For the upcoming PyTorch 2.0 feature release (target March 2023), we will target CUDA 11.7 as the stable version and CUDA 11.8 as the experimental version of CUDA and Python &amp;gt;=3.8, &amp;lt;=3.11.&lt;/p&gt;

&lt;p&gt;If you are still using or depending on CUDA 11.6 or Python 3.7 builds, we strongly recommend moving to at least CUDA 11.7 and Python 3.8, as it would be the minimum versions required for PyTorch 2.0.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Please note that as of Feb 1, CUDA 11.6 and Python 3.7  are no longer included in the nightlies&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please refer to the Release Compatibility Matrix for PyTorch releases:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;PyTorch Version&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Python&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Stable CUDA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Experimental CUDA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2.0
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.8, &amp;lt;=3.11
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.7, CUDNN 8.5.0.96
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.8, CUDNN 8.7.0.84
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.13
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.7, &amp;lt;=3.10
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.6, CUDNN 8.3.2.44
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.7, CUDNN 8.5.0.96
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1.12
   &lt;/td&gt;
   &lt;td&gt;&amp;gt;=3.7, &amp;lt;=3.10
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.3, CUDNN 8.3.2.44
   &lt;/td&gt;
   &lt;td&gt;CUDA 11.6, CUDNN 8.3.2.44
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As of 2/1/2023&lt;/p&gt;

&lt;p&gt;For more information on PyTorch releases, updated compatibility matrix and release policies, please see (and bookmark) &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/RELEASE.md#release-compatibility-matrix&quot;&gt;Readme&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">For the upcoming PyTorch 2.0 feature release (target March 2023), we will target CUDA 11.7 as the stable version and CUDA 11.8 as the experimental version of CUDA and Python &amp;gt;=3.8, &amp;lt;=3.11.</summary>
      

      
      
    </entry>
  
</feed>


