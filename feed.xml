<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2022-09-07T17:46:12-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Fast Beam Search Decoding in PyTorch with TorchAudio and Flashlight Text</title>
      <link href="https://pytorch.org/blog/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text/" rel="alternate" type="text/html" title="Fast Beam Search Decoding in PyTorch with TorchAudio and Flashlight Text" />
      <published>2022-08-29T00:00:00-07:00</published>
      <updated>2022-08-29T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text</id>
      <content type="html" xml:base="https://pytorch.org/blog/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text/">&lt;p&gt;Beam search decoding with industry-leading speed from &lt;a href=&quot;https://github.com/flashlight/text&quot;&gt;Flashlight Text&lt;/a&gt; (part of the &lt;a href=&quot;https://arxiv.org/abs/2201.12465&quot;&gt;Flashlight&lt;/a&gt; ML framework) is now available with official support in &lt;a href=&quot;https://pytorch.org/audio/0.12.0/models.decoder.html#ctcdecoder&quot;&gt;TorchAudio&lt;/a&gt;, bringing high-performance beam search and text utilities for speech and text applications built on top of PyTorch. The current integration supports CTC-style decoding, but it can be used for &lt;em&gt;any modeling setting that outputs token-level probability distributions over time steps&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-brief-beam-search-refresher&quot;&gt;A brief beam search refresher&lt;/h2&gt;

&lt;p&gt;In speech and language settings, &lt;em&gt;beam search&lt;/em&gt; is an efficient, greedy algorithm that can convert sequences of &lt;em&gt;continuous values&lt;/em&gt; (i.e. probabilities or scores) into &lt;em&gt;graphs&lt;/em&gt; or &lt;em&gt;sequences&lt;/em&gt; (i.e. tokens, word-pieces, words) using &lt;em&gt;optional constraints&lt;/em&gt; on valid sequences (i.e. a lexicon), &lt;em&gt;optional external scoring&lt;/em&gt; (i.e. an LM which scores valid sequences), and other &lt;em&gt;score adjustments&lt;/em&gt; for particular sequences.&lt;/p&gt;

&lt;p&gt;In the example that follows, we‚Äôll consider ‚Äî a token set of {œµ, a, b}, where œµ is a special token that we can imagine denotes a space between words or a pause in speech. Graphics here and below are taken from Awni Hannun‚Äôs excellent &lt;a href=&quot;https://distill.pub/2017/ctc/&quot;&gt;distill.pub writeup&lt;/a&gt; on CTC and beam search.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-1.jpeg&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;With a greedy-like approach, beam search considers the next viable token given an existing sequence of tokens ‚Äî in the example above, a, b, b is a valid sequence, but a, b, a is not. We &lt;em&gt;rank&lt;/em&gt; each possible next token at each step of the beam search according to a scoring function. Scoring functions (s) typically looks something like:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-2.jpeg&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Where &lt;strong&gt;≈∑&lt;/strong&gt; is a potential path/sequence of tokens, &lt;strong&gt;x&lt;/strong&gt; is the input &lt;em&gt;&lt;strong&gt;(P(≈∑|x)&lt;/strong&gt;&lt;/em&gt; represents the model‚Äôs predictions over time), and ùõº is a weight on the language model probability &lt;em&gt;&lt;strong&gt;(P(y)&lt;/strong&gt;&lt;/em&gt; the probability of the sequence under the language model). Some scoring functions add &lt;em&gt;&lt;strong&gt;ùú∑&lt;/strong&gt;&lt;/em&gt; which adjusts a score based on the length of the predicted sequence &lt;strong&gt;|≈∑|&lt;/strong&gt;. This particular scoring function is used in &lt;a href=&quot;https://arxiv.org/pdf/1911.08460.pdf&quot;&gt;FAIR‚Äôs prior work&lt;/a&gt; on end-to-end ASR, and there are many variations on scoring functions which can vary across application areas.&lt;/p&gt;

&lt;p&gt;Given a particular sequence, to assess the next viable token in that sequence (perhaps constrained by a set of allowed words or sequences, such as a lexicon of words), the beam search algorithm scores the sequence with each candidate token added, and sorts token candidates based on those scores. For efficiency and since the number of paths is exponential in the token set size, the &lt;em&gt;&lt;strong&gt;top-k&lt;/strong&gt;&lt;/em&gt; highest-scoring candidates are kept ‚Äî &lt;em&gt;&lt;strong&gt;k&lt;/strong&gt;&lt;/em&gt; represents the &lt;em&gt;&lt;strong&gt;beam size&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-3.jpeg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;There are many other nuances with how beam search can progress: similar hypothesis sequences can be ‚Äúmerged‚Äù, for instance.
&lt;/p&gt;

&lt;p&gt;The scoring function can be further augmented to up/down-weight token insertion or long or short words. Scoring with &lt;em&gt;stronger external language&lt;/em&gt; models, while incurring computational cost, can also significantly improve performance; this is frequently referred to as &lt;em&gt;LM fusion&lt;/em&gt;. There are many other knobs to tune for decoding ‚Äî these are documented in &lt;a href=&quot;https://pytorch.org/audio/0.12.0/models.decoder.html#ctcdecoder&quot;&gt;TorchAudio‚Äôs documentation&lt;/a&gt; and explored further in &lt;a href=&quot;https://pytorch.org/audio/0.12.0/tutorials/asr_inference_with_ctc_decoder_tutorial.html#beam-search-decoder-parameters&quot;&gt;TorchAudio‚Äôs ASR Inference tutorial&lt;/a&gt;. Since decoding is quite efficient, parameters can be easily swept and tuned.&lt;/p&gt;

&lt;p&gt;Beam search has been used in ASR extensively over the years in far too many works to cite, and in strong, recent results and systems including &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf&quot;&gt;wav2vec 2.0&lt;/a&gt; and &lt;a href=&quot;https://developer.nvidia.com/nvidia-nemo&quot;&gt;NVIDIA‚Äôs NeMo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-beam-search&quot;&gt;Why beam search?&lt;/h2&gt;

&lt;p&gt;Beam search remains a fast competitor to heavier-weight decoding approaches such as &lt;a href=&quot;https://arxiv.org/pdf/1211.3711.pdf&quot;&gt;RNN-Transducer&lt;/a&gt; that Google has invested in putting &lt;a href=&quot;https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html&quot;&gt;on-device&lt;/a&gt; and has shown strong results with on &lt;a href=&quot;https://arxiv.org/pdf/2010.10504.pdf&quot;&gt;common benchmarks&lt;/a&gt;. Autoregressive text models at scale can benefit from beam search as well. Among other things, beam search gives:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A flexible performance/latency tradeoff ‚Äî by adjusting beam size and the external LM, users can sacrifice latency for accuracy or pay for more accurate results with a small latency cost. Decoding with no external LM can improve results at very little performance cost.&lt;/li&gt;
  &lt;li&gt;Portability without retraining ‚Äî existing neural models can benefit from multiple decoding setups and plug-and-play with external LMs without training or fine-tuning.&lt;/li&gt;
  &lt;li&gt;A compelling complexity/accuracy tradeoff ‚Äî adding beam search to an existing modeling pipeline incurs little additional complexity and can improve performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-benchmarks&quot;&gt;Performance Benchmarks&lt;/h2&gt;

&lt;p&gt;Today‚Äôs most commonly-used beam search decoding libraries today that support external language model integration include Kensho‚Äôs &lt;a href=&quot;https://github.com/kensho-technologies/pyctcdecode&quot;&gt;pyctcdecode&lt;/a&gt;, NVIDIA‚Äôs &lt;a href=&quot;https://github.com/NVIDIA/NeMo/tree/stable/scripts/asr_language_modeling&quot;&gt;NeMo toolkit&lt;/a&gt;. We benchmark the TorchAudio + Flashlight decoder against them with a &lt;em&gt;wav2vec 2.0&lt;/em&gt; base model trained on 100 hours of audio evaluated on &lt;a href=&quot;https://www.openslr.org/12&quot;&gt;LibriSpeech&lt;/a&gt; dev-other with the official &lt;a href=&quot;https://github.com/kpu/kenlm/&quot;&gt;KenLM&lt;/a&gt; 3-gram LM. Benchmarks were run on Intel E5-2698 CPUs on a single thread. All computation was in-memory ‚Äî KenLM memory mapping was disabled as it wasn‚Äôt widely supported.&lt;/p&gt;

&lt;p&gt;When benchmarking, we measure the &lt;em&gt;time-to-WER (word error rate)&lt;/em&gt; ‚Äî because of subtle differences in the implementation of decoding algorithms and the complex relationships between parameters and decoding speed, some hyperparameters differed across runs. To fairly assess performance, we first sweep for parameters that achieve a baseline WER, minimizing beam size if possible.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-4.jpeg&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Decoding performance on Librispeech dev-other of a pretrained wav2vec 2.0 model. TorchAudio + Flashlight decoding outperforms by an order of magnitude at low WERs.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-5.jpeg&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Time-to-WER results, deferring to smaller beam size, across decoders. The TorchAudio + Flashlight decoder scales far better with larger beam sizes and at lower WERs.
&lt;/p&gt;

&lt;h2 id=&quot;torchaudio-api-and-usage&quot;&gt;TorchAudio API and Usage&lt;/h2&gt;

&lt;p&gt;TorchAudio provides a Python API for CTC beam search decoding, with support for the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lexicon and lexicon-free decoding&lt;/li&gt;
  &lt;li&gt;KenLM n-gram language model integration&lt;/li&gt;
  &lt;li&gt;character and word-piece decoding&lt;/li&gt;
  &lt;li&gt;sample pretrained LibriSpeech KenLM models and corresponding lexicon and token files&lt;/li&gt;
  &lt;li&gt;various customizable beam search parameters (beam size, pruning threshold, LM weight‚Ä¶)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To set up the decoder, use the factory function torchaudio.models.decoder.ctc_decoder&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchaudio.models.decoder&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctc_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;download_pretrained_files&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;download_pretrained_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;librispeech-4-gram&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctc_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;lexicon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lexicon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;nbest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;additional&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optional&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;customizable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Given emissions of shape &lt;em&gt;(batch, time, num_tokens)&lt;/em&gt;, the decoder will compute and return a List of batch Lists, each consisting of the nbest hypotheses corresponding to the emissions. Each hypothesis can be further broken down into tokens, words (if a lexicon is provided), score, and timesteps components.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;emissions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acoustic_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waveforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (B, T, N)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_hypotheses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emissions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# List[List[CTCHypothesis]]
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# transcript for a lexicon decoder
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transcripts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hypo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypo&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_hypotheses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# transcript for a lexicon free decoder, splitting by sil token
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idxs_to_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hypo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypo&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_hypotheses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transcripts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please refer to the &lt;a href=&quot;https://pytorch.org/audio/stable/models.decoder.html#ctcdecoder&quot;&gt;documentation&lt;/a&gt; for more API details, and the tutorial (&lt;a href=&quot;https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html&quot;&gt;ASR Inference Decoding&lt;/a&gt;) or sample &lt;a href=&quot;https://github.com/pytorch/audio/tree/main/examples/asr/librispeech_ctc_decoder&quot;&gt;inference script&lt;/a&gt; for more usage examples.&lt;/p&gt;

&lt;h2 id=&quot;upcoming-improvements&quot;&gt;Upcoming Improvements&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Full NNLM support&lt;/strong&gt; ‚Äî decoding with large neural language models (e.g. transformers) remains somewhat unexplored at scale. Already supported in Flashlight, we plan to add support in TorchAudio, allowing users to use custom decoder-compatible LMs. Custom word level language models are already available in the nightly TorchAudio build, and is slated to be released in TorchAudio 0.13.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Autoregressive/seq2seq decoding&lt;/strong&gt; ‚Äî Flashlight Text also supports &lt;a href=&quot;https://github.com/flashlight/text/blob/main/flashlight/lib/text/decoder/LexiconSeq2SeqDecoder.h&quot;&gt;sequence-to-sequence (seq2seq) decoding&lt;/a&gt; for autoregressive models, which we hope to add bindings for and add to TorchAudio and TorchText with efficient GPU implementations as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Better build support&lt;/strong&gt; ‚Äî to benefit from improvements in Flashlight Text, TorchAudio will directly submodule Flashlight Text to make upstreaming modifications and improvements easier. This is already in effect in the nightly TorchAudio build, and is slated to be released in TorchAudio 0.13.&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;

&lt;p&gt;To cite the decoder, please use the following:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inproceedings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kahn2022flashlight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flashlight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Enabling&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;innovation&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;machine&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Kahn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Jacob&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pratap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vineel&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Likhomanenko&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tatiana&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Qiantong&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Hannun&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Awni&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Cai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Jeff&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tomasello&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Paden&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lee&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ann&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Grave&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Edouard&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Avidov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Gilad&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;others&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;booktitle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;International&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Conference&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Machine&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Learning&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10557&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10574&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2022&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;organization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PMLR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inproceedings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yang2022torchaudio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Torchaudio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Building&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blocks&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;audio&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speech&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;processing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Yang&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Yao&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Yuan&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Hira&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Moto&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ni&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Zhaoheng&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Astafurov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Artyom&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Chen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Caroline&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Puhrsch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Christian&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pollack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;David&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Genzel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dmitriy&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Greenberg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Donny&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Yang&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Edward&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;others&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;booktitle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ICASSP&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2022&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2022&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IEEE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;International&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Conference&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Acoustics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Speech&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Signal&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Processing&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ICASSP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6982&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6986&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2022&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;organization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IEEE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Caroline Chen, Jacob Kahn (@jacob_d_kahn)</name>
        
        
      </author>

      

      

      
        <summary type="html">Beam search decoding with industry-leading speed from Flashlight Text (part of the Flashlight ML framework) is now available with official support in TorchAudio, bringing high-performance beam search and text utilities for speech and text applications built on top of PyTorch. The current integration supports CTC-style decoding, but it can be used for any modeling setting that outputs token-level probability distributions over time steps.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing nvFuser, a deep learning compiler for PyTorch</title>
      <link href="https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/" rel="alternate" type="text/html" title="Introducing nvFuser, a deep learning compiler for PyTorch" />
      <published>2022-08-26T00:00:00-07:00</published>
      <updated>2022-08-26T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-nvfuser-a-deep-learning-compiler-for-pytorch/">&lt;p&gt;nvFuser is a Deep Learning Compiler for NVIDIA GPUs that automatically just-in-time compiles fast and flexible kernels to reliably accelerate users‚Äô networks. It provides significant speedups for deep learning networks running on Volta and later CUDA accelerators by generating fast custom ‚Äúfusion‚Äù kernels at runtime. nvFuser is specifically designed to meet the unique requirements of the PyTorch community, and it supports diverse network architectures and programs with dynamic inputs of varying shapes and strides.
In this blog post we‚Äôll describe nvFuser and how it‚Äôs used today, show the significant performance improvements it can obtain on models from HuggingFace and TIMM, and look ahead to nvFuser in PyTorch 1.13 and beyond. If you would like to know more about how and why fusion improves the speed of training for Deep Learning networks, please see our previous talks on nvFuser from &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41958/&quot;&gt;GTC 2022&lt;/a&gt; and &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31952/&quot;&gt;GTC 2021&lt;/a&gt;.
nvFuser relies on a graph representation of PyTorch operations to optimize and accelerate. Since PyTorch has an eager execution model, the PyTorch operations users are running are not directly accessible as a whole program that can be optimized by a system like nvFuser. Therefore users must utilize systems built on top of nvFuser which are capable of capturing users programs and translating them into a form that is optimizable by nvFuser. These higher level systems then pass these captured operations to nvFuser, so that nvFuser can optimize the execution of the user‚Äôs script for NVIDIA GPUs. There are three systems that capture, translate, and pass user programs to nvFuser for optimization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script&quot;&gt;TorchScript jit.script&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;This system directly parses sections of an annotated python script to translate into its own representation what the user is doing. This system then applies its own version of auto differentiation to the graph, and passes sections of the subsequent forward and backwards graphs to nvFuser for optimization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/functorch/stable/generated/functorch.compile.memory_efficient_fusion.html#functorch.compile.memory_efficient_fusion&quot;&gt;FuncTorch&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;This system doesn‚Äôt directly look at the user python script, instead inserting a mechanism that captures PyTorch operations as they‚Äôre being run. We refer to this type of capture system as ‚Äútrace program acquisition‚Äù, since we‚Äôre tracing what has been performed. FuncTorch doesn‚Äôt perform its own auto differentiation ‚Äì it simply traces PyTorch‚Äôs autograd directly to get backward graphs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/torchdynamo&quot;&gt;TorchDynamo&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;TorchDynamo is another program acquisition mechanism built on top of FuncTorch. TorchDynamo parses the Python bytecode produced from the user script in order to select portions to trace with FuncTorch. The benefit of TorchDynamo is that it‚Äôs able to apply decorators to a user‚Äôs script, effectively isolating what should be sent to FuncTorch, making it easier for FuncTorch to successfully trace complex Python scripts.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These systems are available for users to interact with directly while nvFuser automatically and seamlessly optimizes performance critical regions of the user‚Äôs code. These systems automatically send parsed user programs to nvFuser so nvFuser can:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Analyze the operations being run on GPUs&lt;/li&gt;
  &lt;li&gt;Plan parallelization and optimization strategies for those operations&lt;/li&gt;
  &lt;li&gt;Apply those strategies in generated GPU code&lt;/li&gt;
  &lt;li&gt;Runtime-compile the generated optimized GPU functions&lt;/li&gt;
  &lt;li&gt;Execute those CUDA kernels on subsequent iterations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is important to note nvFuser does not yet support all PyTorch operations, and there are still some scenarios that are actively being improved in nvFuser that are discussed herein. However, nvFuser does support many DL performance critical operations today, and the number of supported operations will grow in subsequent PyTorch releases. nvFuser is capable of generating highly specialized and optimized GPU functions for the operations it does have support for. This means nvFuser is able to power new PyTorch systems like TorchDynamo and FuncTorch to combine the flexibility PyTorch is known for with unbeatable performance.&lt;/p&gt;

&lt;h2 id=&quot;nvfuser-performance&quot;&gt;nvFuser Performance&lt;/h2&gt;

&lt;p&gt;Before getting into how to use nvFuser, in this section we‚Äôll show the improvements in training speed nvFuser provides for a variety of models from the &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;HuggingFace Transformers&lt;/a&gt; and &lt;a href=&quot;https://github.com/rwightman/pytorch-image-models&quot;&gt;PyTorch Image Models (TIMM)&lt;/a&gt; repositories and we will discuss current gaps in nvFuser performance that are under development today. All performance numbers in this section were taken using an NVIDIA A100 40GB GPU, and used either FuncTorch alone or Functorch with TorchDynamo.&lt;/p&gt;

&lt;h2 id=&quot;huggingface-transformer-benchmarks&quot;&gt;HuggingFace Transformer Benchmarks&lt;/h2&gt;

&lt;p&gt;nvFuser can dramatically accelerate training of HuggingFace Transformers when combined with another important optimization (more on that in a moment). Performance improvements can be seen in Figure 1 to range between 1.12x and 1.50x across a subset of popular HuggingFace Transformer networks.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/introducing-nvfuser-a-deep-learning-compiler-for-pytorch-1.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 1: Performance gains of 8 training scenarios from HuggingFace‚Äôs Transformer repository. First performance boost in the dark green is due to replacing the optimizer with an NVIDIA Apex fused AdamW optimizer. The light green is due to adding nvFuser. Models were run with batch size and sequence lengths of [64, 128], [8, 512], [2, 1024], [64, 128], [8, 512], [8, src_seql=512, tgt_seql=128], [8, src_seql=1024, tgt_seql=128], and [8, 512] respectively. All networks were run with Automatic Mixed Precision (AMP) enabled with dtype=float16.
&lt;/p&gt;

&lt;p&gt;While these speedups are significant, it‚Äôs important to understand that nvFuser doesn‚Äôt (yet) automate everything about running networks quickly. For HuggingFace Transformers, for example, it was important to use the AdamW fused optimizer from &lt;a href=&quot;https://github.com/NVIDIA/apex&quot;&gt;NVIDIA‚Äôs Apex repository&lt;/a&gt; as the optimizer otherwise consumed a large portion of runtime. Using the fused AdamW optimizer to make the network faster exposes the next major performance bottleneck ‚Äî memory bound operations. These operations are optimized by nvFuser, providing another large performance boost. With the fused optimizer and nvFuser enabled, the training speed of these networks improved between 1.12x to 1.5x.
HuggingFace Transformer models were run with &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;the torch.amp module&lt;/a&gt;. (‚Äúamp‚Äù stands for Automated Mixed Precision, see the &lt;a href=&quot;https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/&quot;&gt;‚ÄúWhat Every User Should Know about Mixed Precision in PyTorch‚Äù&lt;/a&gt; blog post for details.) An option to use nvFuser was added to HuggingFace‚ÄôsTrainer. If you have &lt;a href=&quot;https://github.com/pytorch/torchdynamo#requirements-and-setup&quot;&gt;TorchDynamo installed&lt;/a&gt; you can activate it to enable nvFuser in HuggingFace by passing &lt;em&gt;torchdynamo = ‚Äònvfuser‚Äô&lt;/em&gt; to the Trainer class.
nvFuser has great support for normalization kernels and related fusions frequently found in Natural Language Processing (NLP) models, and it is recommended users try nvFuser in their NLP workloads.&lt;/p&gt;

&lt;h2 id=&quot;pytorch-image-models-timm-benchmarks&quot;&gt;PyTorch Image Models (TIMM) Benchmarks&lt;/h2&gt;
&lt;p&gt;nvFuser, can also significantly reduce the training time of TIMM networks, up to over 1.3x vs. eager PyTorch, and up to 1.44x vs. eager PyTorch when combined with the torch.amp module. Figure 1 shows nvFuser‚Äôs speedup without torch.amp, and when torch.amp is used with the NHWC (‚Äúchannels last‚Äù) and NCHW (‚Äúchannels first‚Äù) formats. nvFuser is integrated in TIMM through FuncTorch tracing directly (without TorchDynamo) and can be used by adding the &lt;a href=&quot;https://github.com/rwightman/pytorch-image-models/commit/ca991c1fa57373286b9876aa63370fd19f5d6032&quot;&gt;‚Äìaot-autograd command line argument&lt;/a&gt; when running the TIMM benchmark or training script.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/introducing-nvfuser-a-deep-learning-compiler-for-pytorch-2.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 1: The Y-axis is the performance gain nvFuser provides over not using nvFuser. A value of 1.0 means no change in perf, 2.0 would mean nvFuser is twice as fast, 0.5 would mean nvFuser takes twice the time to run. Square markers are with float16 Automatic Mixed Precision (AMP) and channels first contiguous inputs, circle markers are float32 inputs, and triangles are with float16 AMP and channels last contiguous inputs. Missing data points are due to an error being encountered when tracing.
&lt;/p&gt;

&lt;p&gt;When running with float32 precision nvFuser provides a 1.12x geometric mean (‚Äúgeomean‚Äù) speedup on TIMM networks, and when running with torch.amp and ‚Äúchannels first‚Äù it provides a 1.14x geomean speedup. However, nvFuser currently doesn‚Äôt speedup torch.amp and ‚Äúchannels last‚Äù training (a .9x geomean regression), so we recommend not using it in those cases. We are actively working on improving ‚Äúchannels last‚Äù performance now, and soon we will have two additional optimization strategies (grid persistent optimizations for channels-last normalizations and fast transposes) which we expect will provide speedups comparable to ‚Äúchannels first‚Äù in PyTorch version 1.13 and later. Many of nvFuser‚Äôs optimizations can also help in inference cases. However, in PyTorch when running inference on small batch sizes, the performance is typically limited by CPU overhead, which nvFuser can‚Äôt completely remove or fix. Therefore, typically the most important optimization for inference is to enable &lt;a href=&quot;https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/&quot;&gt;CUDA Graphs&lt;/a&gt; when possible. Once CUDA Graphs is enabled, then it can also be beneficial to also enable fusion through nvFuser. Performance of inference is shown in Figure 2 and Figure 3. Inference is only run with float16 AMP as it is uncommon to run inference workloads in full float32 precision.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/introducing-nvfuser-a-deep-learning-compiler-for-pytorch-3.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/introducing-nvfuser-a-deep-learning-compiler-for-pytorch-4.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 2: Performance gains of enabling CUDA Graphs, and CUDA Graphs with nvFuser compared to the performance of native PyTorch without CUDA Graphs and nvFuser across TIMM models with float16 AMP, &lt;b&gt;channels first inputs&lt;/b&gt;, and a batch size of 1 and 8 respectively. There is a geomean speedup of 2.74x with CUDA Graphs and 2.71x with CUDA Graphs + nvFuser respectively. nvFuser provides a maximum regression of 0.68x and a maximum performance gain of 2.74x (relative to CUDA Graphs without nvFuser). Performance gain is measured relative to the average time per iteration PyTorch takes without CUDA Graphs and without nvFuser. Models are sorted by how much additional performance nvFuser is providing.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/introducing-nvfuser-a-deep-learning-compiler-for-pytorch-5.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/introducing-nvfuser-a-deep-learning-compiler-for-pytorch-6.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 3: Performance gains of enabling CUDA Graphs, and CUDA Graphs with nvFuser compared to the performance of native PyTorch without CUDA Graphs and nvFuser across TIMM models with AMP, &lt;b&gt;channels last inputs&lt;/b&gt;, and a batch size of 1 and 8 respectively. There is a geomean speedup of 2.29x with CUDA Graphs and 2.95x with CUDA Graphs + nvFuser respectively. nvFuser provides a maximum regression of 0.86x and a maximum performance gain of 3.82x (relative to CUDA Graphs without nvFuser). Performance gain is measured relative to the average time per iteration PyTorch takes without CUDA Graphs and without nvFuser. Models are sorted by how much additional performance nvFuser is providing.
&lt;/p&gt;

&lt;p&gt;So far nvFuser performance has not been tuned for inference workloads so its performance benefit is not consistent across all cases. However, there are still many models that benefit significantly from nvFuser during inference and we encourage users to try nvFuser in inference workloads to see if you would benefit today. Performance of nvFuser in inference workloads will improve in the future and if you‚Äôre interested in nvFuser in inference workloads please reach out to us on the PyTorch forums.&lt;/p&gt;

&lt;h2 id=&quot;getting-started---accelerate-your-scripts-with-nvfuser&quot;&gt;Getting Started - Accelerate Your Scripts with nvFuser&lt;/h2&gt;

&lt;p&gt;We‚Äôve created &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/nvfuser_intro_tutorial.html&quot;&gt;a tutorial&lt;/a&gt; demonstrating how to take advantage of nvFuser to accelerate part of a standard transformer block, and how nvFuser can be used to define fast and novel operations. There are still some rough edges in nvFuser that we‚Äôre working hard on improving as we‚Äôve outlined in this blog post. However we‚Äôve also demonstrated some great improvements for training speed on multiple networks in HuggingFace and TIMM and we expect there are opportunities in your networks where nvFuser can help today, and many more opportunities it will help in the future.
If you would like to learn more about nvFuser we recommend watching our presentations from NVIDIA‚Äôs GTC conference &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41958/&quot;&gt;GTC 2022&lt;/a&gt; and &lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31952/&quot;&gt;GTC 2021&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Christian Sarofeen, Piotr Bialecki, Jie Jiang, Kevin Stephano, Masaki Kozuki, Neal Vaidya, Stas Bekman</name>
        
        
      </author>

      

      

      
        <summary type="html">nvFuser is a Deep Learning Compiler for NVIDIA GPUs that automatically just-in-time compiles fast and flexible kernels to reliably accelerate users‚Äô networks. It provides significant speedups for deep learning networks running on Volta and later CUDA accelerators by generating fast custom ‚Äúfusion‚Äù kernels at runtime. nvFuser is specifically designed to meet the unique requirements of the PyTorch community, and it supports diverse network architectures and programs with dynamic inputs of varying shapes and strides. In this blog post we‚Äôll describe nvFuser and how it‚Äôs used today, show the significant performance improvements it can obtain on models from HuggingFace and TIMM, and look ahead to nvFuser in PyTorch 1.13 and beyond. If you would like to know more about how and why fusion improves the speed of training for Deep Learning networks, please see our previous talks on nvFuser from GTC 2022 and GTC 2021. nvFuser relies on a graph representation of PyTorch operations to optimize and accelerate. Since PyTorch has an eager execution model, the PyTorch operations users are running are not directly accessible as a whole program that can be optimized by a system like nvFuser. Therefore users must utilize systems built on top of nvFuser which are capable of capturing users programs and translating them into a form that is optimizable by nvFuser. These higher level systems then pass these captured operations to nvFuser, so that nvFuser can optimize the execution of the user‚Äôs script for NVIDIA GPUs. There are three systems that capture, translate, and pass user programs to nvFuser for optimization:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating PyTorch Vision Models with Channels Last on CPU</title>
      <link href="https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/" rel="alternate" type="text/html" title="Accelerating PyTorch Vision Models with Channels Last on CPU" />
      <published>2022-08-24T00:00:00-07:00</published>
      <updated>2022-08-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Memory formats has significant impact on performance when running vision models, generally Channels Last is a more favorable from performance perspective due to better data locality.&lt;/p&gt;

&lt;p&gt;This blog will introduce fundamental concepts of memory formats and demonstrate performance benefits using Channels Last on popular PyTorch vision models on Intel¬Æ Xeon¬Æ Scalable processors.&lt;/p&gt;

&lt;h2 id=&quot;memory-formats-introduction&quot;&gt;Memory Formats Introduction&lt;/h2&gt;

&lt;p&gt;Memory format refers to data representation that describes how a multidimensional (nD) array is stored in linear (1D) memory address space. The concept of memory format has two aspects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Physical Order&lt;/strong&gt; is the layout of data storage in physical memory. For vision models, usually we talk about NCHW, NHWC. These are the descriptions of physical memory layout, also referred as Channels First and Channels Last respectively.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Logical Order&lt;/strong&gt; is a convention on how to describe tensor shape and stride. In PyTorch, this convention is NCHW. No matter what the physical order is, tensor shape and stride will always be depicted in the order of NCHW.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fig-1 is the physical memory layout of a tensor with shape of [1, 3, 4, 4] on both Channels First and Channels Last memory format (channels denoted as R, G, B respectively):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/accelerating-pytorch-vision-models-with-channels-last-on-cpu-1.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Fig-1 Physical memory layout of Channels First and Channels Last&lt;/b&gt;
&lt;/p&gt;

&lt;h2 id=&quot;memory-formats-propagation&quot;&gt;Memory Formats Propagation&lt;/h2&gt;

&lt;p&gt;The general rule for PyTorch memory format propagation is to preserve the input tensor‚Äôs memory format. Which means a Channels First input will generate a Channels First output and a Channels Last input will generate a Channels Last output.&lt;/p&gt;

&lt;p&gt;For Convolution layers, PyTorch uses oneDNN (oneAPI Deep Neural Network Library) by default to achieve optimal performance on Intel CPUs. Since it is physically impossible to achieve highly optimized performance directly with Channels Frist memory format, input and weight are firstly converted to blocked format and then computed. oneDNN may choose different blocked formats according to input shapes, data type and hardware architecture, for vectorization and cache reuse purposes. The blocked format is opaque to PyTorch, so the output needs to be converted back to Channels First. Though blocked format would bring about optimal computing performance, the format conversions may add overhead and therefore offset the performance gain.&lt;/p&gt;

&lt;p&gt;On the other hand, oneDNN is optimized for Channels Last memory format to use it for optimal performance directly and PyTorch will simply pass a memory view to oneDNN. Which means the conversion of input and output tensor is saved. Fig-2 indicates memory format propagation behavior of convolution on PyTorch CPU (the solid arrow indicates a memory format conversion, and the dashed arrow indicates a memory view):&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/accelerating-pytorch-vision-models-with-channels-last-on-cpu-2.png&quot; width=&quot;70%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Fig-2 CPU Conv memory format propagation&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;On PyTorch, the default memory format is Channels First. In case a particular operator doesn‚Äôt have support on Channels Last, the NHWC input would be treated as a non-contiguous NCHW and therefore fallback to Channels First, which will consume the previous memory bandwidth on CPU and result in suboptimal performance.&lt;/p&gt;

&lt;p&gt;Therefore, it is very important to extend the scope of Channels Last support for optimal performance. And we have implemented Channels Last kernels for the commonly use operators in CV domain, applicable for both inference and training, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Activations (e.g., ReLU, PReLU, etc.)&lt;/li&gt;
  &lt;li&gt;Convolution (e.g., Conv2d)&lt;/li&gt;
  &lt;li&gt;Normalization (e.g., BatchNorm2d, GroupNorm, etc.)&lt;/li&gt;
  &lt;li&gt;Pooling (e.g., AdaptiveAvgPool2d, MaxPool2d, etc.)&lt;/li&gt;
  &lt;li&gt;Shuffle (e.g., ChannelShuffle, PixelShuffle)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Refer to &lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support&quot;&gt;Operators-with-Channels-Last-support&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;native-level-optimization-on-channels-last&quot;&gt;Native Level Optimization on Channels Last&lt;/h2&gt;

&lt;p&gt;As mentioned above, PyTorch uses oneDNN to achieve optimal performance on Intel CPUs for convolutions. The rest of memory format aware operators are optimized at PyTorch native level, which doesn‚Äôt require any third-party library support.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Cache friendly parallelization scheme:&lt;/strong&gt; keep the same parallelization scheme for all the memory format aware operators, this will help increase data locality when passing each layer‚Äôs output to the next.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Vectorization on multiple archs:&lt;/strong&gt; generally, we can vectorize on the most inner dimension on Channels Last memory format. And each of the vectorized CPU kernels will be generated for both AVX2 and AVX512.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While contributing to Channels Last kernels, we tried our best to optimize Channels First counterparts as well. The fact is some operators are physically impossible to achieve optimal performance on Channels First, such as Convolution, Pooling, etc.&lt;/p&gt;

&lt;h2 id=&quot;run-vision-models-on-channels-last&quot;&gt;Run Vision Models on Channels Last&lt;/h2&gt;

&lt;p&gt;The Channels Last related APIs are documented at &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html&quot;&gt;PyTorch memory format tutorial&lt;/a&gt;. Typically, we can convert a 4D tensor from Channels First to Channels Last by:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# convert x to channels last
# suppose x‚Äôs shape is (N, C, H, W)
# then x‚Äôs stride will be (HWC, 1, WC, C)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To run models on Channels Last memory format, simply need to convert input and model to Channels Last and then you are ready to go. The following is a minimal example showing how to run ResNet50 with TorchVision on Channels Last memory format:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# convert input and model to channels last
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memory_format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;channels_last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Channels Last optimization is implemented at native kernel level, which means you may apply other functionalities such as torch.fx and torch script together with Channels Last as well.&lt;/p&gt;

&lt;h2 id=&quot;performance-gains&quot;&gt;Performance Gains&lt;/h2&gt;

&lt;p&gt;We benchmarked inference performance of TorchVision models on Intel¬Æ Xeon¬Æ Platinum 8380 CPU @ 2.3 GHz, single instance per socket (batch size = 2 x number of physical cores). Results show that Channels Last has 1.3x to 1.8x performance gain over Channels First.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/accelerating-pytorch-vision-models-with-channels-last-on-cpu-3.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The performance gain primarily comes from two aspects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For Convolution layers, Channels Last saved the memory format conversion to blocked format for activations, which improves the overall computation efficiency.&lt;/li&gt;
  &lt;li&gt;For Pooling and Upsampling layers, Channels Last can use vectorized logic along the most inner dimension, e.g., ‚ÄúC‚Äù, while Channels First can‚Äôt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For memory format non aware layers, Channels Last and Channels First has the same performance.&lt;/p&gt;

&lt;h2 id=&quot;conclusion--future-work&quot;&gt;Conclusion &amp;amp; Future Work&lt;/h2&gt;

&lt;p&gt;In this blog we introduced fundamental concepts of Channels Last and demonstrated the performance benefits of CPU using Channels Last on vision models. The current work is limited to 2D models at the current stage, and we will extend the optimization effort to 3D models in near future!&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The results presented in this blog is a joint effort of Meta and Intel PyTorch team. Special thanks to Vitaly Fedyunin and Wei Wei from Meta who spent precious time and gave substantial assistance! Together we made one more step on the path of improving the PyTorch CPU eco system.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html&quot;&gt;PyTorch memory format tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://oneapi-src.github.io/oneDNN/dev_guide_understanding_memory_formats.html&quot;&gt;oneDNN guide on memory formats&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support&quot;&gt;PyTorch operators with Channels Last support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Mingfei Ma (Intel), Vitaly Fedyunin (Meta), Wei Wei (Meta)</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Easily list and initialize models with new APIs in TorchVision</title>
      <link href="https://pytorch.org/blog/easily-list-and-initialize-models-with-new-apis-in-torchvision/" rel="alternate" type="text/html" title="Easily list and initialize models with new APIs in TorchVision" />
      <published>2022-08-18T00:00:00-07:00</published>
      <updated>2022-08-18T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/easily-list-and-initialize-models-with-new-apis-in-torchvision</id>
      <content type="html" xml:base="https://pytorch.org/blog/easily-list-and-initialize-models-with-new-apis-in-torchvision/">&lt;p&gt;TorchVision now supports listing and initializing all available built-in models and weights by name. This new API builds upon the recently introduced &lt;a href=&quot;https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/&quot;&gt;Multi-weight support API&lt;/a&gt;, is currently in Beta, and it addresses a long-standing &lt;a href=&quot;https://github.com/pytorch/vision/issues/1143&quot;&gt;request&lt;/a&gt; from the community.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;\assets\images\easily-list-and-initialize-models-with-new-apis-in-torchvision.gif&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;You can try out the new API in the &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;latest nightly&lt;/a&gt; release of TorchVision. We‚Äôre looking to collect feedback ahead of finalizing the feature in TorchVision v0.14. We have created a dedicated &lt;a href=&quot;https://github.com/pytorch/vision/issues/6365&quot;&gt;Github Issue&lt;/a&gt; where you can post your comments, questions and suggestions!&lt;/p&gt;

&lt;h2 id=&quot;querying-and-initializing-available-models&quot;&gt;Querying and initializing available models&lt;/h2&gt;

&lt;p&gt;Before the new model registration API, developers had to query the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__dict__&lt;/code&gt; attribute of the modules in order to list all available models or to fetch a specific model builder method by its name:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Initialize a model by its name:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__dict__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# List available models:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;available_models&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__dict__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;callable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;islower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;_&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above approach does not always produce the expected results and is hard to discover. For example, since the &lt;a href=&quot;https://pytorch.org/vision/main/models.html#using-models-from-hub&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_weight()&lt;/code&gt;&lt;/a&gt; method is exposed publicly under the same module, it will be included in the list despite not being a model. In general, reducing the verbosity (less imports, shorter names etc) and being able to initialize models and weights directly from their names (better support of configs, TorchHub etc) was &lt;a href=&quot;https://github.com/pytorch/vision/issues/5088&quot;&gt;feedback&lt;/a&gt; provided previously by the community. To solve this problem, we have developed a model registration API.&lt;/p&gt;

&lt;h2 id=&quot;a-new-approach&quot;&gt;A new approach&lt;/h2&gt;

&lt;p&gt;We‚Äôve added 4 new methods under the torchvision.models module:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_models&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The styles and naming conventions align closely with a prototype mechanism proposed by Philip Meier for the &lt;a href=&quot;https://github.com/pytorch/vision/blob/main/torchvision/prototype/datasets/_api.py&quot;&gt;Datasets V2&lt;/a&gt; API, aiming to offer a similar user experience. The model registration methods are kept private on purpose as we currently focus only on supporting the built-in models of TorchVision.&lt;/p&gt;

&lt;h3 id=&quot;list-models&quot;&gt;List models&lt;/h3&gt;

&lt;p&gt;Listing all available models in TorchVision can be done with a single function call:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alexnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mobilenet_v3_large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mobilenet_v3_small'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'quantized_mobilenet_v3_large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To list the available models of specific submodules:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'alexnet'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mobilenet_v3_large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mobilenet_v3_small'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...]&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'quantized_mobilenet_v3_large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;initialize-models&quot;&gt;Initialize models&lt;/h3&gt;

&lt;p&gt;Now that you know which models are available, you can easily initialize a model with pre-trained weights:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;quantized_mobilenet_v3_large&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DEFAULT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;QuantizableMobileNetV3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;....&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;get-weights&quot;&gt;Get weights&lt;/h3&gt;
&lt;p&gt;Sometimes, while working with config files or using TorchHub, you might have the name of a specific weight entry and wish to get its instance. This can be easily done with the following method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ResNet50_Weights.IMAGENET1K_V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ResNet50_Weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IMAGENET1K_V2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To get the enum class with all available weights of a specific model you can use either its name:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;quantized_mobilenet_v3_large&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'MobileNet_V3_Large_QuantizedWeights'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or its model builder method:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;quantization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mobilenet_v3_large&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'MobileNet_V3_Large_QuantizedWeights'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;torchhub-support&quot;&gt;TorchHub support&lt;/h3&gt;
&lt;p&gt;The new methods are also available via TorchHub:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Fetching a specific weight entry by its name:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pytorch/vision&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;get_weight&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ResNet50_Weights.IMAGENET1K_V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Fetching the weights enum class to list all available entries:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_enum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pytorch/vision&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;get_model_weights&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;resnet50&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_enum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;putting-it-all-together&quot;&gt;Putting it all together&lt;/h2&gt;

&lt;p&gt;For example, if you wanted to retrieve all the small-sized models with pre-trained weights and initialize one of them, it‚Äôs a matter of using the above APIs:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_models&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;max_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000000&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tiny_models&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;weights_enum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights_enum&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;num_params&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tiny_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tiny_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ['mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mobilenet_v2', ...]
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tiny_models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DEFAULT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 2239188
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more technical details please see the original &lt;a href=&quot;https://github.com/pytorch/vision/pull/6330&quot;&gt;RFC&lt;/a&gt;. Please spare a few minutes to provide your feedback on the new API, as this is crucial for graduating it from beta and including it in the next release. You can do this on the dedicated &lt;a href=&quot;https://github.com/pytorch/vision/issues/6365&quot;&gt;Github Issue&lt;/a&gt;. We are looking forward to reading your comments!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Vasilis Vryniotis and Laurence Rouesnel</name>
        
        
      </author>

      

      

      
        <summary type="html">TorchVision now supports listing and initializing all available built-in models and weights by name. This new API builds upon the recently introduced Multi-weight support API, is currently in Beta, and it addresses a long-standing request from the community.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Empowering PyTorch on Intel¬Æ Xeon¬Æ Scalable processors with Bfloat16</title>
      <link href="https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16/" rel="alternate" type="text/html" title="Empowering PyTorch on Intel¬Æ Xeon¬Æ Scalable processors with Bfloat16" />
      <published>2022-08-16T00:00:00-07:00</published>
      <updated>2022-08-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16</id>
      <content type="html" xml:base="https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Recent years, the growing complexity of AI models have been posing requirements on hardware for more and more compute capability. Reduced precision numeric format has been proposed to address this problem. Bfloat16 is a custom 16-bit floating point format for AI which consists of one sign bit, eight exponent bits, and seven mantissa bits. With the same dynamic range as float32, bfloat16 doesn‚Äôt require a special handling such as loss scaling. Therefore, bfloat16 is a drop-in replacement for float32 when running deep neural networks for both inference and training.&lt;/p&gt;

&lt;p&gt;The 3rd Gen Intel&lt;sup&gt;¬Æ&lt;/sup&gt; Xeon&lt;sup&gt;¬Æ&lt;/sup&gt; Scalable processor (codenamed Cooper Lake), is the first general purpose x86 CPU with native bfloat16 support. Three new bfloat16 instructions were introduced in Intel&lt;sup&gt;¬Æ&lt;/sup&gt; Advanced Vector Extensions-512 (Intel&lt;sup&gt;¬Æ&lt;/sup&gt; AVX-512): VCVTNE2PS2BF16, VCVTNEPS2BF16, and VDPBF16PS. The first two instructions perform conversion from float32 to bfloat16, and the last one performs a dot product of bfloat16 pairs. Bfloat16 theoretical compute throughput is doubled over float32 on Cooper Lake. On the next generation of Intel&lt;sup&gt;¬Æ&lt;/sup&gt; Xeon&lt;sup&gt;¬Æ&lt;/sup&gt; Scalable Processors, bfloat16 compute throughput will be further enhanced through Advanced Matrix Extensions (Intel&lt;sup&gt;¬Æ&lt;/sup&gt; AMX) instruction set extension.&lt;/p&gt;

&lt;p&gt;Intel and Meta previously collaborated to enable bfloat16 on PyTorch, and the related work was published in an earlier &lt;a href=&quot;https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-and-Facebook-Accelerate-PyTorch-Performance-with-3rd-Gen/post/1335659&quot;&gt;blog&lt;/a&gt; during launch of Cooper Lake. In that blog, we introduced the hardware advancement for native bfloat16 support and showcased a performance boost of 1.4x to 1.6x of bfloat16 over float32 from DLRM, ResNet-50 and ResNext-101-32x4d.&lt;/p&gt;

&lt;p&gt;In this blog, we will introduce the latest software enhancement on bfloat16 in PyTorch 1.12, which would apply to much broader scope of user scenarios and showcase even higher performance boost.&lt;/p&gt;

&lt;h2 id=&quot;native-level-optimization-on-bfloat16&quot;&gt;Native Level Optimization on Bfloat16&lt;/h2&gt;

&lt;p&gt;On PyTorch CPU bfloat16 path, the compute intensive operators, e.g., convolution, linear and bmm, use oneDNN (oneAPI Deep Neural Network Library) to achieve optimal performance on Intel CPUs with AVX512_BF16 or AMX support. The other operators,  such as tensor operators and neural network operators, are optimized at PyTorch native level. We have enlarged bfloat16 kernel level optimizations to majority of operators on dense tensors, both inference and training applicable (sparse tensor bfloat16 support will be covered in future work), specifically:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bfloat16 vectorization&lt;/strong&gt;: Bfloat16 is stored as unsigned 16-bit integer, which requires it to be casted to float32 for arithmetic operations such as add, mul, etc. Specifically, each bfloat16 vector will be converted to two float32 vectors, processed accordingly and then converted back. While for non-arithmetic operations such as cat, copy, etc., it is a straight memory copy and no data type conversion will be involved.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bfloat16 reduction&lt;/strong&gt;: Reduction on bfloat16 data uses float32 as accumulation type to guarantee numerical stability, e.g., sum, BatchNorm2d, MaxPool2d, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Channels Last optimization&lt;/strong&gt;: For vision models, Channels Last is the preferable memory format over Channels First from performance perspective. We have implemented fully optimized CPU kernels for all the commonly used CV modules on channels last memory format, taking care of both float32 and bfloat16.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;run-bfloat16-with-auto-mixed-precision&quot;&gt;Run Bfloat16 with Auto Mixed Precision&lt;/h2&gt;

&lt;p&gt;To run model on bfloat16, typically user can either explicitly convert the data and model to bfloat16, for example:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;with explicit conversion
&lt;span class=&quot;go&quot;&gt;input = input.to(dtype=torch.bfloat16)
model = model.to(dtype=torch.bfloat16)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or utilize torch.amp (Automatic Mixed Precision) package. The autocast instance serves as context managers or decorators that allow regions of your script to run in mixed precision, for example:&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;with AMP
&lt;span class=&quot;go&quot;&gt;with torch.autocast(device_type=&quot;cpu&quot;, dtype=torch.bfloat16):
    output = model(input)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Generally, the explicit conversion approach and AMP approach have similar performance. Even though, we recommend run bfloat16 models with AMP, because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Better user experience with automatic fallback&lt;/strong&gt;: If your script includes operators that don‚Äôt have bfloat16 support, autocast will implicitly convert them back to float32 while the explicit converted model will give a runtime error.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mixed data type for activation and parameters&lt;/strong&gt;: Unlike the explicit conversion which converts all the model parameters to bfloat16, AMP mode will run in mixed data type. To be specific, input/output will be kept in bfloat16 while parameters, e.g., weight/bias, will be kept in float32. The mixed data type of activation and parameters will help improve performance while maintaining the accuracy.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-gains&quot;&gt;Performance Gains&lt;/h2&gt;

&lt;p&gt;We benchmarked inference performance of TorchVision models on Intel¬Æ Xeon¬Æ Platinum 8380H CPU @ 2.90GHz (codenamed Cooper Lake), single instance per socket (batch size = 2 x number of physical cores). Results show that bfloat16 has 1.4x to 2.2x performance gain over float32.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;\assets\images\empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;the-performance-boost-of-bfloat16-over-float32-primarily-comes-from-3-aspects&quot;&gt;The performance boost of bfloat16 over float32 primarily comes from 3 aspects:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The compute intensive operators take advantage of the new bfloat16 native instruction VDPBF16PS which doubles the hardware compute throughput.&lt;/li&gt;
  &lt;li&gt;Bfloat16 have only half the memory footprint of float32, so theoretically the memory bandwidth intensive operators will be twice faster.&lt;/li&gt;
  &lt;li&gt;On Channels Last, we intentionally keep the same parallelization scheme for all the memory format aware operators (can‚Äôt do this on Channels First though), which increases the data locality when passing each layer‚Äôs output to the next. Basically, it keeps the data closer to CPU cores while data would reside in cache anyway.  And bfloat16 will have a higher cache hit rate compared with float32 in such scenarios due to smaller memory footprint.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion--future-work&quot;&gt;Conclusion &amp;amp; Future Work&lt;/h2&gt;

&lt;p&gt;In this blog, we introduced recent software optimizations on bfloat16 introduced in PyTorch 1.12. Results on the 3rd Gen Intel&lt;sup&gt;¬Æ&lt;/sup&gt; Xeon&lt;sup&gt;¬Æ&lt;/sup&gt; Scalable processor show that bfloat16 has 1.4x to 2.2x performance gain over float32 on the TorchVision models. Further improvement is expected on the next generation of Intel&lt;sup&gt;¬Æ&lt;/sup&gt; Xeon&lt;sup&gt;¬Æ&lt;/sup&gt; Scalable Processors with AMX instruction support. Though the performance number for this blog is collected with TorchVision models, the benefit is broad across all topologies. And we will continue to extend the bfloat16 optimization effort to a broader scope in the future!&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;

&lt;p&gt;The results presented in this blog is a joint effort of Meta and Intel PyTorch team. Special thanks to Vitaly Fedyunin and Wei Wei from Meta who spent precious time and gave substantial assistance! Together we made one more step on the path of improving the PyTorch CPU eco system.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/tpu/docs/bfloat16?hl=en&quot;&gt;The bfloat16 numerical format&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/master/amp.html#torch.autocast&quot;&gt;https://pytorch.org/docs/master/amp.html#torch.autocast&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-and-Facebook-Accelerate-PyTorch-Performance-with-3rd-Gen/post/1335659&quot;&gt;Intel and Facebook Accelerate PyTorch Performance with 3rd Gen Intel¬Æ Xeon¬Æ Processors and Intel¬Æ Deep Learning Boost‚Äôs new BFloat16 capability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Mingfei Ma (Intel), Vitaly Fedyunin (Meta), Wei Wei (Meta)</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing the PlayTorch app: Rapidly Create Mobile AI Experiences</title>
      <link href="https://pytorch.org/blog/introducing-the-playtorch-app/" rel="alternate" type="text/html" title="Introducing the PlayTorch app: Rapidly Create Mobile AI Experiences" />
      <published>2022-07-22T00:00:00-07:00</published>
      <updated>2022-07-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/introducing-the-playtorch-app</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-the-playtorch-app/">&lt;p align=&quot;center&quot;&gt;
  &lt;video width=&quot;100%&quot; controls=&quot;&quot;&gt;
    &lt;source src=&quot;/assets/videos/PlayTorch-video.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/p&gt;

&lt;p&gt;In December, we announced PyTorch Live, a toolkit for building AI-powered mobile prototypes in minutes. The initial release included a command-line interface to set up a development environment and an SDK for building AI-powered experiences in React Native. Today, we‚Äôre excited to share that PyTorch Live will now be known as PlayTorch. This new release provides an improved and simplified developer experience. PlayTorch development is independent from the PyTorch project and the PlayTorch code repository is moving into the Meta Research GitHub organization.&lt;/p&gt;

&lt;h2 id=&quot;a-new-workflow-the-playtorch-app&quot;&gt;A New Workflow: The PlayTorch App&lt;/h2&gt;

&lt;p&gt;The PlayTorch team is excited to announce that we have partnered with &lt;a href=&quot;https://expo.dev&quot;&gt;Expo&lt;/a&gt; to change the way AI powered mobile experiences are built. Our new release simplifies the process of building mobile AI experiences by eliminating the need for a complicated development environment. You will now be able to build cross platform AI powered prototypes from the very browser you are using to read this blog.&lt;/p&gt;

&lt;p&gt;In order to make this happen, we are releasing the &lt;a href=&quot;https://playtorch.dev/&quot;&gt;PlayTorch app&lt;/a&gt; which is able to run AI-powered experiences built in the &lt;a href=&quot;https://snack.expo.dev/@playtorch/playtorch-starter?supportedPlatforms=my-device&quot;&gt;Expo Snack&lt;/a&gt; web based code editor.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-introducing-the-playtorch-app-1.gif&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The PlayTorch app can be downloaded from the Apple App Store and Google Play Store. With the app installed, you can head over to &lt;a href=&quot;https://playtorch.dev/snack&quot;&gt;playtorch.dev/snack&lt;/a&gt; and write the code for your AI-powered PlayTorch Snack. When you want to try what you‚Äôve built, you can use the PlayTorch app‚Äôs QR code scanner to scan the QR code on the Snack page and load the code to your device.&lt;/p&gt;

&lt;p&gt;NOTE: PlayTorch Snacks will not work in the Expo Go app.&lt;/p&gt;

&lt;h2 id=&quot;more-to-explore-in-the-playtorch-app&quot;&gt;More to Explore in the PlayTorch App&lt;/h2&gt;

&lt;h3 id=&quot;ai-demos&quot;&gt;AI Demos&lt;/h3&gt;

&lt;p&gt;The PlayTorch app comes with several examples of how you can build AI powered experiences with a variety of different machine learning models from object detection to natural language processing. See what can be built with the PlayTorch SDK and be inspired to make something of your own as you play with the examples.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-introducing-the-playtorch-app-2.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;sharing-your-creations&quot;&gt;Sharing Your Creations&lt;/h3&gt;

&lt;p&gt;Any PlayTorch Snack that you run in the PlayTorch app can be shared with others in an instant. When they open the link on their device, the PlayTorch app will instantly load what you‚Äôve built from the cloud so they can experience it first hand.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-introducing-the-playtorch-app-3.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;When you have something you want to share, let us know on &lt;a href=&quot;https://discord.gg/sQkXTqEt33&quot;&gt;Discord&lt;/a&gt; or &lt;a href=&quot;https://twitter.com/PlayTorch&quot;&gt;Twitter&lt;/a&gt; or embed the PlayTorch Snack on your own webpage.&lt;/p&gt;

&lt;h2 id=&quot;sdk-overhaul&quot;&gt;SDK Overhaul&lt;/h2&gt;

&lt;p&gt;We learned a lot from the community after our initial launch in December and have been hard at work over the past several months to make the PlayTorch SDK (formerly known as PyTorch Live) simple, performant, and robust. In our initial version, the SDK relied on config files to define how a model ingested and output data.&lt;/p&gt;

&lt;p&gt;Today, we are happy to announce the next version of our SDK can handle data processing in JavaScript for your prototypes with the new PlayTorch API that leverages the JavaScript Interface (JSI) to directly call C++ code. Not only have we completely redone the way you can interact with models, but we have also greatly expanded the variety of supported model architectures.&lt;/p&gt;

&lt;h2 id=&quot;a-new-data-processing-api-for-prototyping&quot;&gt;A New Data Processing API for Prototyping&lt;/h2&gt;

&lt;p&gt;With this JSI API, we now allow users direct access to tensors (data format for machine learning). Instead of only having access to predefined transformations, you can now manipulate tensors however you would like for your prototypes.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-introducing-the-playtorch-app-4.gif&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;No more switching back and forth between code and config. You will now be able to write everything in JavaScript and have access to all of the type annotations and autocomplete features available to you in those languages.&lt;/p&gt;

&lt;p&gt;Check out our &lt;a href=&quot;https://playtorch.dev/tutorials&quot;&gt;tutorials&lt;/a&gt; to see the new Data Processing API in action, take a deeper dive in the &lt;a href=&quot;https://playtorch.dev/docs/api/core/&quot;&gt;API docs&lt;/a&gt;, or inspect the code yourself on &lt;a href=&quot;https://github.com/facebookresearch/playtorch&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;expanded-use-cases&quot;&gt;Expanded Use Cases&lt;/h3&gt;

&lt;p&gt;With the new version of the SDK, we have added support for several cutting edge models.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-introducing-the-playtorch-app-5.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Image-to-image transformations are now supported thanks to our robust JSI API, so you can see what your world would look like if it were an anime.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-introducing-the-playtorch-app-6.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Translate French to English with an AI powered translator using the Seq2Seq model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-introducing-the-playtorch-app-7.jpg&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Use DeepLab V3 to segment images!&lt;/p&gt;

&lt;h2 id=&quot;start-playing&quot;&gt;Start Playing&lt;/h2&gt;

&lt;p&gt;If you want to start creating AI experiences yourself, head over to &lt;a href=&quot;https://playtorch.dev&quot;&gt;playtorch.dev&lt;/a&gt; and try out our &lt;a href=&quot;https://playtorch.dev/tutorials/&quot;&gt;tutorials&lt;/a&gt;. Each tutorial will guide you through building a simple AI powered experience that you can instantly run on your phone and share with others.&lt;/p&gt;

&lt;h2 id=&quot;how-to-get-support&quot;&gt;How to Get Support&lt;/h2&gt;

&lt;p&gt;Join us on &lt;a href=&quot;https://discord.gg/sQkXTqEt33&quot;&gt;Discord&lt;/a&gt;, collaborate with us on &lt;a href=&quot;https://github.com/facebookresearch/playtorch&quot;&gt;GitHub&lt;/a&gt;, or follow us on &lt;a href=&quot;https://twitter.com/playtorch&quot;&gt;Twitter&lt;/a&gt;. Got questions or feedback? We‚Äôd love to hear from you!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>PlayTorch Team</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">What Every User Should Know About Mixed Precision Training in PyTorch</title>
      <link href="https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/" rel="alternate" type="text/html" title="What Every User Should Know About Mixed Precision Training in PyTorch" />
      <published>2022-07-19T00:00:00-07:00</published>
      <updated>2022-07-19T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/">&lt;p&gt;Efficient training of modern neural networks often relies on using lower precision data types. Peak float16 matrix multiplication and convolution performance is 16x faster than peak float32 performance on A100 GPUs. And since the float16 and bfloat16 data types are only half the size of float32 they can double the performance of bandwidth-bound kernels and reduce the memory required to train a network, allowing for larger models, larger batches, or larger inputs. Using a module like &lt;a href=&quot;https://pytorch.org/docs/master/amp.html&quot;&gt;torch.amp&lt;/a&gt; (short for ‚ÄúAutomated Mixed Precision‚Äù) makes it easy to get the speed and memory usage benefits of lower precision data types while preserving convergence behavior.&lt;/p&gt;

&lt;p&gt;Going faster and using less memory is always advantageous ‚Äì deep learning practitioners can test more model architectures and hyperparameters, and larger, more powerful models can be trained. Training very large models like those described in &lt;a href=&quot;https://arxiv.org/pdf/2104.04473.pdf&quot;&gt;Narayanan et al.&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;&gt;Brown et al.&lt;/a&gt; (which take thousands of GPUs months to train even with expert handwritten optimizations) is infeasible without using mixed precision.&lt;/p&gt;

&lt;p&gt;We‚Äôve talked about mixed precision techniques before (&lt;a href=&quot;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://developer.nvidia.com/automatic-mixed-precision&quot;&gt;here&lt;/a&gt;), and this blog post is a summary of those techniques and an introduction if you‚Äôre new to mixed precision.&lt;/p&gt;

&lt;h2 id=&quot;mixed-precision-training-in-practice&quot;&gt;Mixed Precision Training in Practice&lt;/h2&gt;

&lt;p&gt;Mixed precision training techniques ‚Äì the use of the lower precision float16 or bfloat16 data types alongside the float32 data type ‚Äì are broadly applicable and effective. See Figure 1 for a sampling of models successfully trained with mixed precision, and Figures 2 and 3 for example speedups using torch.amp.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mixed-precision-training-figure1.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
 Figure 1: Sampling of DL Workloads Successfully Trained with float16 (&lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dusan_stosic-training-neural-networks-with-tensor-cores.pdf&quot;&gt;Source&lt;/a&gt;).
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mixed-precision-training-figure2.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
 Figure 2: Performance of mixed precision training using torch.amp on NVIDIA 8xV100 vs. float32 training on 8xV100 GPU. Bars represent the speedup factor of torch.amp over float32. 
(Higher is better.) (&lt;a href=&quot;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&quot;&gt;Source&lt;/a&gt;).
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mixed-precision-training-figure3.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
 Figure 3. Performance of mixed precision training using torch.amp on NVIDIA 8xA100 vs. 8xV100 GPU. Bars represent the speedup factor of A100 over V100.
(Higher is Better.) (&lt;a href=&quot;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&quot;&gt;Source&lt;/a&gt;).
&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://github.com/NVIDIA/DeepLearningExamples&quot;&gt;NVIDIA Deep Learning Examples repository&lt;/a&gt; for more sample mixed precision workloads.&lt;/p&gt;

&lt;p&gt;Similar performance charts can be seen in &lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dong_yang-mixed-precision-training-for-3d-medical-image-analysis.pdf&quot;&gt;3D medical image analysis&lt;/a&gt;, &lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/shalini_de_mello-mixed-precision-training-for-faze.pdf&quot;&gt;gaze estimation&lt;/a&gt;, &lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/tingchun_wang-mixed-precision-vid2vid.pdf&quot;&gt;video synthesis&lt;/a&gt;, &lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/mingyu_liu-amp-imaginaire.pdf&quot;&gt;conditional GANs&lt;/a&gt;, and &lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/wonmin_byeon-mixed-precision-training-for-convolutional-tensor-train-lstm.pdf&quot;&gt;convolutional LSTMs&lt;/a&gt;. &lt;a href=&quot;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&quot;&gt;Huang et al&lt;/a&gt;. showed that mixed precision training is 1.5x to 5.5x faster over float32 on V100 GPUs, and an additional 1.3x to 2.5x faster on A100 GPUs on a variety of networks. On very large networks the need for mixed precision is even more evident. &lt;a href=&quot;https://arxiv.org/pdf/2104.04473.pdf&quot;&gt;Narayanan et al&lt;/a&gt;. reports that it would take 34 days to train GPT-3 175B on 1024 A100 GPUs (with a batch size of 1536), but it‚Äôs estimated it would take over a year using float32!&lt;/p&gt;

&lt;h2 id=&quot;getting-started-with-mixed-precision-using-torchamp&quot;&gt;Getting Started With Mixed Precision Using torch.amp&lt;/h2&gt;

&lt;p&gt;torch.amp, introduced in PyTorch 1.6, makes it easy to leverage mixed precision training using the float16 or bfloat16 dtypes. See this &lt;a href=&quot;https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/&quot;&gt;blog post&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html&quot;&gt;tutorial&lt;/a&gt;, and &lt;a href=&quot;https://pytorch.org/docs/master/amp.html&quot;&gt;documentation&lt;/a&gt; for more details. Figure 4 shows an example of applying AMP with grad scaling to a network.&lt;/p&gt;

&lt;div class=&quot;language-console highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;go&quot;&gt;import torch
&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;Creates once at the beginning of training
&lt;span class=&quot;go&quot;&gt;scaler = torch.cuda.amp.GradScaler()

for data, label in data_iter:
   optimizer.zero_grad()
&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;   #&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;Casts operations to mixed precision
&lt;span class=&quot;go&quot;&gt;   with torch.amp.autocast(device_type=‚Äúcuda‚Äù, dtype=torch.float16):
      loss = model(data)

&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;   #&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;Scales the loss, and calls backward&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;   #&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;to create scaled gradients
&lt;span class=&quot;go&quot;&gt;   scaler.scale(loss).backward()

&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;   #&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;Unscales gradients and calls
&lt;span class=&quot;gp&quot;&gt;   #&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;or skips optimizer.step&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;go&quot;&gt;   scaler.step(optimizer)

&lt;/span&gt;&lt;span class=&quot;gp&quot;&gt;   #&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;Updates the scale &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;next iteration
&lt;span class=&quot;go&quot;&gt;   scaler.update()
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
 Figure 4: AMP recipe
&lt;/p&gt;

&lt;h3 id=&quot;picking-the-right-approach&quot;&gt;Picking The Right Approach&lt;/h3&gt;

&lt;p&gt;Out-of-the-box mixed precision training with either float16 or bfloat16 is effective at speeding up the convergence of many deep learning models, but some models may require more careful numerical accuracy management. Here are some options:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full float32 precision. Floating point tensors and modules are created in float32 precision by default in PyTorch, but this is a historic artifact not representative of training most modern deep learning networks. It‚Äôs rare that networks need this much numerical accuracy.&lt;/li&gt;
  &lt;li&gt;Enabling TensorFloat32 (TF32) mode. On Ampere and later CUDA devices matrix multiplications and convolutions can use the TensorFloat32 (TF32) mode for faster but slightly less accurate computations. See the &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/&quot;&gt;Accelerating AI Training with NVIDIA TF32 Tensor Cores&lt;/a&gt; blog post for more details. By default PyTorch enables TF32 mode for convolutions but not matrix multiplications, and unless a network requires full float32 precision we recommend enabling this setting for matrix multiplications, too (see the documentation &lt;a href=&quot;https://pytorch.org/docs/master/generated/torch.set_float32_matmul_precision.html?highlight=precision#torch.set_float32_matmul_precision&quot;&gt;here&lt;/a&gt; for how to do so). It can significantly speed up computations with typically negligible loss of numerical accuracy.&lt;/li&gt;
  &lt;li&gt;Using torch.amp with bfloat16 or float16. Both these low precision floating point data types are usually comparably fast, but some networks may only converge with one vs the other. If a network requires more precision it may need to use float16, and if a network requires more dynamic range it may need to use bfloat16, whose dynamic range is equal to that of float32. If overflows are observed, for example, then we suggest trying bfloat16.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are even more advanced options than those presented here, like using torch.amp‚Äôs autocasting for only parts of a model, or managing mixed precision directly. These topics are largely beyond the scope of this blog post, but see the ‚ÄúBest Practices‚Äù section below.&lt;/p&gt;

&lt;h3 id=&quot;best-practices&quot;&gt;Best Practices&lt;/h3&gt;

&lt;p&gt;We strongly recommend using mixed precision with torch.amp or the TF32 mode (on Ampere and later CUDA devices) whenever possible when training a network. If one of those approaches doesn‚Äôt work, however, we recommend the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High Performance Computing (HPC) applications, regression tasks, and generative networks may simply require full float32 IEEE precision to converge as expected.&lt;/li&gt;
  &lt;li&gt;Try selectively applying torch.amp. In particular we recommend first disabling it on regions performing operations from the torch.linalg module or when doing pre- or post-processing. These operations are often especially sensitive. Note that TF32 mode is a global switch and can‚Äôt be used selectively on regions of a network. Enable TF32 first to check if a network‚Äôs operators are sensitive to the mode, otherwise disable it.&lt;/li&gt;
  &lt;li&gt;If you encounter type mismatches while using torch.amp we don‚Äôt suggest inserting manual casts to start. This error is indicative of something being off with the network, and it‚Äôs usually worth investigating first.&lt;/li&gt;
  &lt;li&gt;Figure out by experimentation if your network is sensitive to range and/or precision of a format. For example &lt;a href=&quot;https://github.com/huggingface/transformers/pull/10956&quot;&gt;fine-tuning bfloat16-pretrained models in float16&lt;/a&gt; can easily run into range issues in float16 because of the potentially large range from training in bfloat16, so users should stick with bfloat16 fine-tuning if the model was trained in bfloat16.&lt;/li&gt;
  &lt;li&gt;The performance gain of mixed precision training can depend on multiple factors (e.g. compute-bound vs memory-bound problems) and users should use the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html&quot;&gt;tuning guide&lt;/a&gt; to remove other bottlenecks in their training scripts. Although having similar theoretical performance benefits, BF16 and FP16 can have different speeds in practice. It‚Äôs recommended to try the mentioned formats and use the one with best speed while maintaining the desired numeric behavior.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, refer to the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html&quot;&gt;AMP Tutorial&lt;/a&gt;, &lt;a href=&quot;https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/.&quot;&gt;Training Neural Networks with Tensor Cores&lt;/a&gt;, and see the post ‚Äú&lt;a href=&quot;https://dev-discuss.pytorch.org/t/more-in-depth-details-of-floating-point-precision/654&quot;&gt;More In-Depth Details of Floating Point Precision&lt;/a&gt;‚Äù on PyTorch Dev Discussion.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Mixed precision training is an essential tool for training deep learning models on modern hardware, and it will become even more important in the future as the performance gap between lower precision operations and float32 continues to grow on newer hardware, as reflected in Figure 5.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/mixed-precision-training-figure5.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Figure 5: Relative peak throughput of float16 (FP16) vs float32 matrix multiplications on Volta and Ampere GPUs. On Ampere relative peak throughput for the TensorFloat32 (TF32) mode and bfloat16 matrix multiplications are shown, too. The relative peak throughput of low precision data types like float16 and bfloat16 vs. float32 matrix multiplications is expected to grow as new hardware is released.
&lt;/p&gt;

&lt;p&gt;PyTorch‚Äôs torch.amp module makes it easy to get started with mixed precision, and we highly recommend using it to train faster and reduce memory usage. torch.amp supports both float16 and bfloat16 mixed precision.&lt;/p&gt;

&lt;p&gt;There are still some networks that are tricky to train with mixed precision, and for these networks we recommend trying TF32 accelerated matrix multiplications on Ampere and later CUDA hardware. Networks are rarely so precision sensitive that they require full float32 precision for every operation.&lt;/p&gt;

&lt;p&gt;If you have questions or suggestions for torch.amp or mixed precision support in PyTorch then let us know by posting to the &lt;a href=&quot;https://discuss.pytorch.org/c/mixed-precision/27&quot;&gt;mixed precision category on the PyTorch Forums&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/new/choose&quot;&gt;filing an issue on the PyTorch GitHub page&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Syed Ahmed, Christian Sarofeen, Mike Ruberry, Eddie Yan, Natalia Gimelshein, Michael Carilli, Szymon Migacz, Piotr Bialecki, Paulius Micikevicius, Dusan Stosic, Dong Yang, and Naoya Maruyama</name>
        
        
      </author>

      

      

      
        <summary type="html">Efficient training of modern neural networks often relies on using lower precision data types. Peak float16 matrix multiplication and convolution performance is 16x faster than peak float32 performance on A100 GPUs. And since the float16 and bfloat16 data types are only half the size of float32 they can double the performance of bandwidth-bound kernels and reduce the memory required to train a network, allowing for larger models, larger batches, or larger inputs. Using a module like torch.amp (short for ‚ÄúAutomated Mixed Precision‚Äù) makes it easy to get the speed and memory usage benefits of lower precision data types while preserving convergence behavior.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Case Study: PathAI Uses PyTorch to Improve Patient Outcomes with AI-powered Pathology</title>
      <link href="https://pytorch.org/blog/PathAI-Uses-PyTorch-to-Improve-Patient-Outcomes-with-AI-powered-Pathology/" rel="alternate" type="text/html" title="Case Study: PathAI Uses PyTorch to Improve Patient Outcomes with AI-powered Pathology" />
      <published>2022-07-15T00:00:00-07:00</published>
      <updated>2022-07-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/PathAI-Uses-PyTorch-to-Improve-Patient-Outcomes-with-AI-powered-Pathology</id>
      <content type="html" xml:base="https://pytorch.org/blog/PathAI-Uses-PyTorch-to-Improve-Patient-Outcomes-with-AI-powered-Pathology/">&lt;p&gt;‚Äã&lt;a href=&quot;https://pathai.com&quot;&gt;‚ÄãPathAI&lt;/a&gt; is the leading provider of AI-powered technology tools and services for pathology (the study of disease). Our platform was built to enable substantial improvements to the accuracy of diagnosis and the measurement of therapeutic efficacy for complex diseases, leveraging modern approaches in machine learning like image segmentation, graph neural networks, and multiple instance learning.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-PathAI-Uses-PyTorch-to-Improve-Patient-Outcomes-with-AI-powered-Pathology-1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Traditional manual pathology is prone to &lt;a href=&quot;https://www.journal-of-hepatology.eu/article/S0168-8278(20)30399-8/fulltext&quot;&gt;subjectivity and observer variability&lt;/a&gt; that can negatively affect diagnoses and drug development trials. Before we dive into how we use PyTorch to improve our diagnosis workflow, let us first lay out the traditional analog Pathology workflow without machine learning.&lt;/p&gt;

&lt;h2 id=&quot;how-traditional-biopharma-works&quot;&gt;How Traditional Biopharma Works&lt;/h2&gt;

&lt;p&gt;There are many avenues that biopharma companies take to discover novel therapeutics or diagnostics. One of those avenues relies heavily on the analysis of pathology slides to answer a variety of questions: how does a particular cellular communication pathway work? Can a specific disease state be linked to the presence or lack of a particular protein? Why did a particular drug in a clinical trial work for some patients but not others? Might there be an association between patient outcomes and a novel biomarker?&lt;/p&gt;

&lt;p&gt;To help answer these questions, biopharma companies rely on expert pathologists to analyze slides and help evaluate the questions they might have.¬†&lt;/p&gt;

&lt;p&gt;As you might imagine, it takes an expert board certified pathologist to make accurate interpretations and diagnosis. In &lt;a href=&quot;https://www.bmj.com/content/357/bmj.j2813.full&quot;&gt;one study&lt;/a&gt;, a single biopsy result was given to 36 different pathologists and the outcome was 18 different diagnoses varying in severity from no treatment to aggressive treatment necessary. Pathologists also often solicit feedback from colleagues in difficult edge cases. Given the complexity of the problem, even with expert training and collaboration, pathologists can still have a hard time making a correct diagnosis. This potential variance can be the difference between a drug being approved and it failing the clinical trial.&lt;/p&gt;

&lt;h2 id=&quot;how-pathai-utilizes-machine-learning-to-power-drug-development&quot;&gt;How PathAI utilizes machine learning to power drug development&lt;/h2&gt;

&lt;p&gt;PathAI develops machine learning models which provide insights for drug development R&amp;amp;D, for powering clinical trials, and for making diagnoses. To this end, PathAI leverages PyTorch for slide level inference using a variety of methods including graph neural networks (GNN) as well as multiple instance learning. In this context, ‚Äúslides‚Äù refers to full size scanned images of glass slides, which are pieces of glass with a thin slice of tissue between them, stained to show various cell formations. PyTorch enables our teams using these different methodologies to share a common framework which is robust enough to work in all the conditions we need. PyTorch‚Äôs high level, imperative, and pythonic syntax allows us to prototype models quickly and then take those models to scale once we have the results we want.¬†&lt;/p&gt;

&lt;h2 id=&quot;multi-instance-learning-on-gigabyte-images&quot;&gt;Multi-instance learning on gigabyte images&lt;/h2&gt;

&lt;p&gt;One of the uniquely challenging aspects of applying ML to pathology is the immense size of the images. These digital slides can often be 100,000 x 100,000 pixels or more in resolution and gigabytes in size. Loading the full image in GPU memory and applying traditional computer vision algorithms on them is an almost impossible task. It also takes both a considerable amount of time and resources to have a full slide image (100k x 100k) annotated, especially when annotators need to be domain experts (board-certified pathologists). We often build models to predict image-level labels, like the presence of cancer, on a patient slide which covers a few thousand pixels in the whole image. The cancerous area is sometimes a tiny fraction of the entire slide, which makes the ML problem similar to finding a needle in a haystack. On the other hand, some problems like the prediction of certain histological biomarkers require an aggregation of information from the whole slide which is again hard due to the size of the images. All these factors add significant algorithmic, computational, and logistical complexity when applying ML techniques to pathology problems.&lt;/p&gt;

&lt;p&gt;Breaking down the image into smaller patches, learning patch representations, and then pooling those representations to predict an image-level label is one way to solve this problem as is depicted in the image below. One popular method for doing this is called &lt;a href=&quot;https://paperswithcode.com/task/multiple-instance-learning&quot;&gt;Multiple Instance Learning (MIL)&lt;/a&gt;. Each patch is considered an ‚Äòinstance‚Äô and a set of patches forms a ‚Äòbag‚Äô. The individual patch representations are pooled together to predict a final bag-level label. Algorithmically, the individual patch instances in the bag do not require labels and hence allow us to learn bag-level labels in a weakly-supervised way. They also use permutation invariant pooling functions which make the prediction independent of the order of patches and allows for an efficient aggregation of information. Typically, attention based pooling functions are used which not only allow for efficient aggregation but also provide attention values for each patch in the bag. These values indicate the importance of the corresponding patch in the prediction and can be visualized to better understand the model predictions. This element of interpretability can be very important to drive adoption of these models in the real world and we use variations like &lt;a href=&quot;https://arxiv.org/pdf/2206.01794.pdf&quot;&gt;Additive MIL models&lt;/a&gt; to enable such spatial explainability. Computationally, MIL models circumvent the problem of applying neural networks to large image sizes since patch representations are obtained independently of the size of the image.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-PathAI-Uses-PyTorch-to-Improve-Patient-Outcomes-with-AI-powered-Pathology-2.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;At PathAI, we use custom MIL models based on deep nets to predict image-level labels. The overview of this process is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Select patches from a slide using different sampling approaches.&lt;/li&gt;
  &lt;li&gt;Construct a bag of patches based on random sampling or heuristic rules.&lt;/li&gt;
  &lt;li&gt;Generate patch representations for each instance based on pre-trained models or large-scale representation learning models.&lt;/li&gt;
  &lt;li&gt;Apply permutation invariant pooling functions to get the final slide-level score.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that we have walked through some of the high-level details around MIL in PyTorch, let‚Äôs look at some code to see how simple it is to go from ideation to code in production with PyTorch. We begin by defining a sampler, transformations, and our MIL dataset:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Create a bag sampler which randomly samples patches from a slide
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bag_sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomBagSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bag_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Setup the transformations
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;crop_transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FlipRotateCenterCrop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;use_flips&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create the dataset which loads patches for each bag
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MILDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;bag_sampler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bag_sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;samples_loader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;crop_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After we have defined our sampler and dataset, we need to define the model we will actually train with said dataset. PyTorch‚Äôs familiar model definition syntax makes this easy to do while also allowing us to create bespoke models at the same time.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DefaultPooledClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_dims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DefaultAttentionModule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;input_dims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;hidden_dims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;output_activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StableSoftmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Define the model which is a composition of the featurizer, pooling module and a classifier
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DefaultMILGraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;featurizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ShuffleNetV2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since these models are trained end-to-end, they offer a powerful way to go directly from a gigapixel whole slide image to a single label. Due to their wide applicability to different biological problems, two aspects of their implementation and deployment are important:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Configurable control over each part of the pipeline including the data loaders, the modular parts of the model, and their interaction with each other.&lt;/li&gt;
  &lt;li&gt;Ability to rapidly iterate through the ideate-implement-experiment-productionize loop.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;PyTorch has various advantages when it comes to MIL modeling. It offers an intuitive way to create dynamic computational graphs with flexible control flow which is great for rapid research experimentation. The map-style datasets, configurable sampler and batch-samplers allow us to customize how we construct bags of patches, enabling faster experimentation. Since MIL models are IO heavy, data parallelism and pythonic data loaders make the task very efficient and user friendly. Lastly, the object-oriented nature of PyTorch enables building of reusable modules which aid in the rapid experimentation, maintainable implementation and ease of building compositional components of the pipeline.&lt;/p&gt;

&lt;h2 id=&quot;exploring-spatial-tissue-organization-with-gnns-in-pytorch&quot;&gt;Exploring spatial tissue organization with GNNs in PyTorch&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-15-PathAI-Uses-PyTorch-to-Improve-Patient-Outcomes-with-AI-powered-Pathology-3.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;In both healthy and diseased tissue, the spatial arrangement and structure of cells can oftentimes be as important as the cells themselves. For example, when assessing lung cancers, pathologists try to look at the overall grouping and structure of tumor cells (do they form solid sheets? Or do they occur in smaller, localized clusters?) to determine if the cancer belongs to specific subtypes which can have vastly &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3369269/&quot;&gt;different prognosis&lt;/a&gt;. Such spatial relationships between cells and other tissue structures can be modeled using graphs to capture tissue topology and cellular composition at the same time. &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPRW_2020/papers/w16/Lu_Capturing_Cellular_Topology_in_Multi-Gigapixel_Pathology_Images_CVPRW_2020_paper.pdf&quot;&gt;Graph Neural Networks&lt;/a&gt; (GNNs) allow learning spatial patterns within these graphs that relate to other clinical variables, for example overexpression of genes in certain cancers.&lt;/p&gt;

&lt;p&gt;In late 2020, when PathAI started using GNNs on tissue samples, PyTorch had the best and most mature support for GNN functionality via the &lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/&quot;&gt;PyG package&lt;/a&gt;. This made PyTorch the natural choice for our team given that GNN models were something that we knew would be an important ML concept we wanted to explore.¬†&lt;/p&gt;

&lt;p&gt;One of the main value-adds of GNN‚Äôs in the context of tissue samples is that the graph itself can uncover spatial relationships that would otherwise be very difficult to find by visual inspection alone. In our recent &lt;a href=&quot;https://aacrjournals.org/cancerres/article/82/12_Supplement/1922/701539&quot;&gt;AACR publication&lt;/a&gt;, we showed that by using GNNs, we can better understand the way the presence of immune cell aggregates (specifically tertiary lymphoid structures, or TLS) in the tumor microenvironment can influence patient prognosis. In this case, the GNN approach was used to predict expression of genes associated with the presence of TLS, and identify histological features beyond the TLS region itself that are relevant to TLS. Such insights into gene expression are difficult to identify from tissue sample images when unassisted by ML models.¬†&lt;/p&gt;

&lt;p&gt;One of the most promising GNN variations we have had success with is &lt;a href=&quot;https://arxiv.org/pdf/1904.08082.pdf&quot;&gt;self attention graph pooling&lt;/a&gt;. Let‚Äôs take a look at how we define our Self Attention Graph Pooling (SAGPool) model using PyTorch and PyG:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SAGPool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GraphConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModuleList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pools&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModuleList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GraphConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aggr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SAGPooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GNN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GraphConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min_score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jump&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;JumpingKnowledge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_activation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_activation&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the above code, we begin by defining a single convolutional graph layer and then add two &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html&quot;&gt;module list layers&lt;/a&gt; which allow us to pass in a variable number of layers. We then take our &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=extend#torch.nn.ModuleList.extend&quot;&gt;empty module list and append&lt;/a&gt; a variable number of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GraphConv&lt;/code&gt; layers followed by a variable number of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SAGPooling&lt;/code&gt; layers. We finish up our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SAGPool&lt;/code&gt; definition by adding a &lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.models.JumpingKnowledge&quot;&gt;JumpingKnowledge Layer&lt;/a&gt;, two linear layers, our activation function, and our dropout value. PyTorch‚Äôs intuitive syntax allows us to abstract away the complexity of working with state of the art methods like SAG Poolings while also maintaining the common approach to model development we are familiar with.&lt;/p&gt;

&lt;p&gt;Models like our SAG Pool one described above are just one example of how GNNs with PyTorch are allowing us to explore new and novel ideas. We also recently explored &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022W/CVMI/papers/Dwivedi_Multi_Stain_Graph_Fusion_for_Multimodal_Integration_in_Pathology_CVPRW_2022_paper.pdf&quot;&gt;multimodal CNN - GNN hybrid models&lt;/a&gt; which ended up being 20% more accurate than traditional Pathologist consensus scores. These innovations and interplay between traditional CNNs and GNNs are again enabled by the short research to production model development loop.&lt;/p&gt;

&lt;h2 id=&quot;improving-patient-outcomes&quot;&gt;Improving Patient Outcomes&lt;/h2&gt;
&lt;p&gt;In order to achieve our mission of improving patient outcomes with AI-powered pathology, PathAI needs to rely on an ML development framework that (1) facilitates quick iteration and easy extension (i.e. Model configuration as code) during initial phases of development and exploration (2) scales model training and inference to massive images (3) easily and robustly serves models for production uses of our products (in clinical trials and beyond). As we‚Äôve demonstrated, PyTorch offers us all of these capabilities and more. We are incredibly excited about the future of PyTorch and cannot wait to see what other impactful challenges we can solve using the framework.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Logan Kilpatrick - Sr. Technology Advocate, Harshith Padigela - ML Engineer, Syed Ashar Javed - ML Technical Lead, Robert Egger - Biomedical Data Scientist</name>
        
        
      </author>

      

      

      
        <summary type="html">‚Äã‚ÄãPathAI is the leading provider of AI-powered technology tools and services for pathology (the study of disease). Our platform was built to enable substantial improvements to the accuracy of diagnosis and the measurement of therapeutic efficacy for complex diseases, leveraging modern approaches in machine learning like image segmentation, graph neural networks, and multiple instance learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A BetterTransformer for Fast Transformer Inference</title>
      <link href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="alternate" type="text/html" title="A BetterTransformer for Fast Transformer Inference" />
      <published>2022-07-12T00:00:00-07:00</published>
      <updated>2022-07-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/">&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt; Transformers achieve state-of-the-art performance for NLP, and are becoming popular for a myriad of other tasks. They are computationally expensive which has been a blocker to their widespread productionisation. Launching with PyTorch 1.12, BetterTransformer implements a backwards-compatible fast path of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.TransformerEncoder&lt;/code&gt; for Transformer Encoder Inference and does not require model authors to modify their models. BetterTransformer improvements can exceed 2x in speedup and throughput for many common execution scenarios. To use BetterTransformer, &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;install&lt;/a&gt; PyTorch 1.12 and start using high-quality, high-performance Transformer models with the PyTorch API today.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-12-a-better-transformer-for-fast-transformer-encoder-inference-1.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
Diagram of the Transformer Encoder Architecture (from &quot;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; target=&quot;_blank&quot;&gt;Attention Is All You Need&lt;/a&gt;&quot;). During Inference, the entire module will execute as a single PyTorch-native function.
&lt;/p&gt;

&lt;p&gt;In this blog post, we share the following topics ‚Äî Performance Improvements, Backwards compatibility, and Taking advantage of the FastPath. Learn more about these topics below.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;Performance Improvements&lt;/h2&gt;

&lt;p&gt;BetterTransformer launches with accelerated native implementations of MultiHeadAttention and TransformerEncoderLayer for CPUs and GPUs. These fast paths are integrated in the standard PyTorch Transformer APIs, and will accelerate &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html&quot;&gt;TransformerEncoder&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html&quot;&gt;TransformerEncoderLayer&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html&quot;&gt;MultiHeadAttention&lt;/a&gt; nn.modules. These new modules implement two types of optimizations: (1) fused kernels combine multiple individual operators normally used to implement Transformers to provide a more efficient implementation, and (2) take advantage of sparsity in the inputs to avoid performing unnecessary operations on padding tokens. Padding tokens frequently account for a large fraction of input batches in many Transformer models used for Natural Language Processing.&lt;/p&gt;

&lt;h2 id=&quot;backwards-compatibility&quot;&gt;Backwards compatibility&lt;/h2&gt;

&lt;p&gt;Advantageously, &lt;strong&gt;no model changes are necessary to benefit from the performance boost offered by BetterTransformer.&lt;/strong&gt; To benefit from fast path execution, inputs and operating conditions must satisfy some access conditions (see below). While the internal implementation of Transformer APIs has changed, PyTorch 1.12 maintains strict compatibility with Transformer modules shipped in previous versions, enabling PyTorch users to use models created and trained with previous PyTorch releases while benefiting from BetterTransformer improvements.&lt;/p&gt;

&lt;p&gt;In addition to enabling the PyTorch nn.Modules, BetterTransformer provides improvements for PyTorch libraries. Performance benefits will become available through two different enablement paths:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;&lt;strong&gt;Transparent acceleration:&lt;/strong&gt;&lt;/u&gt; Current users of PyTorch nn.Modules such as &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html&quot;&gt;MultiHeadAttention&lt;/a&gt; as well as higher-level Transformer components will benefit from the improved performance of the new nn.Modules automatically. An example of this is the &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;visual transformer (ViT)&lt;/a&gt; implementation used in the torchvision library (&lt;a href=&quot;https://github.com/pytorch/vision/blob/87cde716b7f108f3db7b86047596ebfad1b88380/torchvision/models/vision_transformer.py#L103&quot;&gt;code link&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;u&gt;&lt;strong&gt;Torchtext library acceleration:&lt;/strong&gt;&lt;/u&gt; As part of this project, we have optimized Torchtext to build on the PyTorch core API to benefit from BetterTransformer enhancements while maintaining strict and transparent compatibility with previous library versions and models trained with previous Torchtext versions. Using PyTorch Transformers in Torchtext also ensures that Torchtext will benefit from expected future enhancements to the PyTorch Transformer implementation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;taking-advantage-of-the-fastpath&quot;&gt;Taking advantage of the Fastpath&lt;/h2&gt;

&lt;p&gt;BetterTransformer is a fastpath for the PyTorch Transformer API. The fastpath is a native, specialized implementation of key Transformer functions for CPU and GPU that applies to common Transformer use cases.&lt;/p&gt;

&lt;p&gt;To take advantage of input sparsity (i.e. padding) in accelerating your model (see Figure 2), set the keyword argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable_nested_tensor=True&lt;/code&gt; when instantiating a &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html&quot;&gt;TransformerEncoder&lt;/a&gt; and pass in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src_key_padding_mask&lt;/code&gt; argument (which denotes padding tokens) during inference. This requires the padding mask to be contiguous, which is the typical case.&lt;/p&gt;

&lt;p&gt;Currently, the BetterTransformer speedup only applies to transformer encoder models used in inference. To benefit from fastpath execution, models must be composed of any of the following components: &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html&quot;&gt;TransformerEncoder&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html&quot;&gt;TransformerEncoderLayer&lt;/a&gt; or &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html&quot;&gt;MultiheadAttention&lt;/a&gt; (MHA). Fastpath execution is also subject to some criteria. Most importantly, the model must be executed in inference mode and operate on input tensors that do not collect gradient tape information (e.g., running with torch.no_grad). The full list of conditions can be found at these links for &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/29189d2ba8e583b2355cd0e9517a1ee742ba12cf/torch/nn/modules/activation.py#L1060&quot;&gt;nn.MultiHeadAttention&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/29189d2ba8e583b2355cd0e9517a1ee742ba12cf/torch/nn/modules/transformer.py#L206&quot;&gt;nn.TransformerEncoder&lt;/a&gt;, respectively. If the criteria are not met, control flows to the legacy PyTorch 1.11 Transformer implementation which has the same API, but lacks the fastpath performance boost.&lt;/p&gt;

&lt;p&gt;Other transformer models (such as decoder models) which use the PyTorch MultiheadAttention module will benefit from the BetterTransformer fastpath. Planned future work is to expand the end-to-end BetterTransformer fastpath to models based on &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html&quot;&gt;TransformerDecoder&lt;/a&gt; to support popular seq2seq and decoder-only (e.g., &lt;a href=&quot;https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/&quot;&gt;OPT&lt;/a&gt;) model architectures, and to training.&lt;/p&gt;

&lt;h2 id=&quot;speedups&quot;&gt;Speedups&lt;/h2&gt;

&lt;p&gt;The following graphs show the performance achieved for the &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;-base model with small and large-scale inputs:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-12-a-better-transformer-for-fast-transformer-encoder-inference-2.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 1: PyTorch 1.12 Improvements with BetterTransformer fastpath execution&lt;/b&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/2022-7-12-a-better-transformer-for-fast-transformer-encoder-inference-3.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Figure 2: PyTorch 1.12 Improvements with BetterTransformer fastpath execution&lt;br /&gt;
with sparsity optimization enabled by enable_nested_tensor=True&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;BetterTransformer includes two types of optimization: (1) fused kernels implementing multiple operations more efficiently in a single kernel, and (2) exploiting sparsity by avoiding unnecessary processing on padding tokens. Enhanced performance for small input sizes benefits primarily from the fused kernel implementations, and shows a constant performance improvement regardless of padding amount. While large inputs still benefit from fused kernels, the computation heavy processing limits the benefits that may be obtained by the fused kernels as baseline performance is already closer to the theoretical peak. However, as we increase the amount of padding, performance increases dramatically as increasingly large amounts of computation can be avoided by exploiting the sparsity introduced by padding in NLP workloads.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;As part of our ongoing work on PyTorch BetterTransformer, we are working on extending BetterTransformer improvements to Transformer Decoders. We aim to expand beyond inference to training as well.&lt;/p&gt;

&lt;p&gt;We are partnering to enable BetterTransformer on additional libraries such as FairSeq, MetaSeq, and HuggingFace to benefit all Transformer-based PyTorch models. We‚Äôll provide future updates on the progress of BetterTransformer accelerations for the larger PyTorch ecosystem as part of this blog series.&lt;/p&gt;

&lt;p&gt;Acknowledgements: The authors would like to thank Lin Qiao, Ajit Mathews, Andrew Tulloch, Dmytro Dzhulgakov, Natalia Gimelshein, Emad El-Haraty, Mark Saroufim, Adnan Aziz, Geeta Chauhan, and Hamid Shojanazeri for their support, contributions and many helpful suggestions throughout the course of this project, and in the preparation of this blog.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Michael Gschwind, Eric Han, Scott Wolchok, Rui Zhu, Christian Puhrsch</name>
        
        
      </author>

      

      

      
        <summary type="html">tl;dr Transformers achieve state-of-the-art performance for NLP, and are becoming popular for a myriad of other tasks. They are computationally expensive which has been a blocker to their widespread productionisation. Launching with PyTorch 1.12, BetterTransformer implements a backwards-compatible fast path of torch.nn.TransformerEncoder for Transformer Encoder Inference and does not require model authors to modify their models. BetterTransformer improvements can exceed 2x in speedup and throughput for many common execution scenarios. To use BetterTransformer, install PyTorch 1.12 and start using high-quality, high-performance Transformer models with the PyTorch API today.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">New library updates in PyTorch 1.12</title>
      <link href="https://pytorch.org/blog/pytorch-1.12-new-library-releases/" rel="alternate" type="text/html" title="New library updates in PyTorch 1.12" />
      <published>2022-06-28T00:00:00-07:00</published>
      <updated>2022-06-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-1.12-new-library-releases</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-1.12-new-library-releases/">&lt;p&gt;We are bringing a number of improvements to the current PyTorch libraries, alongside the &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v1.12.0&quot;&gt;PyTorch 1.12 release&lt;/a&gt;. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.&lt;/p&gt;

&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TorchVision&lt;/strong&gt; - Added multi-weight support API, new architectures, model variants, and pretrained weight. See the release notes &lt;a href=&quot;https://github.com/pytorch/vision/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchAudio&lt;/strong&gt; - Introduced beta features including a streaming API, a CTC beam search decoder, and new beamforming modules and methods. See the release notes &lt;a href=&quot;https://github.com/pytorch/audio/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchText&lt;/strong&gt; - Extended support for scriptable BERT tokenizer and added datasets for GLUE benchmark. See the release notes &lt;a href=&quot;https://github.com/pytorch/text/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchRec&lt;/strong&gt; - Added EmbeddingModule benchmarks, examples for TwoTower Retrieval, inference and sequential embeddings, metrics, improved planner and demonstrated integration with production components. See the release notes &lt;a href=&quot;https://github.com/pytorch/torchrec/releases&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TorchX&lt;/strong&gt; - Launch PyTorch trainers developed on local workspaces onto five different types of schedulers. See the release notes &lt;a href=&quot;https://github.com/pytorch/torchx/blob/main/CHANGELOG.md?plain=1#L3&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FBGemm&lt;/strong&gt; - Added and improved kernels for Recommendation Systems inference workloads, including table batched embedding bag, jagged tensor operations, and other special-case optimizations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchvision-v013&quot;&gt;TorchVision v0.13&lt;/h2&gt;

&lt;h3 id=&quot;multi-weight-support-api&quot;&gt;Multi-weight support API&lt;/h3&gt;

&lt;p&gt;TorchVision v0.13 offers a new &lt;a href=&quot;https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/&quot;&gt;Multi-weight support API&lt;/a&gt; for loading different weights to the existing model builder methods:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Old weights with accuracy 76.130%
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResNet50_Weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IMAGENET1K_V1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# New weights with accuracy 80.858%
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResNet50_Weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IMAGENET1K_V2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Best available weights (currently alias for IMAGENET1K_V2)
# Note that these weights may change across versions
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResNet50_Weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DEFAULT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Strings are also supported
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;IMAGENET1K_V2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# No weights - random initialization
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The new API bundles along with the weights important details such as the preprocessing transforms and meta-data such as labels. Here is how to make the most out of it:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_image&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResNet50_Weights&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;test/assets/encode_jpeg/grace_hopper_517x606.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Step 1: Initialize model with the best available weights
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResNet50_Weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DEFAULT&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Step 2: Initialize the inference transforms
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Step 3: Apply inference preprocessing transforms
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Step 4: Use the model and print the predicted category
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;category_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;categories&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category_name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can read more about the new API in the &lt;a href=&quot;https://pytorch.org/vision/0.13/models.html&quot;&gt;docs&lt;/a&gt;. To provide your feedback, please use this dedicated &lt;a href=&quot;https://github.com/pytorch/vision/issues/5088&quot;&gt;Github issue&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;new-architectures-and-model-variants&quot;&gt;New architectures and model variants&lt;/h3&gt;

&lt;h4 id=&quot;classification&quot;&gt;Classification&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/2103.14030&quot;&gt;Swin Transformer&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2104.00298&quot;&gt;EfficienetNetV2&lt;/a&gt; are two popular classification models which are often used for downstream vision tasks. This release includes 6 pre-trained weights for their classification variants. Here is how to use the new models:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swin_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DEFAULT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;efficientnet_v2_s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DEFAULT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In addition to the above, we also provide new variants for existing architectures such as ShuffleNetV2, ResNeXt and MNASNet. The accuracies of all the new pre-trained models obtained on ImageNet-1K are seen below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Acc@1&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Acc@5&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;swin_t&lt;/td&gt;
      &lt;td&gt;81.474&lt;/td&gt;
      &lt;td&gt;95.776&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;swin_s&lt;/td&gt;
      &lt;td&gt;83.196&lt;/td&gt;
      &lt;td&gt;96.36&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;swin_b&lt;/td&gt;
      &lt;td&gt;83.582&lt;/td&gt;
      &lt;td&gt;96.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;efficientnet_v2_s&lt;/td&gt;
      &lt;td&gt;84.228&lt;/td&gt;
      &lt;td&gt;96.878&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;efficientnet_v2_m&lt;/td&gt;
      &lt;td&gt;85.112&lt;/td&gt;
      &lt;td&gt;97.156&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;efficientnet_v2_l&lt;/td&gt;
      &lt;td&gt;85.808&lt;/td&gt;
      &lt;td&gt;97.788&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnext101_64x4d&lt;/td&gt;
      &lt;td&gt;83.246&lt;/td&gt;
      &lt;td&gt;96.454&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnext101_64x4d (quantized)&lt;/td&gt;
      &lt;td&gt;82.898&lt;/td&gt;
      &lt;td&gt;96.326&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shufflenet_v2_x1_5&lt;/td&gt;
      &lt;td&gt;72.996&lt;/td&gt;
      &lt;td&gt;91.086&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shufflenet_v2_x1_5 (quantized)&lt;/td&gt;
      &lt;td&gt;72.052&lt;/td&gt;
      &lt;td&gt;0.700&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shufflenet_v2_x2_0&lt;/td&gt;
      &lt;td&gt;76.230&lt;/td&gt;
      &lt;td&gt;93.006&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;shufflenet_v2_x2_0 (quantized)&lt;/td&gt;
      &lt;td&gt;75.354&lt;/td&gt;
      &lt;td&gt;92.488&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mnasnet0_75&lt;/td&gt;
      &lt;td&gt;71.180&lt;/td&gt;
      &lt;td&gt;90.496&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mnas1_3&lt;/td&gt;
      &lt;td&gt;76.506&lt;/td&gt;
      &lt;td&gt;93.522&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We would like to thank Hu Ye for contributing to TorchVision the Swin Transformer implementation.&lt;/p&gt;

&lt;h4 id=&quot;beta-object-detection-and-instance-segmentation&quot;&gt;(BETA) Object Detection and Instance Segmentation&lt;/h4&gt;

&lt;p&gt;We have introduced 3 new model variants for RetinaNet, FasterRCNN and MaskRCNN that include several &lt;a href=&quot;https://github.com/pytorch/vision/pull/5444&quot;&gt;post-paper architectural optimizations&lt;/a&gt; and improved training recipes. All models can be used similarly:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.models.detection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retinanet_resnet50_fpn_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;DEFAULT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# model = fasterrcnn_resnet50_fpn_v2(weights=&quot;DEFAULT&quot;)
# model = maskrcnn_resnet50_fpn_v2(weights=&quot;DEFAULT&quot;)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below we present the metrics of the new variants on COCO val2017. In parenthesis we denote the improvement over the old variants:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Box mAP&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Mask mAP&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;retinanet_resnet50_fpn_v2&lt;/td&gt;
      &lt;td&gt;41.5 (+5.1)&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fasterrcnn_resnet50_fpn_v2&lt;/td&gt;
      &lt;td&gt;46.7 (+9.7)&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;maskrcnn_resnet50_fpn_v2&lt;/td&gt;
      &lt;td&gt;47.4 (+9.5)&lt;/td&gt;
      &lt;td&gt;41.8 (+7.2)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We would like to thank Ross Girshick, Piotr Dollar, Vaibhav Aggarwal, Francisco Massa and Hu Ye for their past research and contributions to this work.&lt;/p&gt;

&lt;h3 id=&quot;new-pre-trained-weights&quot;&gt;New pre-trained weights&lt;/h3&gt;

&lt;h4 id=&quot;swag-weights&quot;&gt;SWAG weights&lt;/h4&gt;

&lt;p&gt;The ViT and RegNet model variants offer new pre-trained &lt;a href=&quot;https://arxiv.org/abs/2201.08371&quot;&gt;SWAG&lt;/a&gt; (‚Äã‚ÄãSupervised Weakly from hashtAGs) weights. One of the biggest of these models achieves a whopping 88.6% accuracy on ImageNet-1K. We currently offer two versions of the weights: 1) fine-tuned end-to-end weights on ImageNet-1K (highest accuracy) and 2) frozen trunk weights with a linear classifier fit on ImageNet-1K (great for transfer learning). Below we see the detailed accuracies of each model variant:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Model Weights&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Acc@1&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Acc@5&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1&lt;/td&gt;
      &lt;td&gt;86.012&lt;/td&gt;
      &lt;td&gt;98.054&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1&lt;/td&gt;
      &lt;td&gt;83.976&lt;/td&gt;
      &lt;td&gt;97.244&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1&lt;/td&gt;
      &lt;td&gt;86.838&lt;/td&gt;
      &lt;td&gt;98.362&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1&lt;/td&gt;
      &lt;td&gt;84.622&lt;/td&gt;
      &lt;td&gt;97.48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1&lt;/td&gt;
      &lt;td&gt;88.228&lt;/td&gt;
      &lt;td&gt;98.682&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1&lt;/td&gt;
      &lt;td&gt;86.068&lt;/td&gt;
      &lt;td&gt;97.844&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1&lt;/td&gt;
      &lt;td&gt;85.304&lt;/td&gt;
      &lt;td&gt;97.65&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1&lt;/td&gt;
      &lt;td&gt;81.886&lt;/td&gt;
      &lt;td&gt;96.18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1&lt;/td&gt;
      &lt;td&gt;88.064&lt;/td&gt;
      &lt;td&gt;98.512&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ViT_L_16_Weights.IMAGENET1K_SWAG_LINEAR_V1&lt;/td&gt;
      &lt;td&gt;85.146&lt;/td&gt;
      &lt;td&gt;97.422&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1&lt;/td&gt;
      &lt;td&gt;88.552&lt;/td&gt;
      &lt;td&gt;98.694&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ViT_H_14_Weights.IMAGENET1K_SWAG_LINEAR_V1&lt;/td&gt;
      &lt;td&gt;85.708&lt;/td&gt;
      &lt;td&gt;97.73&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The SWAG weights are released under the &lt;a href=&quot;https://github.com/facebookresearch/SWAG/blob/main/LICENSE&quot;&gt;Attribution-NonCommercial 4.0 International&lt;/a&gt; license. We would like to thank Laura Gustafson, Mannat Singh and Aaron Adcock for their work and support in making the weights available to TorchVision.&lt;/p&gt;

&lt;h4 id=&quot;model-refresh&quot;&gt;Model Refresh&lt;/h4&gt;

&lt;p&gt;The release of the Multi-weight support API enabled us to refresh the most popular models and offer more accurate weights. We improved on average each model by ~3 points. The new recipe used was learned on top of ResNet50 and its details were covered on a &lt;a href=&quot;https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/&quot;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Old weights&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;New weights&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;efficientnet_b1&lt;/td&gt;
      &lt;td&gt;78.642&lt;/td&gt;
      &lt;td&gt;79.838&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mobilenet_v2&lt;/td&gt;
      &lt;td&gt;71.878&lt;/td&gt;
      &lt;td&gt;72.154&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mobilenet_v3_large&lt;/td&gt;
      &lt;td&gt;74.042&lt;/td&gt;
      &lt;td&gt;75.274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_y_400mf&lt;/td&gt;
      &lt;td&gt;74.046&lt;/td&gt;
      &lt;td&gt;75.804&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_y_800mf&lt;/td&gt;
      &lt;td&gt;76.42&lt;/td&gt;
      &lt;td&gt;78.828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_y_1_6gf&lt;/td&gt;
      &lt;td&gt;77.95&lt;/td&gt;
      &lt;td&gt;80.876&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_y_3_2gf&lt;/td&gt;
      &lt;td&gt;78.948&lt;/td&gt;
      &lt;td&gt;81.982&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_y_8gf&lt;/td&gt;
      &lt;td&gt;80.032&lt;/td&gt;
      &lt;td&gt;82.828&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_y_16gf&lt;/td&gt;
      &lt;td&gt;80.424&lt;/td&gt;
      &lt;td&gt;82.886&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_y_32gf&lt;/td&gt;
      &lt;td&gt;80.878&lt;/td&gt;
      &lt;td&gt;83.368&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_x_400mf&lt;/td&gt;
      &lt;td&gt;72.834&lt;/td&gt;
      &lt;td&gt;74.864&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_x_800mf&lt;/td&gt;
      &lt;td&gt;75.212&lt;/td&gt;
      &lt;td&gt;77.522&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_x_1_6gf&lt;/td&gt;
      &lt;td&gt;77.04&lt;/td&gt;
      &lt;td&gt;79.668&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_x_3_2gf&lt;/td&gt;
      &lt;td&gt;78.364&lt;/td&gt;
      &lt;td&gt;81.196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_x_8gf&lt;/td&gt;
      &lt;td&gt;79.344&lt;/td&gt;
      &lt;td&gt;81.682&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_x_16gf&lt;/td&gt;
      &lt;td&gt;80.058&lt;/td&gt;
      &lt;td&gt;82.716&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;regnet_x_32gf&lt;/td&gt;
      &lt;td&gt;80.622&lt;/td&gt;
      &lt;td&gt;83.014&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnet50&lt;/td&gt;
      &lt;td&gt;76.13&lt;/td&gt;
      &lt;td&gt;80.858&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnet50 (quantized)&lt;/td&gt;
      &lt;td&gt;75.92&lt;/td&gt;
      &lt;td&gt;80.282&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnet101&lt;/td&gt;
      &lt;td&gt;77.374&lt;/td&gt;
      &lt;td&gt;81.886&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnet152&lt;/td&gt;
      &lt;td&gt;78.312&lt;/td&gt;
      &lt;td&gt;82.284&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnext50_32x4d&lt;/td&gt;
      &lt;td&gt;77.618&lt;/td&gt;
      &lt;td&gt;81.198&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnext101_32x8d&lt;/td&gt;
      &lt;td&gt;79.312&lt;/td&gt;
      &lt;td&gt;82.834&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;resnext101_32x8d (quantized)&lt;/td&gt;
      &lt;td&gt;78.986&lt;/td&gt;
      &lt;td&gt;82.574&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;wide_resnet50_2&lt;/td&gt;
      &lt;td&gt;78.468&lt;/td&gt;
      &lt;td&gt;81.602&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;wide_resnet101_2&lt;/td&gt;
      &lt;td&gt;78.848&lt;/td&gt;
      &lt;td&gt;82.51&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We would like to thank Piotr Dollar, Mannat Singh and Hugo Touvron for their past research and contributions to this work.&lt;/p&gt;

&lt;h3 id=&quot;new-augmentations-layers-and-losses&quot;&gt;New Augmentations, Layers and Losses&lt;/h3&gt;

&lt;p&gt;This release brings a bunch of new primitives which can be used to produce SOTA models. Some highlights include the addition of &lt;a href=&quot;https://arxiv.org/abs/1912.02781&quot;&gt;AugMix&lt;/a&gt; data-augmentation method, the &lt;a href=&quot;https://arxiv.org/abs/1810.12890&quot;&gt;DropBlock&lt;/a&gt; layer, the &lt;a href=&quot;https://arxiv.org/abs/1911.08287&quot;&gt;cIoU/dIoU&lt;/a&gt; loss and &lt;a href=&quot;https://github.com/pytorch/vision/issues/5410&quot;&gt;many more&lt;/a&gt;. We would like to thank Aditya Oke, Abhijit Deo, Yassine Alouini and Hu Ye for contributing to the project and for helping us maintain TorchVision relevant and fresh.&lt;/p&gt;

&lt;h3 id=&quot;documentation&quot;&gt;Documentation&lt;/h3&gt;

&lt;p&gt;We completely revamped our models documentation to make them easier to browse, and added various key information such as supported image sizes, or image pre-processing steps of pre-trained weights. We now have a &lt;a href=&quot;https://pytorch.org/vision/main/models.html&quot;&gt;main model page&lt;/a&gt; with various &lt;a href=&quot;https://pytorch.org/vision/main/models.html#table-of-all-available-classification-weights&quot;&gt;summary tables&lt;/a&gt; of available weights, and each model has a &lt;a href=&quot;https://pytorch.org/vision/main/models/resnet.html&quot;&gt;dedicated page&lt;/a&gt;. Each model builder is also documented in their &lt;a href=&quot;https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50&quot;&gt;own page&lt;/a&gt;, with more details about the available weights, including accuracy, minimal image size, link to training recipes, and other valuable info. For comparison, our previous models docs are &lt;a href=&quot;https://pytorch.org/vision/0.12/models.html&quot;&gt;here&lt;/a&gt;. To provide feedback on the new documentation, please use the dedicated &lt;a href=&quot;https://github.com/pytorch/vision/issues/5511&quot;&gt;Github issue&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchaudio-v012&quot;&gt;TorchAudio v0.12&lt;/h2&gt;

&lt;h3 id=&quot;beta-streaming-api&quot;&gt;(BETA) Streaming API&lt;/h3&gt;

&lt;p align=&quot;middle&quot; float=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/streamingapi.jpeg&quot; width=&quot;40%&quot; /&gt; &lt;img src=&quot;/assets/images/torchaudio-0-12-streaming-ASR-2.gif&quot; width=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;StreamReader is TorchAudio‚Äôs new I/O API. It is backed by FFmpeg‚Ä†, and allows users to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Decode audio and video formats, including MP4 and AAC&lt;/li&gt;
  &lt;li&gt;Handle input forms, such as local files, network protocols, microphones, webcams, screen captures and file-like objects&lt;/li&gt;
  &lt;li&gt;Iterate over and decode chunk-by-chunk, while changing the sample rate or frame rate&lt;/li&gt;
  &lt;li&gt;Apply audio and video filters, such as low-pass filter and image scaling&lt;/li&gt;
  &lt;li&gt;Decode video with Nvidia‚Äôs hardware-based decoder (NVDEC)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For usage details, please check out the &lt;a href=&quot;https://pytorch.org/audio/0.12.0/io.html#streamreader&quot;&gt;documentation&lt;/a&gt; and tutorials:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/tutorials/streaming_api_tutorial.html&quot;&gt;Media Stream API - Pt.1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/tutorials/streaming_api2_tutorial.html&quot;&gt;Media Stream API - Pt.2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/tutorials/online_asr_tutorial.html&quot;&gt;Online ASR with Emformer RNN-T&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/tutorials/device_asr.html&quot;&gt;Device ASR with Emformer RNN-T&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/hw_acceleration_tutorial.html&quot;&gt;Accelerated Video Decoding with NVDEC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;‚Ä† To use StreamReader, FFmpeg libraries are required. Please install FFmpeg. The coverage of codecs depends on how these libraries are configured. TorchAudio official binaries are compiled to work with FFmpeg 4 libraries; FFmpeg 5 can be used if TorchAudio is built from source.&lt;/p&gt;

&lt;h3 id=&quot;beta-ctc-beam-search-decoder&quot;&gt;(BETA) CTC Beam Search Decoder&lt;/h3&gt;

&lt;p&gt;TorchAudio integrates the wav2letter CTC beam search decoder from &lt;a href=&quot;https://arxiv.org/pdf/2201.12465.pdf&quot;&gt;Flashlight&lt;/a&gt; (&lt;a href=&quot;https://github.com/flashlight/flashlight&quot;&gt;GitHub&lt;/a&gt;). The addition of this inference time decoder enables running end-to-end CTC ASR evaluation using TorchAudio utils.&lt;/p&gt;

&lt;p&gt;Customizable lexicon and lexicon-free decoders are supported, and both are compatible with KenLM n-gram language models or without using a language model. TorchAudio additionally supports downloading token, lexicon, and pretrained KenLM files for the LibriSpeech dataset.&lt;/p&gt;

&lt;p&gt;For usage details, please check out the &lt;a href=&quot;https://pytorch.org/audio/0.12.0/models.decoder.html#ctcdecoder&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/0.12.0/tutorials/asr_inference_with_ctc_decoder_tutorial.html&quot;&gt;ASR inference tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-new-beamforming-modules-and-methods&quot;&gt;(BETA) New Beamforming Modules and Methods&lt;/h3&gt;

&lt;p&gt;To improve flexibility in usage, the release adds two new beamforming modules under torchaudio.transforms: &lt;a href=&quot;https://pytorch.org/audio/0.12.0/transforms.html#soudenmvdr&quot;&gt;SoudenMVDR&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/0.12.0/transforms.html#rtfmvdr&quot;&gt;RTFMVDR&lt;/a&gt;. The main differences from &lt;a href=&quot;https://pytorch.org/audio/0.11.0/transforms.html#mvdr&quot;&gt;MVDR&lt;/a&gt; are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use power spectral density (PSD) and relative transfer function (RTF) matrices as inputs instead of time-frequency masks. The module can be integrated with neural networks that directly predict complex-valued STFT coefficients of speech and noise&lt;/li&gt;
  &lt;li&gt;Add 'reference_channel' as an input argument in the forward method, to allow users to select the reference channel in model training or dynamically change the reference channel in inference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides the two modules, new function-level beamforming methods are added under torchaudio.functional. These include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/functional.html#psd&quot;&gt;psd&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/functional.html#mvdr-weights-souden&quot;&gt;mvdr_weights_souden&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/functional.html#mvdr-weights-rtf&quot;&gt;mvdr_weights_rtf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/functional.html#rtf-evd&quot;&gt;rtf_evd&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/functional.html#rtf-power&quot;&gt;rtf_power&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/audio/0.12.0/functional.html#apply-beamforming&quot;&gt;apply_beamforming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For usage details, please check out the documentation at &lt;a href=&quot;https://pytorch.org/audio/0.12.0/transforms.html#multi-channel&quot;&gt;torchaudio.transforms&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/audio/0.12.0/functional.html#multi-channel&quot;&gt;torchaudio.functional&lt;/a&gt; and the &lt;a href=&quot;https://pytorch.org/audio/0.12.0/tutorials/mvdr_tutorial.html&quot;&gt;Speech Enhancement with MVDR Beamforming tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchtext-v013&quot;&gt;TorchText v0.13&lt;/h2&gt;

&lt;h3 id=&quot;glue-datasets&quot;&gt;Glue Datasets&lt;/h3&gt;

&lt;p&gt;We increased the number of datasets in TorchText from 22 to 30 by adding the remaining 8 datasets from the GLUE benchmark (SST-2 was already supported). The complete list of GLUE datasets is as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nyu-mll.github.io/CoLA/&quot;&gt;CoLA&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/pdf/1805.12471.pdf&quot;&gt;paper&lt;/a&gt;): Single sentence binary classification acceptability task&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nlp.stanford.edu/sentiment/&quot;&gt;SST-2&lt;/a&gt; (&lt;a href=&quot;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&quot;&gt;paper&lt;/a&gt;): Single sentence binary classification sentiment task&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&quot;&gt;MRPC&lt;/a&gt; (&lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/I05-50025B15D.pdf&quot;&gt;paper&lt;/a&gt;): Dual sentence binary classification paraphrase task&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs&quot;&gt;QQP&lt;/a&gt;: Dual sentence binary classification paraphrase task&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark&quot;&gt;STS-B&lt;/a&gt; (&lt;a href=&quot;https://aclanthology.org/S17-2001.pdf&quot;&gt;paper&lt;/a&gt;): Single sentence to float regression sentence similarity task&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/&quot;&gt;MNLI&lt;/a&gt; (&lt;a href=&quot;https://cims.nyu.edu/~sbowman/multinli/paper.pdf&quot;&gt;paper&lt;/a&gt;): Sentence ternary classification NLI task&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gluebenchmark.com/&quot;&gt;QNLI&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/pdf/1804.07461.pdf&quot;&gt;paper&lt;/a&gt;): Sentence binary classification QA and NLI tasks&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aclweb.org/aclwiki/Recognizing_Textual_Entailment&quot;&gt;RTE&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/pdf/2010.03061.pdf&quot;&gt;paper&lt;/a&gt;): Dual sentence binary classification NLI task&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html&quot;&gt;WNLI&lt;/a&gt; (&lt;a href=&quot;http://commonsensereasoning.org/2011/papers/Levesque.pdf&quot;&gt;paper&lt;/a&gt;): Dual sentence binary classification coreference and NLI tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scriptable-bert-tokenizer&quot;&gt;Scriptable BERT Tokenizer&lt;/h3&gt;

&lt;p&gt;TorchText has extended support for scriptable tokenizer by adding the WordPiece tokenizer used in BERT. It is one of the commonly used algorithms for splitting input text into sub-words units and was introduced in &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf&quot;&gt;Japanese and Korean Voice Search (Schuster et al., 2012)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TorchScriptabilty support would allow users to embed the BERT text-preprocessing natively in C++ without needing the support of python runtime. As TorchText now supports the CMAKE build system to natively link torchtext binaries with application code, users can easily integrate BERT tokenizers for deployment needs.&lt;/p&gt;

&lt;p&gt;For usage details, please refer to the corresponding &lt;a href=&quot;https://pytorch.org/text/main/transforms.html#torchtext.transforms.BERTTokenizer&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;torchrec-v020&quot;&gt;TorchRec v0.2.0&lt;/h2&gt;

&lt;h3 id=&quot;embeddingmodule--dlrm-benchmarks&quot;&gt;EmbeddingModule + DLRM benchmarks&lt;/h3&gt;

&lt;p&gt;A set of &lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/benchmarks&quot;&gt;benchmarking tests&lt;/a&gt;, showing performance characteristics of TorchRec‚Äôs base modules  and research models built out of TorchRec.&lt;/p&gt;

&lt;h3 id=&quot;twotower-retrieval-example-with-faiss&quot;&gt;TwoTower Retrieval Example, with FAISS&lt;/h3&gt;

&lt;p&gt;We provide an &lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/examples/retrieval&quot;&gt;example&lt;/a&gt; demonstrating training a distributed TwoTower (i.e. User-Item) Retrieval model that is sharded using TorchRec. The projected item embeddings are added to an IVFPQ FAISS index for candidate generation. The retrieval model and KNN lookup are bundled in a Pytorch model for efficient end-to-end retrieval.&lt;/p&gt;

&lt;h3 id=&quot;integrations&quot;&gt;Integrations&lt;/h3&gt;

&lt;p&gt;We demonstrate that TorchRec works out of the box with many components commonly used alongside PyTorch models in production like systems, such as&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/examples/ray&quot;&gt;Training&lt;/a&gt; a TorchRec model on Ray Clusters utilizing the Torchx Ray scheduler&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/torchrec/datasets/scripts/nvt&quot;&gt;Preprocessing&lt;/a&gt; and DataLoading with NVTabular on DLRM&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/examples/torcharrow&quot;&gt;Training&lt;/a&gt; a TorchRec model with on-the-fly preprocessing with TorchArrow showcasing RecSys domain UDFs&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sequential-embeddings-example-bert4rec&quot;&gt;Sequential Embeddings Example: Bert4Rec&lt;/h3&gt;

&lt;p&gt;We provide an &lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/examples/bert4rec&quot;&gt;example&lt;/a&gt;, using TorchRec, that reimplements the &lt;a href=&quot;https://arxiv.org/abs/1904.06690&quot;&gt;BERT4REC&lt;/a&gt; paper, showcasing EmbeddingCollection for non-pooled embeddings. Using DistributedModelParallel we see a 35% QPS gain over conventional data parallelism.&lt;/p&gt;

&lt;h3 id=&quot;beta-planner&quot;&gt;(Beta) Planner&lt;/h3&gt;

&lt;p&gt;The TorchRec library includes a built-in &lt;a href=&quot;https://pytorch.org/torchrec/torchrec.distributed.planner.html&quot;&gt;planner&lt;/a&gt; that selects near optimal sharding plan for a given model.  The planner attempts to identify the best sharding plan by evaluating a series of proposals which are statically analyzed and fed into an integer partitioner.  The planner is able to automatically adjust plans for a wide range of hardware setups, allowing users to scale performance seamlessly from local development environment to large scale production hardware. See this &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/main/torchrec/distributed/planner/Planner_Introduction.ipynb&quot;&gt;notebook&lt;/a&gt; for a more detailed tutorial.&lt;/p&gt;

&lt;h3 id=&quot;beta-inference&quot;&gt;(Beta) Inference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/torchrec/inference&quot;&gt;TorchRec Inference&lt;/a&gt; is a C++ library that supports multi-gpu inference. The TorchRec library is used to shard models written and packaged in Python via torch.package (an alternative to TorchScript). The torch.deploy library is used to serve inference from C++ by launching multiple Python interpreters carrying the packaged model, thus subverting the GIL. Two models are provided as examples: &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/main/examples/inference/dlrm_predict.py&quot;&gt;DLRM multi-GPU&lt;/a&gt; (sharded via TorchRec) and &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/main/examples/inference/dlrm_predict_single_gpu.py&quot;&gt;DLRM single-GPU&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-recmetrics&quot;&gt;(Beta) RecMetrics&lt;/h3&gt;

&lt;p&gt;RecMetrics is a &lt;a href=&quot;https://github.com/pytorch/torchrec/tree/main/torchrec/metrics&quot;&gt;metrics&lt;/a&gt; library that collects common utilities and optimizations for Recommendation models.  It extends &lt;a href=&quot;https://torchmetrics.readthedocs.io/en/stable/&quot;&gt;torchmetrics&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A centralized metrics module that allows users to add new metrics&lt;/li&gt;
  &lt;li&gt;Commonly used metrics, including AUC, Calibration, CTR, MSE/RMSE, NE &amp;amp; Throughput&lt;/li&gt;
  &lt;li&gt;Optimization for metrics related operations to reduce the overhead of metric computation&lt;/li&gt;
  &lt;li&gt;Checkpointing&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prototype-single-process-batched--fused-embeddings&quot;&gt;(Prototype) Single process Batched + Fused Embeddings&lt;/h3&gt;

&lt;p&gt;Previously TorchRec‚Äôs abstractions (EmbeddingBagCollection/EmbeddingCollection) over FBGEMM kernels, which provide benefits such as table batching, optimizer fusion, and UVM placement, could only be used in conjunction with DistributedModelParallel. We‚Äôve decoupled these notions from sharding, and introduced the &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/eb1247d8a2d16edc4952e5c2617e69acfe5477a5/torchrec/modules/fused_embedding_modules.py#L271&quot;&gt;FusedEmbeddingBagCollection&lt;/a&gt;, which can be used as a standalone module, with all of the above features, and can also be sharded.&lt;/p&gt;

&lt;h2 id=&quot;torchx-v020&quot;&gt;TorchX v0.2.0&lt;/h2&gt;

&lt;p&gt;TorchX is a job launcher that makes it easier to run PyTorch in distributed training clusters with many scheduler integrations including Kubernetes and Slurm. We‚Äôre excited to release TorchX 0.2.0 with a number of improvements. TorchX is currently being used in production in both on-premise and cloud environments.&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://pytorch.org/torchx/main/quickstart.html&quot;&gt;quickstart&lt;/a&gt; to start launching local and remote jobs.&lt;/p&gt;

&lt;h3 id=&quot;workspaces&quot;&gt;Workspaces&lt;/h3&gt;

&lt;p&gt;TorchX &lt;a href=&quot;https://pytorch.org/torchx/main/workspace.html&quot;&gt;now supports workspaces&lt;/a&gt; which allows users to easily launch training jobs using their local workspace. TorchX can automatically build a patch with your local training code on top of a base image to minimize iteration time and time to training.&lt;/p&gt;

&lt;h3 id=&quot;torchxconfig&quot;&gt;.torchxconfig&lt;/h3&gt;

&lt;p&gt;Specifying options in &lt;a href=&quot;https://pytorch.org/torchx/latest/runner.config.html&quot;&gt;.torchxconfig&lt;/a&gt; saves you from having to type long CLI commands each time you launch a job. You can also define project level generic configs and drop a config file in your home directory for user-level overrides.&lt;/p&gt;

&lt;h3 id=&quot;expanded-scheduler-support&quot;&gt;Expanded Scheduler Support&lt;/h3&gt;

&lt;p&gt;TorchX now supports &lt;a href=&quot;https://pytorch.org/torchx/main/schedulers/aws_batch.html&quot;&gt;AWS Batch&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/torchx/main/schedulers/ray.html&quot;&gt;Ray (experimental)&lt;/a&gt; schedulers in addition to our existing integrations.&lt;/p&gt;

&lt;h3 id=&quot;distributed-training-on-all-schedulers&quot;&gt;Distributed Training On All Schedulers&lt;/h3&gt;

&lt;p&gt;The TorchX dist.ddp component now works on all schedulers without any configuration. Distributed training workers will automatically discover each other when using &lt;a href=&quot;https://pytorch.org/docs/stable/distributed.elastic.html&quot;&gt;torchelastic&lt;/a&gt; via &lt;a href=&quot;https://pytorch.org/torchx/main/components/distributed.html&quot;&gt;the builtin dist.ddp component&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;hyper-parameter-optimization&quot;&gt;Hyper Parameter Optimization&lt;/h3&gt;

&lt;p&gt;TorchX &lt;a href=&quot;https://ax.dev/versions/latest/api/runners.html#module-ax.runners.torchx&quot;&gt;integrates with Ax&lt;/a&gt; to let you scale hyper-parameter optimizations (HPO) by launching the search trials onto remote clusters.&lt;/p&gt;

&lt;h3 id=&quot;file-and-device-mounts&quot;&gt;File and Device Mounts&lt;/h3&gt;

&lt;p&gt;TorchX now supports &lt;a href=&quot;https://pytorch.org/torchx/main/specs.html#mounts&quot;&gt;remote filesystem mounts and custom devices&lt;/a&gt;. This enables your PyTorch jobs to efficiently access cloud storage such as NFS or Lustre. The device mounts enables usage of network accelerators like Infiniband and custom inference/training accelerators.&lt;/p&gt;

&lt;h2 id=&quot;fbgemm-v020&quot;&gt;FBGemm v0.2.0&lt;/h2&gt;

&lt;p&gt;The FBGEMM library contains optimized kernels meant to improve the performance of PyTorch workloads. We‚Äôve added a number of new features and optimizations over the last few months that we are excited to report.&lt;/p&gt;

&lt;h3 id=&quot;inference-table-batched-embedding-tbe&quot;&gt;Inference Table Batched Embedding (TBE)&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/split_table_batched_embeddings_ops.py#L1541&quot;&gt;table batched embedding bag&lt;/a&gt; (TBE) operator is an important base operation for embedding lookup for recommendation system inference on GPU. We added the following enhancements for performance and flexibility:&lt;/p&gt;

&lt;p&gt;Alignment restriction removed&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Embedding dimension * data type size had to be multiple of 4B before and now, it is 1B.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unified Virtual Memory (UVM) caching kernel optimizations&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;UVM caching kernels now scale linearly with # of tables using UVM caching. Previously, it was having similar overhead as all tables using UVM caching&lt;/li&gt;
  &lt;li&gt;UVM caching kernel overhead is much smaller than before&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;inference-fp8-table-batched-embedding-tbe&quot;&gt;Inference FP8 Table Batched Embedding (TBE)&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/split_table_batched_embeddings_ops.py#L1541&quot;&gt;table batched embedding bag&lt;/a&gt; (TBE) previously supported FP32, FP16, INT8, INT4, and INT2 embedding weight types.  While these weight types work well in many models, we integrate FP8 weight types (in both GPU and CPU operations) to allow for numerical and performance evaluations of FP8 in our models.  Compared to INT8, FP8 does not require the additional bias and scale storage and calculations.  Additionally, the next generation of H100 GPUs has the FP8 support on Tensor Core (mainly matmul ops).&lt;/p&gt;

&lt;h3 id=&quot;jagged-tensor-kernels&quot;&gt;Jagged Tensor Kernels&lt;/h3&gt;

&lt;p&gt;We added optimized kernels to speed up &lt;a href=&quot;https://pytorch.org/torchrec/torchrec.sparse.html&quot;&gt;TorchRec JaggedTensor&lt;/a&gt;. The purpose of JaggedTensor is to handle the case where one dimension of the input data is ‚Äújagged‚Äù, meaning that each consecutive row in a given dimension may be a different length, which is often the case with sparse feature inputs in recommendation systems. The internal representation is shown below:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/assets/images/Jagged-Tensor-Figure-from-FBGEMM-section.png&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;We added ops for &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp#L982&quot;&gt;converting jagged tensors from sparse to dense formats&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp#L968&quot;&gt;and back&lt;/a&gt;, performing &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp#L996&quot;&gt;matrix multiplications with jagged tensors&lt;/a&gt;, and &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/src/jagged_tensor_ops_cpu.cpp#L995&quot;&gt;elementwise ops&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;optimized-permute102-baddbmm-permute102&quot;&gt;Optimized permute102-baddbmm-permute102&lt;/h3&gt;

&lt;p&gt;It is difficult to fuse various matrix multiplications where the batch size is not the batch size of the model, switching the batch dimension is a quick solution. We created the &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/src/sparse_ops_cpu.cpp#L2401&quot;&gt;permute102_baddbmm_permute102&lt;/a&gt; operation that switches the first and the second dimension, performs the batched matrix multiplication and then switches back. Currently we only support forward pass with FP16 data type and will support FP32 type and backward pass in the future.&lt;/p&gt;

&lt;h3 id=&quot;optimized-index_select-for-dim-0-index-selection&quot;&gt;Optimized index_select for dim 0 index selection&lt;/h3&gt;

&lt;p&gt;index_select is normally used as part of a sparse operation. While PyTorch supports a generic index_select for an arbitrary-dimension index selection, its performance for a special case like the dim 0 index selection is suboptimal. For this reason, we implement a &lt;a href=&quot;https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/src/sparse_ops_cpu.cpp#L2421&quot;&gt;specialized index_select for dim 0&lt;/a&gt;. In some cases, we have observed 1.4x performance gain from FBGEMM‚Äôs index_select compared to the one from PyTorch (using uniform index distribution).&lt;/p&gt;

&lt;p&gt;More about the implementation of influential instances can be found on our &lt;a href=&quot;https://github.com/pytorch/captum/tree/master/captum/influence&quot;&gt;GitHub&lt;/a&gt; page and &lt;a href=&quot;https://captum.ai/tutorials/TracInCP_Tutorial&quot;&gt;tutorials&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks for reading, If you‚Äôre interested in these updates and want to join the PyTorch community, we encourage you to join the &lt;a href=&quot;https://discuss.pytorch.org/&quot;&gt;discussion forums&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch/issues&quot;&gt;open GitHub issues&lt;/a&gt;. To get the latest news from PyTorch, follow us on &lt;a href=&quot;https://twitter.com/PyTorch&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;https://medium.com/pytorch&quot;&gt;Medium&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/pytorch&quot;&gt;YouTube&lt;/a&gt;, and &lt;a href=&quot;https://www.linkedin.com/company/pytorch&quot;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;

&lt;p&gt;Team PyTorch&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 1.12 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.</summary>
      

      
      
    </entry>
  
</feed>


